content,id,postDate,author
Nice post! Keep it up!,112872,2019-11-23 18:12:57,Vlad Yelisieiev
@kyakovlev looks like it is time to become a master in datasets ;),112872,2019-11-11 09:10:21,Roman
That's the plan. UCF meter readings,112872,2019-11-11 21:35:17,Tim Yee
"I did an automatic FE using a Featuretools library (see ASHRAE - Automatic FE: Featuretools & Selection FE). The analysis showed that in addition to the usual min/max/sum etc. for features, it is advisable to consider the MODE of some meteorological features. I tried to automate this: 
df['data_mode'] = group_df[col].rolling(window=window, min_periods=0).apply(lambda x: mode(x)[0][0], raw=True)
But it doesn't work, unfortunately (both mode(x)[0] and mode(x)[0][0]). Someone else might be able to use it in another way",112872,2019-11-04 21:38:43,Vitalii Mokin
"The format of the most of meteorological features can be optimize by rounding up data to int8 or uint8 without losing precision (see my kernel Very significant safe memory + LightGBM):
df['air_temperature'] = np.int8(round(2*df['air_temperature'],0)) # store values with precision 0.5
df['cloud_coverage'] = np.uint8(round(10*df['cloud_coverage'],0)) # store values with precision 0.1
df['dew_temperature'] = np.int8(round(5*df['dew_temperature'],0)) # store values with precision 0.2
df['precip_depth_1_hr'] = np.uint8(np.clip(round(df['precip_depth_1_hr'],0),0,255)) # transform [-1,343] to [0,255]",112872,2019-11-04 21:51:52,Vitalii Mokin
"Good, that means I can drop in late again :P",112872,2019-10-16 09:48:44,Psi
"Did anyone probed public/private data split?
Is it timebased splits or random? 
Didn't find information about it in competition description. ",112872,2019-10-17 17:52:02,Konstantin Yakovlev
"Seems noone did test)) So I did it.
https://www.kaggle.com/kyakovlev/ashrae-split-test

This leaderboard is calculated with approximately 78% of the test data. The final results will be based on the other 22%, so the final standings may be different.

submission = pd.read_csv('../input/ashrae-baseline-lgbm/submission.csv')
possible_public = int(len(submission)*0.78)

submission.iloc[possible_public:,1] = 0

LB before - 1.36
LB after -> 2.60 -> seems it's random split or 22% first lines of data is private (need one more test).",112872,2019-10-17 20:45:43,Konstantin Yakovlev
"I tried 
possible_public = int(len(submission)*0.22)
submission.iloc[:possible_public:,1] = 0

LB before - 1.37
LB after -> 2.53
 -> seems it's random split",112872,2019-10-18 15:52:49,uratatsu
"
This leaderboard is calculated with approximately 78% of the test data. The final results will be based on the other 22%, so the final standings may be different.

@d1348k , @kyakovlev, if multiply *0.22 in local validation predictions, help to generalize final results ?  LB_before  = int(len(submission_LB_before )*0.22), to generalize results for private dataset. ✨ ",112872,2019-10-19 17:17:29,CaesarLupum
"A logical split for this would be by building_id - so maybe we looking at only 22% of the buildings.
How hard is that assumption to probe?",112872,2019-10-20 19:27:48,PC Jimmmy
I don't understand this multiply thing and how you people come to conclusion. Can anyone please explain?,112872,2019-10-23 18:09:56,Mukul Sharma
In general case if leaderboard splitting is not randomized (not randomized means that first 78% of data is public and last 22% is private) then turning to zeroes private part (last 22%) of submission would not cause LB decreasing,112872,2019-10-23 22:56:30,Andrei S
"And I was wrong.
please see and upvote https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113751

(test set and row_id column are not strictly time ordered).

It's very important!",112872,2019-10-23 23:12:03,Konstantin Yakovlev
"@yourssharmaji 
they are taking 78% (in train case) and 22% (in validation case) of the rows in 'submission.csv' file. Now, they cross-check their position's using 'iloc' in submission.csv to esure if the public/private split is in pattern(time-based) or random…",112872,2019-11-07 21:30:12,Atul Anand
"Minification part added:
pickle is not the unique option btw.  
You can try use feather DataFrame.to_feather()
or Google BigQuery table DataFrame.to_gbq() -> it would be amazing if someone can prepare BigQuery integration notebook for with competition
or parquet DataFrame.to_parquet()",112872,2019-10-16 13:37:49,Konstantin Yakovlev
Was going to mention the pandas to_feather format myself. It would be nice to have a comparison between them. I will probably report one tomorrow.,112872,2019-10-17 21:42:37,Georgios Sarantitis
"Pandas to feather doesn't yet work with float16, so it wont work if you're downcasting data to float16. Be mindful",112872,2019-10-19 14:06:57,Kenechukwu
"Feather format based on parquet and it also has no support for float16. But it's fast.
The types supported by the file format are intended to be as minimal as possible, with a focus on how the types effect on disk storage. For example, 16-bit ints are not explicitly supported in the storage format since they are covered by 32-bit ints with an efficient encoding. This reduces the complexity of implementing readers and writers for the format. The types are:
BOOLEAN: 1 bit boolean
INT32: 32 bit signed ints
INT64: 64 bit signed ints
INT96: 96 bit signed ints
FLOAT: IEEE 32-bit floating point values
DOUBLE: IEEE 64-bit floating point values
BYTE_ARRAY: arbitrarily long byte arrays.

So you may want to change memory_reducer function to use 32bits minimum formats.",112872,2019-10-19 16:52:00,Konstantin Yakovlev
"Here is a blog post which compares feather, md5, parquet, pickle etc. And it shows that feather is the most optimized one. I am using feather for sometime (both at work and projects) and found it to be good (Didn't do any comparison though).
https://towardsdatascience.com/the-best-format-to-save-pandas-data-414dca023e0d",112872,2019-10-22 05:28:31,arnab
"Look forward to seeing your kernel! 
One can really learn a lot when taking part in the same competition with you!",112872,2019-10-16 03:53:14,Alex
"I am looking forward to the notebooks but I do think anything that can get you in medal zone should only be shared as an idea in forums irrespective of time so that you have to work for it rather than enabling freeloaders to inflate the leaderboard. Just a thought, you can still do it as long as it is allowed  by rules.",112872,2019-10-16 09:45:18,Ankit Sati
"added two models with simple fe:
(1.22 - https://www.kaggle.com/kyakovlev/ashrae-lgbm-simple-fe -lgbm)
(1.23 - https://www.kaggle.com/kyakovlev/ashrae-catboost - catboost)",112872,2019-10-25 18:02:42,Konstantin Yakovlev
"@kyakovlev considering both of them are doing good , did you try ensembling them ? ",112872,2019-10-25 18:10:39,Khairul Islam
"I have xgb gpu ready also))) I'll submit it tomorrow and blend probe.
The reason to publish 3 models and blend:

Show tiny differences in models train (categoricals/Nans/params/save/load)
Show how much score boost blend can give
",112872,2019-10-25 18:13:04,Konstantin Yakovlev
"I can literally visualize you doing this after ensembling the models tomorrow. 
",112872,2019-10-25 18:22:16,Khairul Islam
"Yes, blend works. 1.20 lb for lgbm (1.22) and catboost (1.23) - blend 50/50",112872,2019-10-26 18:10:17,Konstantin Yakovlev
"Also same kernels with 2 lines change scores 1.13 and blend should give 1.10 +-
I just don't think it's fair to share such high score kernels right now.",112872,2019-10-27 01:19:48,Konstantin Yakovlev
"Yah right .

with 2 lines change

Wow !!",112872,2019-10-27 04:23:36,Khairul Islam
"Hi @kyakovlev! First, thanks for sharing! I hope you'll do good as IEEE :)
What do you mean when you say:

For unknown site_id and building_id our predictions are terrible

Cause we have the same building and site ids in both train and test as far as my checks are correct 😄",112872,2019-10-18 20:39:45,Federico Raimondi
"You are right. 

Good thing is all our test buildings and test sites present in train set.

https://www.kaggle.com/kyakovlev/ashrae-cv-options


What do you mean

Be careful with kfold -> if any fold will not have all building_ids and all months data it will make bad predictions for such unknown items.",112872,2019-10-18 21:25:38,Konstantin Yakovlev
"Thanks, now I see what you meant.
I'm currently using TimeSeriesSplit but I have to manage to find splits where all building ids are present…",112872,2019-10-18 23:20:24,Federico Raimondi
"Moreover, the fact that building_idis present in both datasets (training and test) and is identical there, means, that models are tuned separately to an each building. Here, if building_idwere not present, and there were only the parameters of different buildings, weather data, etc., that is, a model for an average building, then it really was an universal model. As for me, this is a disadvantage of setting the task of the contest itself. The contest authors will receive not an universal model for their energy modernization tasks, but a one and a half thousand models for individual buildings.",112872,2019-11-04 21:08:53,Vitalii Mokin
You inspire me ! Congrats for help kagglers ! Great @kyakovlev !✔️ ,112872,2019-10-17 23:56:08,CaesarLupum
Thanks for sharing. You help me to understand a lot,112872,2019-10-21 17:50:11,Bertram Gilfoyle
Gilfoyle you should be working on the new internet :D,112872,2019-10-21 19:27:26,Gunes Evitan
"M4 Forecasting Competition winner: Hybrid Exponential Smoothing-Recurrent Neural Networks (ES-RNN)
https://eng.uber.com/m4-forecasting-competition/
https://github.com/M4Competition/M4-methods/tree/slaweks_ES-RNN/118%20-%20slaweks17",112872,2019-10-16 19:03:53,Mileta
"Interesting, but as always -> there is no Silver bullet each dataset is unique and approaches differ a lot.",112872,2019-10-16 19:06:11,Konstantin Yakovlev
Glad to see you here @kyakovlev This competition has a very large data set. Can you focus on some memory optimization and resampling topics this time?,112872,2019-10-16 18:51:57,Gunes Evitan
"I'll try. Normally (IEEE competition) I'm too lazy to make huge optimisations - Base kernel preparation on kaggle kernels -> then pass all to Google Cloud and run 64cores/64GB machine predictions -> no memory issues and much faster. 
One thing I can recommend from start - use HDD space for storing ""intermediate"" results and dataframes (kaggle gives us 5GB per kernel).",112872,2019-10-16 19:01:32,Konstantin Yakovlev
"@gunesevitan I've added example how to use hdd during kernel training.
https://www.kaggle.com/kyakovlev/ashrae-baseline-lgbm
There is also example how to save model and make predictions for multiple models (kfold/seed).",112872,2019-10-18 18:32:35,Konstantin Yakovlev
Thanks about this revelation . I always used to think that everyone else is using kaggle kernel efficiently and I am the only one who find it difficult to use the kaggle kernel because memory/CPU limitation and random failures; May be I process is not efficient enough! ,112872,2019-10-22 05:47:28,arnab
"
Gold zone / 4 weeks before deadline
  Top 200 / 2 weeks before deadline

IEEE all over again…
Anything that affects medal zone should be kept private (at least during the last 3-4 weeks of the competition)",112872,2019-10-16 08:58:29,Stanislav Blinov
"
IEEE all over again…

Is it good or bad? I do think that 2-4 week is a safe time window to learn and adapt. I also think that kaggle is not about medals but about exchanging ideas and learn (learn during competition when you have ""mojo"" to do experiments and apply new knowledge).",112872,2019-10-16 09:06:33,Konstantin Yakovlev
"
Is it good or bad?

It's weird, but it's kinda both.
I agree with you in general, sharing and learning are good, they're what Kaggle is about.
But I'll also try to explain my personal position. I'm so good and fast to adapt as many here. Excluding work and everyday stuff, I have ~1-1.5 hours for kaggling per day, and not every day. Also I'm crazy dude that prefers to develop his algo's and implement new stuff by himself - not just fork-commit-submit or Nforks-combine-submit. Keeping this in mind, I have assburn everytime highscoring public kernel arrives, because I'd like to do things by myself and remain competitive. But that's my personal assburn and I have to live with it - ""if public script beats your model, than your model sucks"" :D

Top 200 / 2 weeks before deadline

I think this competition will have many team since it's tabular. Hence, there will be, say, 500ppl in medals. If you (not ""you"" personally, just somebody) will post kernel in top200 2 weeks before deadline, it may ruin the competition for participants like me, who don't have much time to spend and makes own models.
Oh, it all looks a bit messy, so, trying to summarize: I think that anything affecting medals shouldn't be posted during the last 3-4 weeks of the competition, for the sake of dudes like me. But that's my personal opinion, and I understand that I'm not alone here :)",112872,2019-10-16 09:56:34,Stanislav Blinov
"Actually, the competition host always benefits from such sharing. Strong straightforward combinations are shared openly and can be further improved or used for blending. This pushes everybody to look up for new ways to tackle the problem.
In the end, we get more knowledge and experience. And the host gets better and more variations of solutions.",112872,2019-10-16 10:37:24,Sergii
"@stanislavblinov, ideally I agree with everything you said and here is my take. Sharing of high scoring kernel is a mixed blessing.  On one hand if one is lazy, you can use that kernel and do nothing else which is a waste of time because you may not learn anything. If shared a month before the end of a competition, I personally use it as a motivation to improve my model so that I can beat its score. On the other extreme if there is not much sharing, only hard working people with a lot of time in their hands and those with domain knowledge will do better. For those that are like me and you who is only here to learn and enhance our knowledge whilst challenging our brains, it is 50-50 good and bad :-)
By the way I have been in some competitions where the highest scoring kernels have serious design flaws that sent everyone that used it down over 1000 places because they did not read and understand the codes. A good example is Home Credit Default Risk.",112872,2019-10-17 22:47:35,YaGana Sheriff-Hussaini
So no need to join before 2 weeks to deadline. Thanks.,112872,2019-10-16 13:38:20,Dieter
@christofhenkel please join before and do same as you did for Jigsaw competition. For such newbie as me (and many other kagglers) in NLP your posts were such a great source of learning.,112872,2019-10-16 13:41:52,Konstantin Yakovlev
"Thanks Very much, your summaries always provide so much guidance, appreciated.",112872,2019-12-14 23:44:04,C4rl05/V
Very useful!!,112872,2019-12-08 11:24:15,echo
Great job ! Thanks for sharing!,112872,2019-11-21 03:25:39,SHIBATA
"I have a confusion how to predict on test set which does not have the target encoding column that we created in the train set??
I have created target encoding features based on meter_reading value in the train set with respect to site_id and building_id",112872,2019-11-20 18:42:37,Nitish Gupta
"Very nice ! 
Thank you very much for all your effort to share on this competition! 
You have been so helpfull during all past days.",112872,2019-11-04 23:16:38,mezoganet
"We formed a team. Topic will be frozen for a while. Thank you for all your support. 
Good luck to you.
May the Kaggle be with you.",112872,2019-10-31 14:56:50,Konstantin Yakovlev
"As allways very impressive !
Thank you so muuch !",112872,2019-10-30 19:54:52,mezoganet
it is a great share， I have learned a lot from it,112872,2019-10-30 05:37:15,瓶子
Thanks a lot for sharing!,112872,2019-10-25 10:12:25,Daniel Izzudin
"Thank you so much! I'm participating in this competition for a project in my data mining course, and this is really helpful for planning it. ",112872,2019-10-25 00:10:51,James H
Thanks for sharing.,112872,2019-10-24 01:30:23,snarain
Thanks for sharing! It seems to be very useful!:),112872,2019-10-22 17:12:36,Владимир Туманов (ШАД Минск)
thanks for sharing info.,112872,2019-10-21 21:29:46,snarain
"Thanks for the sharing info.
That solved some of my annoying.",112872,2019-10-21 19:52:19,Siegfried Milcheis
Thanks for sharing!,112872,2019-10-21 10:16:07,Monika Panfil
"Oh wow. Thanks for putting all of this together. I also very much appreciate your active commenting on all the questions / comments! 👍 
Super helpful…",112872,2019-10-20 19:13:47,chmaxx
"Great read thanks a lot, I'm pretty sure you'll do similar to IEEE , all the best!",112872,2019-10-20 18:26:35,Firat Gonen
thx a lot!,112872,2019-10-20 02:17:58,leozhudd
@kyakovlev Thanks for the insights. These are truly some important points to start with!,112872,2019-10-19 02:57:04,adityajumde
Great work,112872,2019-10-18 21:23:19,Anjali Ramaprasad
Any one succeeded in using one hot encoding with categorical features ? My kernel crushes each time whenever i try,112872,2019-10-18 19:19:54,Khairul Islam
"if you use lgbm, you can try to set df[column].astype('categorical')
https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html
Categorical Feature Support section.
More info:

https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575#626083
https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575#624919


Your kernel crushes because even 1/0 (bool/int8) encoding 20M row dataset becoming HUGE in memory and there is no any reason (for current dataset) to use pure one-hot encoding when you have Catboost and Lgbm categorical option.",112872,2019-10-18 19:27:49,Konstantin Yakovlev
"Does data minification only reduces RAM usage, or it improves model training/predicting time too?",112872,2019-10-18 17:31:49,rossinEndrew
"For tree-based models no.
If I remember well (please correct me) fo lgbm we have such flow:

We have dataset with fp8 for example
lgbm will reserve memory for fp32
lgbm will train on fp8

The question here that we need as much free memory as possible for step 2 and minification helps here.
Probably you have noticed during lgbm training how memory usage jumps in the beginning of training round - this is exactly reservation step. 

NN is a different case. We can use fp16 training which is much faster (in most of cases) than fp32 training.",112872,2019-10-18 17:44:14,Konstantin Yakovlev
"Yes, I've noticed that jump in RAM in beginning of training with lgbm, really it's necessary to have free RAM in this step. Thanks for your clarifying answer.",112872,2019-10-18 18:30:57,rossinEndrew
"The spike occurs because when you create your lgb datasets, you're likely passing in a pandas dataframe. LGBM's python api converts everything to float64 IIRC. Try passing in a raw numpy array, and you might not see that occur. There's also a flag you can pass where it'll first write to disk rather than creating another copy of the dataframe in ram.",112872,2019-10-20 04:12:31,authman
"the competition host always benefits from such sharing. Strong straightforward combinations are shared openly and can be further improved or used for blending. 
In the end, we get more knowledge and experience. And the host gets better and more variations of solutions.👍 👍 ",112872,2019-10-18 06:11:19,Kuldeep  Arya
"After going through your solution in IEEE Fraud detection, I feel like I can actually do well in this. Thanks for sharing.",112872,2019-10-17 11:03:38,Kenechukwu
Great :),112872,2019-10-16 11:48:11,Raj Chowdhury
"Questions concerning weather data and timestamp.
Can you validate the weather data for all sites? (site_ids 0 through 15)
The weather data appears inconsistent with the timestamp. For example, at site 4, the peak daily temperature occurs between 9PM and midnight, while the daily low temperatures occur between 1PM and 3 PM. This is not an occasional occurrence, but a pattern that is evident throughout the year. Actual daily lows occur around the hour before sunrise, not 2PM. Actual daily highs occur in the mid-afternoon, not at night time. As the data is at odds with data from the national weather service (I can't find any data on the NWS site that supports temperature profiles like the ones given), it appears to be in error.
This calls into question weather at other sites. A glance reveals some patterns that are once again at odds with actual weather patterns. For example, at site 0 the temperature decreases well past sunrise, while at site 1, the temperature begins to increase well before sunrise.
The meter reading profiles appear consistent with consumption patterns from other data sources. Perhaps the times for the weather data are not local times, but then the weather profile and the meter reading profile are not in synch. Sometimes weather is provided not in local time but by GMT (London time). If this is the case, then the sites are not all in the USA, but lie in other countries. (That would create another problem because we would not know which holiday schedule to apply.)
One minor point, hourly meter readings normally indicate consumption at the end of the hour. I.e. a reading for 0200 indicates the consumption from 0100 to 0200. In this case, the annual data should begin with timestamp 2016-01-01 0100 and end with timestamp 2017-0101 0000. But your training sets all end at 2016-12-31 2300 which does not account for the final hour of the year.
Please give some insight on the weather and timestamps so that we can synchronize the meter reading and weather profiles.
Thank you",112884,2019-10-24 00:53:19,Arthur Mazer
Interesting findings!,112884,2019-10-24 02:48:38,S D
"@amazer @claytonmiller yes I found similar issues in this Notebook and opened a discussion topic for that. Maybe we can hear from you/the organizing team, the best course of action?",112884,2019-10-26 11:05:24,JimFonseca
"@claytonmiller Could you double check that the timestamps for weather data and meter reading are using the same timezone? My impression is that meter readings are using local time while weather data is using some other time zone.
See the thread here: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114447#latest-658783",112884,2019-10-26 15:32:17,S D
@sohier Maybe you could look into this?,112884,2019-10-31 14:31:28,S D
I am assuming that this additional complexity has been introduced to make the challenge interesting.,112884,2019-11-03 17:04:06,arnab
"Hi @claytonmiller 
as you can see at my kernel Starter ⚡ Great Energy Predictor 
I explain this.
How do you interpret meter_reading=0? no consume?
If you check the builing 0 (building_id = 0) you will see:

there is no information between January - May 2016 (months 1 to 5). meter_reading==0.
From June 2016 (month=6) we have information! 

In my opinion you started to measure the consume of that specific building in June 2016. 
In order to complete the database from 2016, you filled the other months with 0.
as @dmitriyguller  said:

Ignoring 0-values won't take care of the problem, it will make the problem different. Instead of under-estimating the energy use for a building with a stuttering meter, it will over-estimate it. Another problem is that we don't actually know whether zeroes are ignored or not, so how are we supposed to build the model intelligently?
  If I were to guess, I would say that the big CV-LB differences are due to either different behavior of zeroes in the test set, or different treatment of them. This is why I'm staying out of this competition: the bad choice of metric is going to turn it into nonsense. You're going to get a much bigger bang for the buck figuring out how to deal with zeroes than you will figuring out how to build an intelligent model.

probably you can't answer, but I had to try :)",112884,2019-10-17 14:25:44,Nanashi
"Hi @claytonmiller, could you provide some details around the meter time measurements and how they correspond to the weather measurements on a site-by-site basis?  There could be a consistent measurement methodology that you folks have employed (and are assuming), but in the wild, all that stuff depends on the utility and can vary greatly.  Similarly,  I'm assuming the readings are in kWh and temperature appears to be in celsius, but I don't really know.  What I'm afraid of is that there's an industrial site or big data center mixed in this dataset that has a substation level MWh meter.  This would throw off the models if one is trying to view information in aggregate.
Thanks!",112884,2019-10-26 03:32:59,Callisto
The goal of this competition (i.e. building a counterfactual model) is very reminiscent of Google's CausalImpact.,112884,2019-10-19 13:06:45,Max Halford
"Just out of curiosity , if it is ASHRAE - Great Energy Predictor III , where are Predictor I and Predictor II ?? 🙄 ",112884,2019-10-16 18:22:25,Khairul Islam
"Check out this overview of the previous competitions: https://www.kaggle.com/c/ashrae-energy-prediction/overview/prior-competitions
They were held in the 90's and the first one even had the contestants receiving floppy disks in the mail to compete :)",112884,2019-10-17 02:12:10,Clayton
What Clayton has mentioned elsewhere - Kaggle has the floppy disk data available for you to see if todays methods can beat the 90's.  ,112884,2019-10-20 18:48:35,PC Jimmmy
Right here!,112884,2019-10-21 16:33:50,Sohier Dane
"Looking forward to an interesting competition, @claytonmiller!
Some kagglers in another thread expressed concern regards to how 'production usable' models out of this competition would be if they incorporated weather information—in reality, only forecasts would be known at inference, yet we're seemingly given actual accurate weather information for up to two years into the future.
How do we reconcile this? If weather truly impacts energy usage, then model output should correlate to weather and without an accurate future two-year forecast, ASHRAE won't be able to do much with the results.",112884,2019-10-16 07:17:19,authman
"Ideally, the goal of the competition should have been to predict the energy usage 'X' days or 'W' weeks in advance, i.e. standing today, my model should be able to predict the energy consumption of the Xth day in future. That way, I don't the need the weather on the Xth day. 
Will be willing to hear your thoughts @claytonmiller ",112884,2019-10-16 09:23:43,arnab
"I think I kind of follow the purpose:
Developing energy savings has two key elements: Forecasting future energy usage without improvements, and forecasting energy use after a specific set of improvements have been implemented, like the installation and purchase of investment-grade meters, whose prices continue to fall.
So, here we are trying to ""forecast energy use after a specific set of improvements have been implemented"" and that's why the weather data is available. ",112884,2019-10-16 10:58:46,arnab
"As per publication provided by @claytonmiller 
""For example, long-term forecasting is usually retroactive as the energy savings prediction is focused on what energy the building would have consumed if the intervention had not taken place. This situation means that input variables such as weather are known and not forecasts themselves. ""
which I think answers your question?
Br,
Martin",112884,2019-10-16 22:43:36,Martin Elfstadius
"Hey @authman and @arnabbiswas1  -- @diginfo hit the nail on the head with his explanation of the goal of long term prediction in this context -- you can find more information from this publication: https://www.mdpi.com/2504-4990/1/3/56
There is a whole domain of performance contracting which uses retroactive long term prediction to quantify what energy consumption would have potentially occurred without an energy savings intervention",112884,2019-10-17 02:10:31,Clayton
"In 2009 I retired and was thinking about a new city/state to live in.  Being a data driver xx?% kind of guy I downloaded all the weather data for USA for a decade.  Than looked for city where it never got too hot or too cold.
When you plot a decade worth of temperatures for a city, the curve is very repeating.  So making a model of the next decade of weather for a city is pretty much only an issue of getting the data set.  Can't of course model climate change, but otherwise not that big of a task for design.  I would assume that such a data set can in fact be purchased so folks designing buildings know what cooling and heating load will occur.",112884,2019-10-20 18:54:10,PC Jimmmy
Do some of the buildings use a source of heat that is not accounted for in the meters?,112884,2019-10-20 16:36:42,S D
"To those considering the best algorithm to start with, consider starting with LightGBM.",112884,2019-10-24 06:55:28,Atharva Patel
Thanks for the hosting.,112884,2019-10-16 16:22:10,Naruhiko Nakanishi
"Hi! I ve a question about metering. Does every building include every meter that it orginaly have? Can we assume that if building has only electricity meter in the competition heating and cooling services  are also provided by electricity?
With Regards",112884,2019-12-10 20:28:22,Przemysław Rejczak
Thanks,112884,2019-10-20 09:07:10,TianBaojie
"Predictor III in the wild. Its called calculus, I have worked in construction for years, The Idea that its colder at night then in the day seems to blow these peoples minds away.",112884,2019-10-17 05:04:21,Craig William
This Comment was deleted.,112884,2019-11-04 08:25:30,No user
This Comment was deleted.,112884,2019-10-21 06:52:48,No user
"Consider the scenario laid out in this diagram. This competition simulates the modeling challenge presented at the end of the timeline when measured energy and weather conditions are known and the adjusted baseline energy must be calculated. 

For further details, I recommend reading this paper by @claytonmiller: https://www.mdpi.com/2504-4990/
Edit: Updated for clarity",112985,2019-10-17 20:51:17,Chris Balbach
"The second link isn't quite right! As it is, the one ending in just 2504-4990 links to a paper on an anti-tumor agent, titled ""Design, Synthesis and Applications of Hyaluronic Acid-Paclitaxel Bioconjugates"" by Francesca Leonelli, Angela La Bella, Luisa Maria Migneco and Rinaldo Marini Bettolo. 
The first link is correct, and the link address is: https://www.mdpi.com/2504-4990/1/3/56",112985,2019-10-22 17:37:10,Lindsey
"Hi Chris! Thanks for hosting this competition. I think it's going to be a lot of fun.
I was wondering if you could clear up some confusion about using actualized weather data in a time series competition.
As you said:

Our shared goals are to uncover (and disseminate to the public) current ""best practices"" for methods of time series predictions that can be useful in our domain.

But typically the best practice is to not actualized data from the future when trying to predict the future - and is considered to be ""Data leakage""
Is there something unique about this competition that makes using actualized weather data in the test set different? Or is it more so because that in a competitive setting there is no possible way to restrict the use of weather data?",112985,2019-10-16 13:59:25,Rob Mulla
"Hi @chrisbalbach 
Thanks!
Can you say whether the dataset has buildings that have\don't have these improvements that are supposed to save energy?
or whether some of the buildings have improvements installed during the train\test period?
Maybe a specific building has improvements that affect warm water and not electricity within the dataset?
Thanks!",112985,2019-10-17 08:19:08,AmirH
"Thanks for hosting this competition @chrisbalbach 
Could you inform us the location where this data was collected?
As holidays are massive outliers on the consumption and could greatly impact the predictability of the models (as your linked paper also points out).
Is the location break down similar to the one in the paper?
Thank you again already in advance.",112985,2019-10-18 11:39:51,Henrique Mendonça
"@chrisbalbach - A related question to Henrique's - also related to location - are the timestamps in local time or GMT? Also, is there an hour shift for DST in the timestamps? I suppose I can derive the time zone by fitting to known consumption patterns, but if you will provide that information, I can save the time..",112985,2019-10-23 23:16:23,Bill Holst
"Hi Rob
One of the 'real world' use cases for the types of prediction models that this competition is targeting are Energy Efficiency Investments to our built environment that are financed using ""Energy Performance Contracting"" (EPC) mechanisms. 
In an effort to increase the standardization (thus lower the costs) of EPC project delivery, the Efficiency Valuation Organization (EVO) has created and distributes ""best practice"" documents for the Measurement and Verification procedures incorporated into Energy Performance Contracts.  These documents and procedures are referred to as the International Performance Measurement and Verification Protocol (IPMVP). 
The details of IPMVP ""best-practice"" documents are behind a pay-wall, but the premise of how models that forecast energy usage can be seen here.
In short, the green area of the time series cartoon, between the forecast energy and the ""actual' (i.e. measured), energy, represents 'savings' related to a specific set of improvements financed under a Performance Contract agreement.
Hope that helps explain some of the ways we hope results from this competition will be able to be used - to improve the techniques currently used for forecasting energy usage, after improvements to a building have occurred. 
_Chris",112985,2019-10-16 22:07:16,Chris Balbach
Thanks for the hosting.,112985,2019-10-16 16:19:01,Naruhiko Nakanishi
sir how can be the row 41697600,112985,2019-12-18 01:34:47,Amit
great，thanks！,112985,2019-12-06 09:19:01,QKmichael
Awesome,112985,2019-11-18 15:05:53,Rohit Chauhan
Thanks for hosting!  I'm happy to see this competition  on Kaggle.,112985,2019-11-16 14:38:35,dachristi
Nice work! Thank you.,112985,2019-10-23 22:23:30,Agastya Kommanamanchi
"Thanks for hosting this interesting competition! In the description of the previous competition in 1994 it's stated that:

Six winning teams were formally recognized, all participants were asked to write an ASHRAE paper to document their efforts and to present at ASHRAE conferences. As a part of these efforts, over a dozen peer-reviewed papers were published and several software vendors incorporated the algorithms described in these papers.

I think I found what is the summarizing paper here: https://www.osti.gov/biblio/33315, but is there anything publicly available?",112985,2019-10-20 14:42:47,tahaum
We posted several of the papers written about the prior competitions with our rehost of the prior competition: https://www.kaggle.com/c/great-energy-predictor-shootout-i/data,112985,2019-10-21 16:32:17,Sohier Dane
"Ahh, didn't notice. Cheers!",112985,2019-10-23 20:01:29,tahaum
"I am noticed that there are two competitions of great energy predictor. Which data makes more sense? And Competition III data seems has some reading errors on meterreading. How could we know the result of this meterreading is not correct, which should not be included when training the model. In other words, any criteria or hinds that we could get?
BTW, when I loaded competition III data, I could not find any data dictionary to describe the columns. However, when I reviewed the other competitor's discussion, some of them seems have files for describing the data. For example, meter column is code: 0 means electricity, 1 is chilledwater, and so on. 
Thanks.",112985,2019-10-30 16:56:38,XiaoyingYu
The data dictionary is available in Data tab once you scroll down the full description,112985,2019-11-14 13:23:00,Aravindan Krishnan
"""The competition data have been anonymously donated to the competition by dozens of data
owners, worldwide""
https://www.ashrae.org/File%20Library/Technical%20Resources/Research/RT_REPORT_A19.r1.pdf
the original plan of train=2 year, test= 1 year may be better
(at least we can model the transition from dec to jan next year)",113338,2019-10-21 15:45:11,Heng CherKeng
Agree. Having 2 years of data in train and 1 year of data in test would make way more sense to me.,113338,2019-10-21 17:07:16,Alexey Pronin
Aren't there total 16 site_id in the dataset? Or am I missing something here?,113338,2019-10-21 03:58:12,Pratik Gandhi
I didn't check but if it starts at 0 and ends at 15 that would be 16 indeed!,113338,2019-10-21 13:02:45,S D
"I said ""suspiciously similar"", not ""equal"".",113338,2019-10-21 15:01:25,Alexey Pronin
Nice finding!! thank you.,113338,2019-10-27 10:45:10,corochann
"I try to visualize the air temperature by heatmap to check against weather data online, alot of yearly average temperature can be found in US cities itself. 
Most of the temperature peak around mid of the year, it should be northern region.
Check this out -> https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113772#latest-654627 
and this kernel -> https://www.kaggle.com/ccjoshua/ashrae-airtemperature ",113338,2019-10-22 07:23:45,CC Joshua 
"Thank you, I'm curious about this topic! Have you faced any mathematical correlation about this aspect?",113338,2019-10-21 18:44:24,Ivan Vigorito
Good catch! Thanks for sharing. 👍 ,113338,2019-10-18 18:08:08,chmaxx
"I plan to model on the assumption that the sites are cities from USA - your find of 15 regions is nice match to 15 sites in our data - but to minimize the weather data they have to be cities - but one or more cities from each region would make some sense.
I am going with USA assumption as I can find that multiple DOE surveys over the last decades have occurred and lots of summary data available with regards to building energy use in USA as a result.  Have not seen anything similar in my google search for other parts of the world.",113338,2019-10-18 22:54:08,PC Jimmmy
What about Canada?  Any July 1st patterns?,113338,2019-10-21 13:03:02,S D
"Interesting find! However:
The site_id variable corresponds to weather stations. Some of those ASHRAE regions are quite large geographically. It wouldn't make sense to have data from a unique weather station for each region, unless somehow the buildings are grouped geographically in each region.",113338,2019-10-18 18:31:15,S D
I thought about that too. It could mean one of two things: (a) the buildings are grouped withing each region or (b) this idea is wrong and the ASHRAE regions have nothing to do with site_id. I think including holidays specific to each of the ASHRAE region might help to clarify this. ,113338,2019-10-18 19:05:51,Alexey Pronin
"I think the idea of holidays is fine, but I believe seasonal time such as winter, summer,.. can bring more importance since this places might use airconditioner or heating. On top of that I will try to mark weekend days to the model, since many of the facilities  ""primary_type"" are Education or Office and they are normally closed on weekends, a.k.a. lower energy consumption those days.",113338,2019-10-19 11:45:37,Ivan de los Santos
Thanks! But actually i don't know how to use this external data to create  new features ,113338,2019-11-07 13:06:23,Dxx
"I used this info and the sites' temperature data to make educated guesses as to which site is which, see here https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans",113338,2019-11-05 12:15:19,Silverback
"Thank for that.
It seem like site_id: 10 is difficult to map to any of the city in ASHRAE in United Stated. 
Anybody has better match for site_id: 10 ?",113338,2019-11-06 03:59:46,CC Joshua 
"I think it is Salt Lake City. It is definitely in the US in that area. Other good correlations were Las Vegas, San Francisco, Portland and Phoenix (if I recall correctly).
The issue here might be that, as Salt Lake City is on a lake, the temperature might change drastically if the temp sensor is next to the lake or a couple Kms/miles to the other side of the city. I just clicked the middle of the city to get the lat longs so could be a few Kms out.",113338,2019-11-07 12:40:40,Silverback
"maybe related?
https://vimeo.com/362155388",113643,2019-10-21 08:35:41,Heng CherKeng
"""More buildings make more generalizable models—Benchmarking prediction methods on open electrical meter data"" - Clayton Miller, 2019
This paper outlines a library of open-source regression techniques from the Scikit-Learn Python library and describes the process of applying them to open hourly electrical meter data from 482 non-residential buildings from the Building Data Genome Project",113643,2019-10-21 04:54:16,Heng CherKeng
"https://www.budslab.org/
Building Data Genome Project (ResearchGate) – Collaboration with Princeton CHAOS Lab – A open data set of hundreds of non-residential buildings from around the world with various temporal data mining techniques implemented
https://github.com/buds-lab/temporal-features-for-nonres-buildings-library
useful publications:
https://scholar.google.com.sg/citations?hl=en&user=akL857IAAAAJ&view_op=list_works&sortby=pubdate",113643,2019-10-21 04:37:16,Heng CherKeng
This is gold. ,113643,2019-10-21 05:10:35,arnab
I am unable to open the link - did you guys succeed?,114196,2019-10-30 12:11:48,Konrad Banachewicz
"Yes. I can. Someone has posted those on github as well:
https://github.com/grapestone5321/Great_Energy_Predictor_Shootout_1_2/tree/master/papers",114196,2019-11-03 16:00:47,arnab
"Yes.  I've posted those: 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113136#latest-656881",114196,2019-11-06 01:23:50,Naruhiko Nakanishi
Maybe this post could be stickied? Edit: thank you!,114196,2019-10-26 14:10:34,S D
"sie this is my first python data science project, i am not able to understand the formate of submission   file sir i  got error ",114196,2019-12-22 06:33:46,Amit
Thanks for sharing… love it dropbox pdf publications,114196,2019-11-03 12:13:38,"Patrick Uzuwe, Ph.D"
Thank you!,114196,2019-10-28 17:14:08,AJ Aberdein
Thanks!,114196,2019-10-28 16:18:50,Ivan Kondratov
"@claytonmiller @chrisbalbach  This is super helpful. 
There are bunch of papers referenced in ""More Buildings Make More Generalizable Models— Benchmarking Prediction Methods on Open Electrical Meter Data"". All most all these papers are behind paywall. Is there any way to access those as well? ",114196,2019-10-27 11:03:35,arnab
"I would suggest that the Pearson Correlation won't be effective for categorical -- continuous variable relationship , it would be better to work with creme's V or mutual info score in such cases . ",114903,2019-10-31 11:14:55,AdityaVikramSingh
i always found featuretools pretty hard to use and not that useful. So far I have never found any good automatic fe tools.,114903,2019-11-04 21:31:12,Louka Ewington-Pitsos
"Before finding the correlation, I encoded categorical features, but I agree with you, @avikrams, the Pearson correlation gave a bad filter. Subsequently, I separately made feature selection on a separate statistical characterictics - MEDIAN and SKEW. The result came out interesting - see ASHRAE - Automatic FE&FC",114903,2019-11-04 20:53:32,Vitalii Mokin
"Interesting share , I made something very similar too. ",114903,2019-11-07 05:21:52,AdityaVikramSingh
How are you dealing with the fact that it is a timeseries problem? I`ve tried using LGBM without much sucess.,119460,2019-11-29 21:44:20,Gabriel Prado
"Hi Gabriel,
It is technically a timeseries problem, but the data is very noisy and it is pretty tough to build a model that can predict 2 years ahead with one year of data. Thats why I think a lot of people are treating it mostly as a tabular prediction problem. LightGBM should work well if you remove some of time aspects (mainly month).
However, there are some great opportunities to focus on timeseries modeling in this competition. If you ignore the noise the energy usage pattern is quite simple, sometimes even constant.
Maybe you can try splitting by meter and train a simple LSTM model for each meter? Here is a nice kernel which implements an LSTM. 
Hope this helps!",119460,2019-11-29 22:28:02,Carlo Lepelaars
"As @carlolepelaars notes, the easiest way is to simply add a few time-based features (i.e. hour-of-day and day-of-week) and then treat it like any other tabular problem. However, you must take the time-series properties into account when doing your validation, or you'll never be able to accurately judge your model's quality.
The kernels I reference at the bottom of the post showcase basically this approach and include both a reasonable CV splitter and a complete end-to-end submission.
The time-series properties can still be used in small ways while mostly using a tabular approach. Interpolation of missing weather features is one. Lag-based value-over-time features are another.",119460,2019-11-30 02:06:18,Robert Stockton
"Yes, you have a point. There are still some fun challenges in this competition. I just hope the hosts will still be able to create a fair private leaderboard ranking when the competition ends.",119460,2019-11-29 17:28:29,Carlo Lepelaars
I agree. I got to around 20 on LB before leaks became widespread. I'm not convinced training on any leaked data will be  a dramatic improvement to private LB. I've been making consistent progress on LB without leaks since the leaks started. Wondering if you would like to consider teaming up - my models seems to work really well with your data cleanup notebook.,119460,2019-11-28 19:53:35,Tim Yee
I'm glad that my posts are providing you with new insights. I'm not quite ready to team-up yet -- I'm having more fun exploring and sharing without worrying about any over-sharing annoying my teammates.,119460,2019-11-29 02:43:19,Robert Stockton
How do you explain the lack of top kagglers (grandmasters etc.) in this competition?,119460,2019-12-01 13:28:12,bluetrain
"Perhaps an idiom?

One man's trash is another man's treasure
",119460,2019-12-01 13:44:56,Tim Yee
"@teeyee314 :)
@stecasasso, my feelings about this competion is mostly captured in this post by @purist1024. As for the grandmasters, they did not find web scraping as something they enjoy as stipulated by @cpmpml and a couple of others. I was as upset as anyone can be initially when the leaks was discovered but I took a step back and gathered my thoughts as I usually do in this type of situations. Since currently, there is no clean way of dropping out of a kaggle competition plus I have a little bit more time for kaggling than usual, I decided to use it as  a way to try out things I have not done before. In short I am glad I stayed because I have learnt quite a bit and learning is my main goal on Kaggle.
Sorry guys and girls for the long post :-)",119460,2019-12-01 14:26:00,YaGana Sheriff-Hussaini
"
How do you explain the lack of top kagglers (grandmasters etc.) in this competition?

Many were busy with NFL, plus several competitions launched near the same time.
That's the first explanation.  The second one is that most top kagglers nowadays are computer vision experts, and this is not a computer vision competition ;)",119460,2019-12-01 16:27:38,CPMP
"
Many were busy with NFL

Hmmm… NFL was launched 6 days before this one: do you mean that within the first 6 days of NFL competition everyone had already joined and fully committed to that? Difficult to imagine. 
I agree with the author of the post: this competition had good potential (I like the dirty data and big data parts).
But unfortunately it also has a poor design: 1 year of training vs. 2 years of test limits considerably the feature engineering, which is already limited because of the not-so-rich dataset.
However, the most worrying thing it's the size of the leakage which I am afraid Kaggle's admins and organizers don't have under control (despite their claim). 
If they had any clue about this, they would have not started this competition with this data => they were not aware of all this data being public => they still don't know 100% about the data which is publicly available => they cannot guarantee a leak-free private LB 
My 2 cents 
I regret not joining the NFL competition, which was also my first choice…",119460,2019-12-01 20:32:04,bluetrain
"Well, I know several GMs who opted for NFL instead of ASHRAE.",119460,2019-12-02 10:18:08,CPMP
"Another thing I found interesting is, the data features consist of continuous features and categorical features, which make designing of my neural networks more interesting.",119460,2019-11-30 23:16:09,LijunZhang
Interesting write-up. Do you have any insight regarding model validation? I tried vanilla KFold as well as GroupKFold (using hour of the day as group label) but the score I get on my test sets is always very different (ie. much better…) than the actual LB score.,119460,2019-11-29 08:37:05,Larry D.
"I provide an implementation of a reasonable scheme in one of my kernels.

We use a time-oriented split which deals in 12 equal time slices (which are almost, but not quite, months). Each fold includes a two adjacent slices for validation, 8 adjacent slices for training, and a 1 slice buffer on each side of the validation set. We use modulo arithmetic to wrap-around the year-end boundary. We thus have slices 4-11 vs 1-2; 6-12&1 vs 3-4; 8-12&1-3 vs 5-6; etc.
While the CV score is always better than the LB score, relative differences between strategies tend to align well between CV and LB.

def np_sample(a, frac):
    return a if frac == 1 else np.random.choice(a, int(len(a) * frac), replace=False)

def make_8121_splits(X, sample_frac):
    np.random.seed(0)
    time_sorted_idx = np.argsort(X.timestamp.values, kind='stable')
    sections = np.array_split(time_sorted_idx, 12)
    folds = []
    for start_ix in range(0, 12, 2):
        val_idxs = np.concatenate(sections[start_ix:start_ix + 2])  # no modulo necessary
        train_idxs = np.concatenate(
            [sections[ix % 12] for ix in range(start_ix + 3, start_ix + 11)])
        folds.append((np_sample(train_idxs, sample_frac), np_sample(val_idxs, sample_frac)))
    return folds

You can do even better by having a different set of time splits for each site, but the code gets noticably more complicated at that point.",119460,2019-11-30 00:23:18,Robert Stockton
Thanks! Are you keeping a fraction only of the train and validation sets for speed purpose or do you have something else in mind?,119460,2019-12-02 10:59:27,Larry D.
"For evaluating new features and the like, I've used as little as 3% of the input data, though it's a bit more stable with 10-20%. This is why I have the ""sample-frac"" built into my CV splitter. For final training, of course, I use the full dataset.",119460,2019-12-02 12:32:23,Robert Stockton
May I ask  your LB without leak?,119460,2019-11-29 02:33:55,zidaoziyan
"No problem. My non-leak score is 1.06, using: the various techniques I describe in my kernels; a few new features which I haven't posted yet; and 6-way CV for both early stopping and blending.",119460,2019-11-29 02:41:15,Robert Stockton
Which CV strategy you found to work best with this data set? ,119460,2019-11-29 14:03:40,Jonathan Mallia
@jonimatix See my reply to @larryd87 above.,119460,2019-11-30 00:24:17,Robert Stockton
"Thanks for trying it out. According to LOFO, wind speed doesn't have any negative or positive impact. My model without it also performed similar on LB. It may be related to how the feature is preprocessed, my adversarial scores are also not even close to 0.96.",122270,2019-12-19 06:34:07,Ahmet Erdem
this could be… a hurricane!,122270,2019-12-19 20:27:00,Mycroft Holmes
This is also consistent with what I found. I had it dropped although it looks counter-intuitive when wind_direction is still somewhat helpful. Thanks for sharing.,122270,2019-12-19 17:56:14,Shaking Goose
thanks，nice job！,122270,2019-12-19 10:55:58,Kai Shu
"Good stuff, thanks for sharing!
Congrats on the new job @tunguz ",122270,2019-12-19 05:08:49,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
congrats on the new job and thank you for the tip.,122270,2019-12-19 03:15:48,eagle4
Thanks for confirming that dropping wind_speed was for the best,122270,2019-12-19 02:57:05,yukiya
"Good stuff, thanks for sharing!",122270,2019-12-19 02:18:55,TJ Klein
"Oh, wow, that's indeed a huge finding! Thank you @tunguz 
LB shakeup is unavoidable :(",122270,2019-12-19 10:01:57,illia kavalevich
"Wow, thats insane!",122270,2019-12-19 01:06:06,Carlo Lepelaars
Good stuff,122270,2019-12-23 12:14:57,wei lu
I tried but it didn't improve my score,122270,2019-12-20 00:17:14,Kai Shu
This Comment was deleted.,122270,2019-12-22 15:22:16,No user
This Comment was deleted.,122270,2019-12-19 04:44:06,No user
"Very interesting. Did you find a way to locally validate the improvements from data cleaning, or did you have to submit predictions each time you modified your cleaning process and look at LB score?",122471,2019-12-22 21:06:12,Larry D.
I had to submit each time.,122471,2019-12-23 04:09:55,Gunes Evitan
"I think local validation wasn't very useful to optimize LB score for this competition.
Validating on LB or on the leaked data was better.",122471,2019-12-24 15:06:43,Vopani
"Yes, totally agree. That's why I thought low submission novice teams were cheating. I used more than 20 submissions only for verifying outlier removal. How come they are using less than 10 submissions and have the same score as me?",122471,2019-12-24 16:13:34,Gunes Evitan
amazing work !,122471,2019-12-22 12:36:04,ravi tanwar
Great work. Data cleaning was the key to get a good score. ,122471,2019-12-21 12:24:47,Oleg Knaub
"nice work @gunesevitan . I also did analysis of all buildings, but i guess i did some aggressive cleaning of data that's why didn't get much help. The only thing which helped me apart from public cleaning of site 0,was:
train_df = train_df[train_df.groupby(['building_id','meter'])['meter_reading'].cumsum().gt(0)]
removed trailing zeros from other buildings and it gave additional boost of around .02",122471,2019-12-21 11:07:21,HarshitMehta
"Nice approach, can it handle the edge cases in site 0? My cleaning code for site 0 is a mess. I found the last zero for every building before 2016-09-01 then dropped everything before that date.
leading_zeros = defaultdict(list)
for building_id in range(105):
    leading_zeros_last_date = df_train.query('building_id == @building_id and meter_reading == 0 and timestamp < ""2016-09-01 01:00:00""', engine='python')['timestamp'].max()
    leading_zeros[leading_zeros_last_date].append(building_id)

for timestamp in leading_zeros.keys():
    building_ids = leading_zeros[pd.Timestamp(timestamp)]
    filtered_idx = df_train[df_train['building_id'].isin(building_ids)].query('meter == 0 and timestamp <= @timestamp').index
    df_train.loc[filtered_idx, 'IsFiltered'] = 1
",122471,2019-12-21 12:05:48,Gunes Evitan
@gunesevitan it didn't take care of edge cases.,122471,2019-12-23 03:40:01,HarshitMehta
"Nice @gunesevitan !
I was experimenting with sigma clipping on the last days but was too late to get anything out of it. Here's the code anyways:

def sigma_filter(df, tolerance=3):
    ''' sigma clipping '''
    df['meter_reading_ln'] = np.log1p(df.meter_reading)
    stats = df.reset_index().set_index('timestamp').groupby(['building_id', 'meter'])\
                    .rolling(24*7, min_periods=2, center=True).meter_reading_ln.agg(['median'])
    std = df.reset_index().set_index('timestamp').groupby(['building_id', 'meter']).meter_reading_ln.std()
    stats['max'] = np.expm1(stats['median'] + tolerance*std)
    stats['min'] = np.expm1(stats['median'] - tolerance*std)
    stats['median'] = np.expm1(stats['median'])
    df = df.merge(stats[['median', 'min', 'max']], left_on=['building_id', 'meter', 'timestamp'], right_index=True)
    return df[(df.meter_reading <= df['max']) &amp; (df.meter_reading >= df['min'])]
",122471,2019-12-20 13:41:11,Henrique Mendonça
Very neat approach… If only you had time to try it :/,122471,2019-12-20 18:40:11,Gunes Evitan
how did you choose tolerance=3 ?,122471,2019-12-23 16:22:26,Kim Wilson
"@dhacker to be clear, the plot above is actually using tolerance/sigma=0.5 , but it cuts too much stuff out in some other cases, so 3 seemed like a safe value (in the building 1251, as above, it only eliminates the zeros) 
However, to be able to use larger sigma values in this data, it's important to use log scale!",122471,2019-12-27 16:54:30,Henrique Mendonça
Great work. ,122471,2019-12-24 17:10:06,remesh ck 
"Awesome job, well done",122471,2019-12-24 08:24:16,Jonathan Mallia
Great work done.,122471,2019-12-22 08:00:51,Waqas Nawaz
Nice insights and explanation! Our team had a strategy for deleting 'near zero' observations but we got into some of the pitfalls you mentioned. Happy to see this so we can learn from it!,122471,2019-12-21 22:28:40,Carlo Lepelaars
This is awesome work. Thank you for sharing :) ,122471,2019-12-20 20:43:15,Vishal 
Thanks for sharing.,122471,2019-12-20 13:08:38,Dean
This Comment was deleted.,122471,2019-12-20 11:42:04,No user
"Very helpful. Please keep the thread updated.
May you should replace all the links with proper titles, so that it becomes easy to follow the threads. Just my thought.",113146,2019-10-19 12:12:07,arnab
Cool! Thanks for putting this together! 👍 ,113146,2019-10-18 21:54:03,chmaxx
"How do you identify ""outliers""?",113678,2019-11-01 19:26:58,bluetrain
"Code and explanation is in my notebook:
https://www.kaggle.com/chmaxx/ashrae-eda-and-visualization-wip
Or this answer to a similar question:
https://www.kaggle.com/general/113313#652302
Code and procedure is based on this article:
https://machinelearningmastery.com/how-to-use-statistics-to-identify-outliers-in-data/",113678,2019-11-02 20:01:02,chmaxx
Thanks for briefing! Helpful.,113678,2019-10-31 03:31:49,LongYin/杰少
杰少，daidaiwo啊,113678,2019-11-15 10:03:25,Cobalt
Thanks for helpful insight of data.,113678,2019-11-01 16:58:43,Neelam
Thanks!,113678,2019-10-28 17:14:39,AJ Aberdein
Quite helpful EDA,113678,2019-10-24 08:12:27,Shashi Prakash Tripathi
thanks!,113678,2019-10-22 06:18:11,Krishna Katyal
Lots of stuff in a nutshell. Very useful.  Can you also add the kernel links where you found these info ? ,113678,2019-10-21 14:19:03,Khairul Islam
"Sure. Many things I found out by myself – see this kernel:
https://www.kaggle.com/chmaxx/ashrae-eda-and-visualization-wip
At the end I reference the kernels of the fellow Kagglers that have inspired the EDA., e.g.:
https://www.kaggle.com/blue07/eda-insights-on-weather-buildings
https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-ashrae",113678,2019-10-21 14:54:15,chmaxx
This is really great. All at one place. Thanks,113678,2019-10-29 18:52:08,Shashi Prakash Tripathi
"Everyone please stop with the contentless 'Thanks' or 'Very helpful' posts (borderline spam). Just simply click the upvote button, that's what it's there for. (Whereas if you post a contentless response, it triggers an individual email to everyone subscribed to each thread.)",113678,2019-10-29 19:27:01,Stephen McInerney
I think that is because it is an easy way to get bronze medals in discussion. Kagglers are kind enough to appreciate those kind words. ,113678,2019-10-31 04:05:04,Khairul Islam
Thanks for briefing!  ,113678,2019-10-21 13:25:52,Manraj Singh
"Guys, give me advice what to do with features yea built, floor count if there is no data for almost 50% of buildings in set?",113678,2019-11-15 12:35:21,AvtandylRudenko
This Comment was deleted.,113678,2019-11-03 14:19:29,No user
"Oh, awesome! Thank you for sharing this! It's nice to confirm own thoughts like this :) Though it is quite logical that they have decided to reserve recent data for the private :P Best of luck in the competition",113751,2019-12-12 17:56:41,illia kavalevich
"As describe in data description: 

There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.

So there might be more than the last 22% of data which are in private set.
I tried to set exp(25) for all values from 2018 and it also score 4.69. To check if it was a problem due to exp(25) being too big, I tried to set this value for all timestamp of the test set and it scored 20.6 (so there is no issue with exp(25)).
For now WE NEED YOU to test the real dates of the private leaderboard (we have just 2 submission a day per person). To do so, here is some code to create a submission with 0 for all timestamps exept for dates greater than 2018-01-01.
import numpy as np
import pandas as pd


# load data
test = pd.read_csv('data/test.csv', parse_dates=['timestamp'])
submission = pd.read_csv('data/sample_submission.csv')

# change last year predictions to exp(25)
mask = test.query('timestamp >= ""2018-01-01""').row_id.tolist()
submission.loc[submission['row_id'].isin(mask), 'meter_reading'] = np.exp(25)

# save submission
submission.to_csv('submission/probing_all.csv.zip', index=False, compression='zip')

Please try some random dates like 2017-12-01 and share your findings in order to find the truth 🙏 ",113751,2019-10-28 09:44:02,Robin Vaysse
"I added the 31st of December 2017 in addition to the whole 2018 year and replace exp(25) by exp(50). It scored 5.27.
mask = test.query('timestamp >= ""2017-12-31""').row_id.tolist()",113751,2019-10-28 16:05:47,Adil Zouitine
"Thx guys! Now we can be pretty confident saying that public and private sets are respectively 2017 and 2018. If the private part really represents 22% of test data as organizers said, it means it has 4-ish more gaps than the public…",113751,2019-10-28 16:44:15,Geoffrey Bolmier
"Great! Thanks for your investigations!
From Adil's number, 5.27 with exp(50) and all 0 score of 4.69 we can write
1/N * sum_inDec31 (y_true - 50)**2 - y_true**2 = 5.27**2 - 4.69**2,

where y = log(1 + meter_reading). 
Approximating all y_true with 4 and assuming N days of equal amount of data,
N ~ (46**2 - 4**2)/(5.27**2 - 4.69**2) = 363.52.

This is consistent with 365 days (if there are ""gaps,"" Dec 31 has the same ratio of gaps as the year 2017 average.) It's most natural to think that gaps are minor in year 2017, as they are in the training set. I rather doubt the official statement that the public is 78%, I think public:private is 50:50.
[I first forgot about the square in the metric, I erased my stupid comment and rewrote]​​​",113751,2019-10-28 21:10:52,🐢 Jun Koda
"Based on your code, I probed more data with the code below:
mask = test[(test.timestamp>'2018-07-20 00:00:00') & (test.timestamp<'2019-01-01 00:00:00')].row_id.tolist()
And still the same score (4.69).
ps.

'2018-07-20 00:00:00'  means 22.5%.
",113751,2019-10-27 03:46:53,light
"Probed a bit more before seeing your message adding last 14 days of July with the same result (4.69 for 22.88%):
mask = test.query('timestamp >= ""2018-07-18""').row_id.tolist()

No more submission for me today…",113751,2019-10-27 09:51:54,Geoffrey Bolmier
"Probed from April and got the same result (4.69 for 37.67%):
mask = test.query('timestamp >= ""2018-04-01""').row_id.tolist()

As organizers said in the data section: 

There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.

Private part containing way more gaps than the public could explain such results, meaning also that test distribution (78% public, 22% private) has been computed excluding those gaps",113751,2019-10-28 08:37:46,Geoffrey Bolmier
"Following are the fraction of rows belonging to a particular month-year (across train and test). It's very clear that the number of rows  across different months in 2016 are varying w.r.t each other, but for each month across 2017 & 2018, the number of rows are matching exactly (up to sixth decimal). For months with 31 days, the fraction value is 0.028600 for 2017/2018. For months with 30 days, the fraction value is 0.027677. These two numbers are not matching with any of the months from 2016. 
This probably indicates the gaps have been systematically filled across the entire test data (2017/2018), but that is NOT the case for train data (2016).
",113751,2019-10-28 14:48:51,arnab
Interesting! Thank you for sharing this. Seems legitimate to reserve the most recent data for the private leaderboard score.,113751,2019-10-24 18:57:29,Carlo Lepelaars
"
(test set and row_id column are not strictly time ordered).

Blame on me. I didn't find it before you said. THANK YOU! 
Seems you are right. 
I did test with 22% of data but it was just last 22% of rows (and that test didn't show time split but I was wrong).",113751,2019-10-23 12:12:49,Konstantin Yakovlev
Not easy to spot at first glance. Glad to have been of help @kyakovlev!,113751,2019-10-23 16:34:47,Geoffrey Bolmier
Thanks for sharing!,113751,2019-10-30 08:32:48,Sogna
"An entire year being in the private set makes more sense than August-December, because we don't skip the important summer period.
However, how do we reconcile that with the 22% figure, even with the gaps? It would mean there is 3.5 (78/22) more data in 2017 than 2018!
(Edit: OP already mentioned that same issue in another post)",113751,2019-10-29 12:59:58,S D
Really nice! 👍 ,113751,2019-10-24 18:11:08,AlvaroSanchez
wow huge!!!,113751,2019-10-23 13:27:12,olaleye eniola
"""Seems like the private dataset is just the last 22% of the data time ordered""
Which it's what makes sense to do.
Thanks for confirming!",113751,2019-10-23 09:16:16,bluetrain
"your probing says that last 5 months is not in public test, but not saying  'private dataset is just the last 22% of the data time ordered'… maybe i misunderstood",113751,2019-10-23 03:17:18,LeiZhou
"There is a sentence above the leaderboard which indicates the proportion of data in the public and private datasets:

This leaderboard is calculated with approximately 78% of the test data.
  The final results will be based on the other 22%, so the final standings may be different.
",113751,2019-10-23 08:22:38,Robin Vaysse
Oh thanks! Now it makes sense to me))),113751,2019-10-23 09:55:56,LeiZhou
This Comment was deleted.,113751,2019-10-22 18:41:40,No user
Thank you for sharing such impressive story!,114345,2019-11-09 05:24:20,fujiyuu75 
"Thanks for sharing!
I checked my diary and found my first competition's record (in-class, it belongs to my statistic machine learning class's project, in > 3 years ago) just now, the interface in 2016 is quite different from the current one. Here I wish to show it with you all : )
That's interesting to look back on previous competitions we've participated. The memories, efforts, exciting for the improvements and worry about huge shake up…
Happy Kaggle everyday!
",114345,2019-10-31 09:29:59,Venn
"Thank you for sharing your history!
I feel now like i am in the place when you've started. You are now advanced person on Kaggle and i am like in primary school.
I started me serious journey with data science in my University. I was the time when i had so many laboratories on which i was doing measurements and then i had to prepare report from this. I've loved this. It was one of my fav things in my Uni.
Then i started first job, completly not connected with my studies. And it lasted for 1.5 year. I was depressed, exhausted. And i decided to change everything. To have fun from my job, to do what i really love to do. 
Now I am working as software automation tester. But i am learning now everyday data science, machine learning, NLP, big data. I really love this. And i hope to change my job soon from tester to data scientist. I will be connected with my studies and with what i love!
I love that kind of stories, to read to gain knowledge!
I have similar schedule to your, but of course sometimes it is hard to strictly kepp with it. I start learning at about 8 pm and it lasts everyday for 4-5 hours. But after several days i feel exhausted.. and just after job there are some days when i just fall asleep.. 
I wish you luck! ",114345,2019-11-05 13:09:35,Monika Panfil
thanks,114345,2019-11-04 13:37:44,chen pu
Indeed helpful and inspiring for beginners. Thank you so much for sharing!,114345,2019-11-02 05:00:52,Gaurav kumar
I just joined a week ago and I'm so amazed at what one can accomplish after finding this website. Wish more people knew about it!,114345,2019-10-31 14:43:39,Carlos Salgado
Thanks for sharing! It is an inspiration to me!,114345,2019-10-31 09:10:23,Sailaja Thippabhotla
Very inspirational..!,114345,2019-10-31 05:34:38,shrutigoel
"I read your code in IEEE, that is quite a lot of work. Thanks for sharing.",114345,2019-10-31 03:18:22,LongYin/杰少
"The interesting thing that it just seems so complicated.
But after the first competition, you have a bunch of useful stuff that migrates from kernel to kernel.",114345,2019-10-31 03:26:27,Konstantin Yakovlev
"Wow 2 hours reading and 2 hours coding a day. @kyakovlev you must be very persistent and committed. And I guess this is on top of normal work commitments. 
I'm spending all time debugging and not enough reading. It is time to re-adjust my trajectory. Thank you for sharing and inspiring! ",114345,2019-11-02 20:30:22,David Tang
it's so inspiring for me as novice,114345,2019-10-30 15:44:21,fithratullah habibie
Thanks for sharing! It is an inspiration to me!,114345,2019-10-30 15:38:42,TSRTSR
Awesome write-up. Very inspiring for beginners like me. ,114345,2019-10-29 19:16:28,AshitShrivastava
Thanks for sharing!,114345,2019-10-29 05:16:19,Tony
Congrats !! 1st competition 210 entries wow,114345,2019-10-29 02:41:49,Anurag
"
200 is not large number)) It's normal for any competition that permits submit 5 times per day.",114345,2019-10-30 09:50:25,Konstantin Yakovlev
Congrats @kyakovlev on an amazing journey and thanks for sharing.,114345,2019-10-29 00:30:41,YaGana Sheriff-Hussaini
"Wow, such an epic journey 😍 ",114345,2019-10-28 23:06:11,Hieu Phung
"Thanks for sharing @kyakovlev !!
You're doing an awesome job!
I share your enthusiasm and passion for Kaggling.
Currently i'm on a break, because of the obsession and addiction Kaggling is causing me.
I am currently working with reinforcement learning on my own - something i can do with a bit more self control.
but it's very difficult seeing the great competitions that come up and only watching from the side.
I appreciate your involvment in the community and the amount of time and energy you spend on sharing your methods.
I don't always agree with the extent and\or timing of it (i.e sharing your process with gold solution, even if still few weeks for competition) but i appreciate it very much =)
Keep up the good work my friend, 
May the Kaggle be with you and good luck in your future endeavors!",114345,2019-10-28 19:46:27,AmirH
Thanks for sharing your Kaggle story!,114345,2019-10-28 18:44:26,Erica Brick
Thanks for sharing your journey! It motivates beginners like me to do better day by day! Congrats and thanks again!,114345,2019-10-28 02:50:41,fit_%
"Great post and great journey, Konstantin.
It was a previlege to know you, to learn from you and to witness your generosity .
 Really inspiring!",114345,2019-10-30 01:15:17,João Gonçalves
Great article @kyakovlev … Thanks for sharing it. ,114345,2019-10-29 15:32:30,Manoj Prabhakar
"Great Write-Up
Amazing & Inspiring
Congrats & Thanks for Sharing @kyakovlev ",114345,2019-10-26 03:12:27,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
"Congratulations for your awesome journey. I am sure you will work very soon with the brightest minds in this domain. Kaggle will be with you.
Thank you for sharing your best practices and learning path. This 2 hour + 2 hour is something new for me. I always feel overwhelmed with the discussions and public kernels. May be I should spend more time reading those.
One question. Do you follow or participate in multiple competitions together? Or do you follow multiple competitions, but actively participate only in one? Please let us know.",114345,2019-10-26 04:10:02,arnab
"
Do you follow or participate in multiple competitions together? 

I don't. During this year I've dedicated time to one competition. But it doesn't mean I don't read kernels and discussions on others competitions - there is a lot of new techniques and interesting ideas that could be applied for different tasks.",114345,2019-10-26 15:55:27,Konstantin Yakovlev
A great journey full of falls and achievements with the final great growth we have recently seen. ,114345,2019-10-25 22:39:15,Mukharbek Organokov
Great writeup! Congrats and thanks for sharing it.,114345,2019-10-25 21:42:36,William Cukierski
Great write-up. Very inspiring! ,114345,2019-10-27 06:33:52,Manraj Singh
This Comment was deleted.,114345,2019-11-04 11:23:10,No user
"Recently created a kernel with timestamp auto-alignment attempt, who is interested - check this out! As far as I see, it improves correlations with log-target, however, I didn't check it in submissions",114483,2019-10-27 17:55:11,Fred Navruzov
"I've found a cluster of low meter reading datapoints for May 2 and May 30 for siteid=1 (building 123). 
Those are Early May Bank Holiday and Spring Bank Holiday in the UK!",114483,2019-10-26 20:57:10,S D
"I just located most of sites according to external dataset from kaggle.
(Already post in ""External Data Disclosure Thread"")
Here's the notebook:
https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature
Hope it helps!",114483,2019-10-30 02:39:19,patrick0302
It seems that you discovered something interesting …,114483,2019-10-30 18:50:14,mezoganet
"Also, potentially, the timestamp for weather data is not aligned with the the timestamp for meter usage. Discussion here
Could make sense to start with buildings from sites 1,5,12, where the peak temperature occurs close to what we would expect for local time.",114483,2019-10-26 17:44:12,S D
"Once you make a determination of what time zone you think the site represents you can convert the weather data to local time (rather than UTC).  When you do that the peak daily temperature falls where its suppose to be at around 3 PM.  Spent over a day converting both train and test weather to local.  Gets messy for start and end of year where you can end up with missing values.  As is normal however I was very impressed with the power of catboost and lightgbm to figure that out all by themselves - cv actually got a touch worse when I use time zone corrected weather data.
Nice catch on the resolution of readings leading to Canada vs US.  There are properties of solar loading were it helps to have a latitude guess.    
Any guesses about the three European cities ?",114483,2019-10-27 01:21:59,PC Jimmmy
"I'm hoping the Kaggle team will fix the time zone issue…
I think site 1 is UK based on the holidays that I've seen (May 2, May 30, Aug 29). Haven't looked at the other ones yet. Probably they are all in the UK?",114483,2019-10-27 01:41:14,S D
"@pcjimmmy could be because, if you aligned weather timestamp, but not the train.csv, then after the merge the meter readings could be aligned to the wrong weather?",114483,2019-11-15 07:55:29,Sidhant Sundrani 
"Great insight ! Thank you so much !
Sure from the beginning that we have to deal with temperatures in °C and °F… this is a real help.
But now, are every temp comparable in dew and air temperatures ?",114483,2019-11-11 11:40:36,mezoganet
"To be clear, all temperatures are in °C; it's just that some of them have been converted from °F, while others were already in °C.",114483,2019-11-11 14:43:50,S D
"it helps me a lot,thanks four your shares",114483,2019-11-11 10:21:20,bingo
"I think all  temperature values are in Celsius .  The values for air temp are in the range of -30 & +40 which has to be celsius readings.  My kernel  here has a section on  Weather trends sitewise which plots the daily min,max and mean temperatures sitewise .",114483,2019-10-29 14:48:00,Rajesh Vikraman
"I agree. 
And as I've said in the original post, a lot of the temperatures have been converted from F to C.",114483,2019-10-29 15:19:58,S D
"I still don't get it. How do you assume values to have a offset of 14 for site 1, 5, 12.  Take a look at my mean highest average temperatures.  They all have the range from 19 to 22 offsets. And averaging subdues any misreadings if the samples are large
",114483,2019-10-28 14:19:42,VivianTitus
"My bad.  Looks like skipping nan values by condition instead of grouping led to the above chart
Site ID:0 == Offset:19, MAXAvgTemp:27.518579
Site ID:1 == Offset:14, MAXAvgTemp:14.872055
Site ID:2 == Offset:23, MAXAvgTemp:30.457377
Site ID:3 == Offset:20, MAXAvgTemp:19.161096
Site ID:4 == Offset:22, MAXAvgTemp:19.107104
Site ID:5 == Offset:14, MAXAvgTemp:13.081967
Site ID:6 == Offset:20, MAXAvgTemp:19.801370
Site ID:7 == Offset:20, MAXAvgTemp:11.457423
Site ID:8 == Offset:19, MAXAvgTemp:27.518579
Site ID:9 == Offset:21, MAXAvgTemp:26.395902
Site ID:10 == Offset:22, MAXAvgTemp:16.366940
Site ID:11 == Offset:20, MAXAvgTemp:11.457423
Site ID:12 == Offset:14, MAXAvgTemp:12.290137
Site ID:13 == Offset:21, MAXAvgTemp:13.173770
Site ID:14 == Offset:20, MAXAvgTemp:17.467213
Site ID:15 == Offset:20, MAXAvgTemp:13.588202
",114483,2019-10-28 14:55:09,VivianTitus
"Glad you were able to confirm my findings, thanks for producing the charts!",114483,2019-10-28 15:08:42,S D
"Hey those charts are neat. I suggest you replot the subplots with shared x,y coords: fig, axes = plt.subplots(6, 4, sharex=True, sharey=True). Seems like the ylim for temperature will be +4..+30 F",114483,2019-11-11 04:12:45,Stephen McInerney
Thank you for sharing your edged intuition.,114483,2019-10-27 22:05:52,Hanjoon Choe
Very inspiring investigations! Good job!,114483,2019-10-26 23:52:23,Fred Navruzov
Great Sharing! Thanks!,114483,2019-10-26 17:32:54,Manraj Singh
This Comment was deleted.,114483,2019-10-27 11:05:13,No user
"I created a dataset with the findings of this thread and various other threads. You can import it into your kernels now! I will only change the locations if there's a confirmation from building data that we have the exact location correct. Otherwise I consider these ""good-enough"". I'm planning on making a dataset for when daylight savings starts and ends soon, but I need more time for that one.",115698,2019-11-16 04:00:14,Rob Rose
"
I think all of them except site 8 are universities",115698,2019-11-05 10:19:35,Gunes Evitan
"Great job @poedator 
It's really a shame the organisers are not transparent about the data. It just generates a lot of work for all participants, which is questionably part of data science. Thanks for sharing!
If the @kaggleteam actually want to protect their data providers they should use differential privacy.
Note: I agree with @kyakovlev , site 0 (and any other public available meters) should be removed from the Public and Private LB evaluation.",115698,2019-11-05 12:14:11,Henrique Mendonça
"Hi guys, I'm late to the party but thanks for the great detective work 🕵️ 
I've summarised your findings into this CSV file which has the correct codes for applying time zone offsets and holiday lookups with your favorite packages time_zones.csv (597 B)",115698,2019-11-12 05:26:52,datasaurus
"Good info. But should be shared with organisers first. In my opinion any public available data should be excluded from test dataset.
My concern is UCF - as meter readings are publicity available. 
Predicting that at least public LB will be ruined soon if this data won't be excluded.",115698,2019-11-04 18:58:24,Konstantin Yakovlev
Summoning @addisonhoward to this thread)),115698,2019-11-04 19:10:28,Konstantin Yakovlev
"We're aware, not to worry. Just remember to keep the external data thread up to date as needed.",115698,2019-11-04 19:24:36,Sohier Dane
So site 0 = UCF confirmed?,115698,2019-11-04 19:39:22,Gunes Evitan
Only lb? Private test is also there. ,115698,2019-11-04 20:12:36,Oleg Knaub
Organizers said that there are only 22% of data in private part. Probably all public data is excluded or they may change private set validation methodology as was done in Santander competition (all synthetic data in that case was excluded from validation).,115698,2019-11-04 20:20:06,Konstantin Yakovlev
I already have a mention in the public info thread but will expand it to mention these sources explicitly.,115698,2019-11-04 20:31:48,Poe Dator
"
1: UK. Most likely Southampton
  The oldes building of University of Southampton was constracted in 1914. Energy consumption of it is open.
  I think site1 is London. It is written in Building Data Genome.

building_id == 111 looks like:
",115698,2019-11-06 11:55:48,Mike Nikonov
"This became a Internet search competition ;D
Thanks for sharing guys!",115698,2019-11-06 12:01:06,Henrique Mendonça
"Mike, you may be right, but I was unable to match building 111 (Education, built in 1909, 118338 sq f.) with the Genome database.  
Do you have the stats for the Rockefeller Building at U College of London?
The older buildings in Southampton may be coming from outside the University of SH. Or Site 1 may be quite large about to include more than 1 city.",115698,2019-11-06 16:23:32,Poe Dator
I was wrong. The Rockefeller Building at U College of London was built in 1907.,115698,2019-11-06 17:54:55,Mike Nikonov
,115698,2019-11-06 21:51:36,Mike Nikonov
"This looks like a very possible match. It is one of those that I mention in the initial post. Note, however, that Europe/London is not a location but a time zone, so the question of identifying the specific location in UK is still open. ",115698,2019-11-07 07:07:55,Poe Dator
I can see that building on my way to work :D,115698,2019-11-12 16:26:53,Dan Ofer
Site 2 data leak / https://cm.asu.edu/,115698,2019-11-14 17:58:59,ym
"Hi Poe Dator, 
which buildings could you identify in 3? I just checked about 10 of the buildings but found none of them in the pdf.

3: DC
https://dgs.dc.gov/sites/default/files/dc/sites/dgs/service_content/attachments/Attachment%20A-%20Municipal%20Facilities%5b1%5d.pdf

Is this really a excel sheet that has been printed and scanned as an image afterwards? 
That's really extremely odd :-(
EDIT: ok got some matches, it really looks like a perfect match because also the facility description matches, so it's unlikely it's just a coincidence.",115698,2019-11-14 08:18:30,Jott Be
Corrected Site 11 guess in the first post. Of course this is Montreal/Ottawa area based on weather match.,115698,2019-11-05 10:03:05,Poe Dator
Looks like this competition is going to become ASHRAE - Great Energy Leak Detector III. Guess it's a hard pass for me.,116773,2019-11-11 12:20:34,Thomas Yokota
"Me too, I'm out until the organizers say something.",116773,2019-11-12 07:05:29,Max Halford
"I already have the labels for site 1 too, so it's a matter of time people will discover them. I'm just curious about what will happen to this competition at this point.",116773,2019-11-11 12:11:24,Gunes Evitan
Thanks Gunes for sharing this finding! Hope organizers will be honest in assessing of you in this competition on really private data.,116773,2019-11-11 12:16:01,Dmitry Labazkin
Another open data service such as southampton … I'm also wondering if it's worth to continue.,116773,2019-11-11 22:39:10,MPWARE
"Appreciate your impressive findings.
You really did a good job in this complex competition.
Hope to see you soon in other competitions!",116773,2019-11-11 12:10:00,patrick0302
Thanks Patrick! ),116773,2019-11-11 12:13:02,Dmitry Labazkin
"Hey buddy thanks for your findings.
And perhaps you're making the right decision to quit this one 😔 ",116773,2019-11-11 11:35:59,Jie Lu
"Thank you!
I feel that for me it is correct decision :)
Good luck!",116773,2019-11-11 11:39:17,Dmitry Labazkin
"Hello, this is my first competition as well and before starting to focus on it, I am seeing that there are quite a few problems. 
This is probably going to be a very naive/noobie question (so apologies in advance!): given the situation,  that some building's test data are leaked, but that to use it, it has to be public (otherwise I assume the team would be disqualified), what use can people do of such leaked data? I think that it's only useful as an extended train set or am I missing something obvious? 
Of course it's bad but in the end, in theory, everybody is going to have the same data available. Another assumption I'm doing here is that scores getting prices will have the code/models reviews or even reproduced, and therefore finding other leaks and not making them public is not possible.
Dmitry thanks a lot for your sharings, they are very insightful and it seems like you invested a fair amount of work! Why not giving it a try to you current model just training when the external data disclosure finishes? (including that extra data for training).",116773,2019-11-15 08:32:21,rpicatoste
It’s great that it is helpful! About competition - I don’t plan to continue),116773,2019-11-15 20:50:47,Dmitry Labazkin
"I compare ASHRAE data and DC data you've mentioned. However, I could not find any correlation between meter reading values for these data sets. I even applied timestamp transformation to DC data set.",116773,2019-11-25 07:37:13,Sefik
"I remember that I checked several sites by comparing plots in 1 hour scale, and as I mentioned before, there are gaps in this data from github as if it is corrupted data, but these plots were similar. I think these gaps can be the main reason why correlation can be low.",116773,2019-11-25 08:15:22,Dmitry Labazkin
I realize gap but still none of buildings have a correlation higher than 0.7 and mae /mean ratio less than 10% even null values dropped.,116773,2019-11-25 08:42:38,Sefik
"Maybe I'm lucky and building that I've checked has similar plots, but I found this similarity for several buildings that I checked. I have not been participating in competitions for 2 weeks, so I can't tell more details.",116773,2019-11-25 09:24:41,Dmitry Labazkin
"@labdmitriy Impressive findings! Thanks for sharing.
It seems quite a lot of effort for me to breach 1.07 without leaks on a single model, much less 1.04. Without leaked data, what is your single/blended model scores on LB? How many features would it have? ",116773,2019-11-18 02:02:58,HyperSeedOptimized
"Hi @gideonteo , at the moment of this post I had 1.09 single, 1.07 blended models, single model was with ~15 features ",116773,2019-11-25 08:19:56,Dmitry Labazkin
"Thank you Dmitry for putting your findings together. 
I just tried Catboost and it does seems to increase accuracy. 
Thanks for sharing :)",116773,2019-11-12 15:58:45,Vishal 
Great!),116773,2019-11-12 16:37:53,Dmitry Labazkin
"I mean, in theory everyone will have to disclose any leaks they find right? So you can just sit back, monitor the data disclosure thread and then integrate everyone else's findings yourself. The competition winner will be the person who can use the leaks most effectively. 
I'm half considering maybe taking a break for a few weeks and coming back once most of the leaks have been found and disclosed…",116773,2019-11-12 03:36:18,Louka Ewington-Pitsos
We have to share otherwise it will be unfair for everyone.,116773,2019-11-12 04:23:55,Gunes Evitan
taking the time to mirror image the profile picture is my favorite part 😆 ,120398,2019-12-05 19:53:26,Rob Mulla
Yeah I am referring to duplicate accounts made.,120398,2019-12-05 19:58:04,Ashish Gupta
What are you guys talking about? This is clearly John's evil twin Doe :D,120398,2019-12-06 05:44:52,Gunes Evitan
Also the effort of setting up two different accounts and making just 1 submission from each! ,120398,2019-12-06 08:19:13,Panos
"
Gunes Evitan wrote:
What are you guys talking about? This is clearly John's evil twin Doe :D

It's Doctor John and Mister Doe ^^",120398,2019-12-10 16:51:48,Serigne 
"@serigne 
Or both of them ? Who knows ?",120398,2019-12-12 19:43:52,mezoganet
how many cheaters in this competition in your opinion ?,120398,2019-12-10 16:16:54,eagle4
"As many cheaters than Kaggle can bear… This is gonna be « cleared » soon, of course.
Sent by Dow Jones #1 ;-)",120398,2019-12-12 19:37:34,mezoganet
Interesting that all of them combined would not hit any day's daily limit. What's the benefit of making multis?,120398,2019-12-06 11:32:05,HyperSeedOptimized
"So that if an experiment fails in one account, Doe John then Doe John #2 submission could be saved. :)",120398,2019-12-09 22:11:35,Ashish Gupta
"Just Kidding. But yes, it doesn't makes sense",120398,2019-12-09 22:12:01,Ashish Gupta
"I imagine there are several more multiple accounts, the daily submission level of 2 has the side effect of encouraging such behaviours. I hope that Kaggle´s algorithm can identify most of them!",120398,2019-12-06 08:13:49,Panos
"I Don’t agree : 2 submissions a day is the rule…accepted when entering the competition. If one doesn’t accept it, he can watch TV.",120398,2019-12-12 19:42:32,mezoganet
I would say that Doe John #2 seems to win ! That’s my conclusion based on scores they (he) did.,120398,2019-12-12 19:47:00,mezoganet
"Thanks for this glimpse ! Allways funny to see that, competitions after competitions.",120398,2019-12-12 19:39:33,mezoganet
"After 73 years of life its been my experience that most actions intended to ""punish"" most often end up hurting the 95% of good folks while being only a minor pain to those who needed the punishment.  This  seems to have been the case through most of the history of time - according to many - all of us have been punished because one person ate the apple.
In case you have not looked out your window for some time - the world is full of cheaters.  Aside from waiting for the invention of Skynet - try not to throw the baby out with the bath water.",122503,2019-12-27 04:01:28,PC Jimmmy
I'm novice 3 days Kaggle and would like to know why people is cheating? Thank you,122503,2019-12-26 22:59:46,Arboleda
"Only been around for two years but here are the reasons I would be tempted to take a bite of the apple.

Submissions limited to 2 (or 5, etc) when the nature of the data lets you generate a lot more submissions that you would love to check.  Cheating by duplicate accounts lets you do more submissions.  This sin the result of Kaggle restrictions on limit of number - likely a fix to previous cheat or limited because of Kaggle hardware restrictions.
Fairly new reason - only 30 hours of GPU time per week.  If you don't have your own resources than a duplicate account cheat is way to increase GPU time.  This sin clearly the result of Kaggle decision to reduce hardware costs and likely a detectable bump in number of duplicate accounts as a result of this change.
Sharing between individuals who are not part of a team.  Would guess that this is the result of penalty team encounters where number of submissions for team is same as for a individual.  Would guess the team submission limit was a fix for previous crying game where individuals felt teams had a huge advantage.  If you do plan to join as team than other reasons may exist.  For me I would have loved to have given a potential team member a try-out.   Sharing before forming a team would help - maybe just me, but I have had a high rate of failure for new team members being duds.  If you have a very good score there is some potential that you will cheat by sharing the submission/kernel with friends.
Size of the prize - not a lot of incentive to cheat when the first prize is $5000.  1 million dollars is a whole other story - having large resources of man and machine improves odds of success.  This one is a hard edge case - when does ""extra"" resources cross the line or does a line even exist to be crossed.  Deepfake will have professional teams and resources working on it - is that cheating?  IMO its not - I would go so far as to suggest that Kaggle wants to see more professional teams but that's getting off topic.
plagiarism - a number of comments in this discussion topic using this word.  I have some issues with the words ""open source"" and ""plagiarism"" being used.  After almost two years my skill set still very heavy on the cut and paste method.  If you don't want your code used by others - than don't fri..ing share it is my motto.   But I do admit to getting peeved when I see someone post an almost duplicate kernel and getting lots of up votes.   Not sure who I most peeved with - the person doing the posting (who may be new and unknowing) or the bunch of folks who up vote.  It seems that the ""cheating line"" exists here with regards to giving the original  author full credit.  The reason to cheat of course is to build your resume.  The concept being that having a high Kaggle rating gives a good opportunity for jobs and/or peer praise.  Having no friends and being too old for a new job makes this the only reason that does not tempt me.  

OK - I have run out of steam - but think this covers 80% of the reasons to cheat.   Did I miss any biggies.",122503,2019-12-28 00:57:59,PC Jimmmy
"Kaggle provides such a great platform for Data Science enthusiasts and if someone cheats here, that person should be ashamed of himself/herself.",122503,2019-12-25 16:06:43,Prithvi Sharma
"And so what ? 
Ashamed with no sanction ?
That’s not a way ro punish… ",122503,2019-12-25 17:27:15,mezoganet
"Simple cheating can be treated probably but the following ones may be a lot harder:

private sharing of engineered features or sharing between people that have decided to team up at competition start.
having a team of DS working for you 
access to big clusters of CPUs/GPUs and/or ML softwares that give insights on features (most competitions require OSS).

Finally and most important of all, only winners have their solutions checked against the rules, the rest of gold, silver and bronze don't… That's an interesting bias between the last in money and his follower.",122503,2019-12-23 08:18:14,olivier
"@ogrellier, I think Kaggle should adopt a rule of randomly checking the solution of a few people who are outside the prize range. They do not even have to tell us how many in the top-N solutions they check randomly. This can serve as a deterrent from engaging in some of these activities.",122503,2019-12-23 12:25:10,YaGana Sheriff-Hussaini
@sheriytm I would check all gold at a minimum… but that takes time.,122503,2019-12-23 15:40:34,olivier
"@ogrellier, I agree but they need to go beyond that which is why I propose a random check. That will make the gold zone even more nervous than the rest since it is so hard to get there.",122503,2019-12-23 15:45:20,YaGana Sheriff-Hussaini
"How would solutions be verified?
If you want the same process as for prize, then it requires substantial work from the participant.  What if participant do not want or cannot spend the 2 man weeks of work to clean code, document it, make a presentation material, and make sure the code can be used to replicate the solution?  Esp if you're a random pick near the bottom of LB.
I reacted to other comments in this topic for the same reason: while I fully support the intent, as cheaters are negatively impacting the fairness and interest of Kagle competitions, I disagree with the single sided and too simplistic cures to the problem that we can read here and there.  
Limiting to gold, and asking gold medalist to provide evidence seems a reasonable trade off to me. Anything beyond is both a toll on Kaggle and on participants.
And don't forget that Kaggle does check suspicious accounts, and we can see they remove people all over the LB, not just near the top.",122503,2019-12-23 16:05:08,CPMP
"
Esp if you're a random pick near the bottom of LB.

@cpmpml, I do not mean the random check to include the bottom of the LB. Just a Top-N number  of X teams to be selected randomly to check. That X can be based on an percentage M of the number of participants. And Kaggle do not need to tell us what the N is in the top-N.
Having said that, I agree that it will be a toll on Kaggle but they do need to do something.
As for a randomly selected participants not wanting to put in the work required to submit their code for review is our contribution to the work Kaggle team is willing to do. If one is selected and felt they do not want to do that work, they can forgoe their position in that comeptition. If what I propose is communicated to the community as the new rule of the competitions, we can abide by it.",122503,2019-12-23 16:38:49,YaGana Sheriff-Hussaini
Maybe they need a competition on here to find ways to identify/deter cheaters.,122503,2019-12-23 17:27:53,Chase Wells
That will be overcrowded with cheaters :),122503,2019-12-23 18:54:08,Vladimir
Fully agree! Unfortunately private sharing is often hard to prove.,122503,2019-12-21 22:17:53,Carlo Lepelaars
"
use of external data not allowed or not disclosed on the related topic etc.) 

I believe this might be even more tricky to detect unless they are required to share a reproducible solution. ",122503,2019-12-23 01:14:15,Sanyam Bhutani
"Yeah. Regarding kernels, we caught someone on big time plagiarism a while ago. He eventually made those kernels private. He now has a suspiciously high position with few gold medals. Besides the fact that he should have been punished, this high position is also still very annoying (I think the ranking points ""gained"" on those cheated kernels made private still count for the ranking…..).
While his rank is fading (he changed his name but rep still damaged), he got away with this way too easy.
Link to the discussion (Gamification Of Kernels Is Discouraging): https://www.kaggle.com/product-feedback/80571",122503,2019-12-21 10:46:12,Erik Bruin
"@erikbruin  I missed that discussion but yes, there is huge gamification on kernels. 
A current kernel Grandmaster plagiarized  almost all my kernel in house prices  without direct fork and without giving me any credit. Then he won easily kernel gold medal by refreshing it every day. 
At first time I didn't complain. But reading later the comments on his kernel, someone asked him how did he optimize the models' parameters . He said he did it locally on his PC. 
I was stunned to read that and I said  ""Ok it's enough ! Things are going too far here"". I told him clearly in the comments: ""Not only  you plagiarized my kernel without crediting me but also you publicly claim to be the original author of my work"". 
He deleted the kernel once he become GM. But you can still see most of his kernels are just mixed plagiarism of medium posts, online courses and others kernels without any credit. ",122503,2019-12-21 12:19:31,Serigne 
OMG…I think I know who you mean. There is only one GM with that kind of kernels. Never trusted him. Will send you a PM when I get home to check the name…,122503,2019-12-21 13:10:53,Erik Bruin
"Edit: wrong people, sorry!",122503,2019-12-21 16:07:09,phi
"@serigne, I think I know who u are talking about. I just checked and he has deleted some of his kernels I have seen before.",122503,2019-12-21 16:58:43,YaGana Sheriff-Hussaini
"Those ""getting started"" competitions are home of plagiarism. I started Kaggle with those competitions and my Titanic kernel is very popular. I see copies of my kernel from different users every week.",122503,2019-12-21 17:47:40,Gunes Evitan
"Are you referring to a guy who can seamlessly achieve gold medal when he publish kernel? I'm quite impressed that he achieve such perfect gold rating (100 %) accuracy. Why most of them have become ""Plagiarist"" Grandmaster! :D",122503,2019-12-22 03:44:22,devai01
"I think most people are on the right track, except for phi. I just checked and the guy that I have in mind (I don't know for sure yet but I would be very surprised if Serigne talks about a different guy) has never teamed up. Devai01, I would like to stress that we are talking about ""only' one GM. I am almost certain that all other GMs are playing it very fair.
On top of that I am aware of 3 cheating Masters (including the one that you can find out about in the link). That leads to about 5% cheaters in kernels Master+.  No, that is not what we want but also means that about 95% is playing it fair…..",122503,2019-12-22 08:15:00,Erik Bruin
"@erikbruin: Yes, you are correct. I mean those who commit plagiarism must have the title ""Plagiarist"" Grandmaster!",122503,2019-12-22 10:27:36,devai01
"Essential guides for plagiarism
",122503,2019-12-27 14:16:38,devai01
"but is cheating really that big of a problem on Kaggle? I am rather new here, but it seems as though most of the time the first place/top solutions are not some miracle/beautiful mind/golden feature shit but rather lots of very meticulous effort?
take the solution of the first guy on the lb for example:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122796 
he got to first place with barely any features (something that was discussed multiple times), validating similarly to the half and half kernel (like the most popular kernel this competition) and mainly by removing outliers (something that was also discussed in depth on the forum). It seems as though all the information needed to reach rank 1 was out there in the open, there really wasn`t any need to cheat. All the leaks got published too as it seems, as the rank 1 guy only used the ones publically available.",122503,2019-12-23 08:39:23,Proletheus
Who is the downvoter? :),122503,2019-12-20 17:51:51,Psi
Let's hunt that guy! 😂 ,122503,2019-12-20 21:25:16,Xuan Cao
It’s not me!,122503,2019-12-20 23:31:45,Kha Vo
"@robikscube 
Rob Mulla knows how to do that ;-)
https://www.kaggle.com/robikscube/identifying-potential-fake-accounts-on-kaggle",122503,2019-12-21 11:06:19,Erik Bruin
I want to see removed teams exposed,122503,2019-12-21 09:42:31,Gunes Evitan
"My team got removed once because a team mate used several accounts. Of course neither I nor my other team mate knew about it.  We found out when our team was removed from gold zone in private LB after competition end.  Do you think it would have been fair to ""expose"" us?",122503,2019-12-21 19:24:58,CPMP
There are cases like yours as you said which would be unfair. How about listing accounts associated with each other? This way teams can find out who broke the rules.,122503,2019-12-21 19:37:36,Gunes Evitan
We easily found who broke the rule as the one who did it said it publicly and apologized for it.  It was courageous of him.,122503,2019-12-21 20:39:34,CPMP
"I remember the comp 😞 
I don't know if it'd possible to do it automatically or how much of manual intervention would be needed from the kaggle team-removing 1 guy, not the entire team, etc. if they were to add stricter ways of dealing with cheating. 
I know Kaggle doesn't mention it publicly but they might already be working on improving (already good) cheater removals (?)",122503,2019-12-23 07:31:32,Sanyam Bhutani
"@cpmpml, your experience in that competition is etched into my memory. Because it made me even more shy about teaming up with people. I was just about getting comfortable towards joining a team when that happened and I had to rethink. 
It is really sad that people break the rule knowingly hoping to get away with it. Kaggle definatly should do more. Perharps randomly check the solution of a few people who are outside the prize range. This may help deter teams from engaging in some of these activities.",122503,2019-12-23 10:38:07,YaGana Sheriff-Hussaini
I'm a newbie on kaggle and even I experienced the same thing ,122503,2019-12-22 07:04:30,ravi tanwar
That's definitely a way to go. I guess something went wrong in the gamification of competitions. It's insane when you wonder that almost half of the teams won't be in the private LB.,122503,2019-12-21 06:47:14,Rasselio D.
Agreed.,122503,2019-12-20 16:15:35,YaGana Sheriff-Hussaini
"« More drastic and deterrent actions against cheaters would be something like malus on (global) points/ranking… »
Fully agree !",122503,2019-12-20 15:48:46,mezoganet
"💯 % agreed. Without a credible threat, the instances of cheating are only going to increase over time.",122503,2019-12-20 15:39:44,sandy1112
Wow didnt know such things exist,122503,2019-12-27 16:39:13,Alvin
Nice,122503,2019-12-26 21:04:34,Caleb Lawhorne
great,122503,2019-12-26 09:18:34,qianerba
Yes,122503,2019-12-26 09:06:09,Sayam Rakshit
I think they can start by adding the RULES,122503,2019-12-25 15:05:21,HishamElamir
Agreed.,122503,2019-12-25 09:38:04,Benjamin
Competitions should be Kernel only and no outside data allowed.,122503,2019-12-24 15:57:48,Macit Giray Gökırmak
Doesn't solve anything. External data can be copy pasted to notebooks.,122503,2019-12-24 16:17:45,Gunes Evitan
Kaggle can quite easily check that though.,122503,2019-12-25 10:25:39,Psi
"Why would you copy paste ? Fitting a model using test ground truth would be enough to tune your models and use the parameters in a kernel. 
Kaggle team does not ask you how you tuned your models and well you can be lucky with tuning.",122503,2019-12-26 21:17:09,olivier
"First of all , everybody is learning from everyone , i hope author also did in past. I agree if someone are taking help of anyone's material (code + content),  that guy should give credit to genuine account .  Don't cheat other's hardwork , give credit to him also . ",122503,2019-12-24 15:18:58,Manish Nath choudhary
I agree with you. Kaggle should block such people.,122503,2019-12-24 14:40:42,Amitr
Here also cheaters exists? ,122503,2019-12-24 12:11:16,SarkarPratap
Agreed,122503,2019-12-24 11:12:24,Paras Chaudhary
true,122503,2019-12-24 11:03:28,Utkarsh Bhadauria
Totally agree!,122503,2019-12-23 15:29:44,Shintaro Ono
"Yes, I agree with you.",122503,2019-12-23 11:36:28,Satyam Prasad Tiwari
"I totally agree this is completely unacceptable, especially for beginners. Personnaly I find it frustrating the fact that on some starter competitions there are like hundreds of teams/persons that only submit the answers of other previously shared notebooks or they use the ground truth solution and just mix it with a % of any of their model and still end up with greater score than any score you can get with only using machine learning models. I'm taking the example of the house pricing regression competition same goes for the titanic competition you have a lot of people doing this 0 rmle or 1 recall score. I think those participants should be removed so one can really evaluate how his model doing. At some point, I was kind of depressed because no matter what I was doing I was not getting better in my results. And I refused to see the published notebooks. Until one day I had enough and I really thought I was no good. Seeing that most notebooks take into account the real answers to get better scores and finding a topic that the real answers can be found in another previous competetion, I was relieved. But still I think they are to be removed because now I'm satisfied with my results but obviously they do not reflect how well I've done. And when someone will look at them, they won't probably know that others cheated. ",122503,2019-12-23 05:28:34,omarB202791
Because of them others deserving people's chances are going waste . Strict actions must be taken.,122503,2019-12-23 05:15:50,Namit Dubey
"Agree, even tho I haven't experienced it.",122503,2019-12-22 20:39:30,MQ37
"The problem with all this cheating that should trouble Kaggle is that as as serious competitor you feel less and less willing to put serious efforts in a competition if the discriminant is not anymore ""try all you can by doing your best"" but being in the right circles and be smart in ignoring and circumventing rules. ",122503,2019-12-22 09:41:39,Luca Massaron
"FYI, more and more in the DS community has stopped seeing Kaggle ranking and competitions as a credible platform due to cheating and issues brought up in this thread. It's a sad reality, and I hope the Kaggle admin team will do something about it to ensure the integrity of the competitions.",122503,2019-12-21 22:02:43,JennyYu
"@erikbruin  thanks for sharing the link to that discussion. It looks like I was living under a rock for so long…looking at that this discussion and the few profiles that everyone has been pointing at, the Kernel medals system does seem to be prone to exploitation in it's current form. The competition and discussion rankings are more robust in that respect (although the issue of cheaters needs to be addressed to clean up the Competition progression system as well). ",122503,2019-12-21 17:23:26,sandy1112
This Comment was deleted.,122503,2019-12-25 09:28:16,No user
"Thanks for sharing your great solution, @hiroyukiex 
I have been impressed by your work throughout this competiton 😄 ",122796,2019-12-27 03:40:16,Isamu Yamashita
"Thanks for sharing this.
How do you find the optimized weight for your ensemble?",122796,2019-12-27 20:06:49,zhuyujie
"Based on site0,1,2,15 leakage and public LB.",122796,2019-12-27 23:49:07,Hiroyuki Namba
thanks,122796,2019-12-25 02:27:51,Taco
Thanks for sharing. Good luck with the final result.,122796,2019-12-24 22:03:03,tensorbot
Thank you @hiroyukiex for the solution.,122796,2019-12-24 03:48:19,Sabin Hashmi
"wonderful solution, thanks for sharing @hiroyukiex , I would love to hear more for the outlier detection rules and those 32 lgbm models that you used for ensembling",122796,2019-12-23 18:17:26,Firat Gonen
"Most of my outlier rules are so complicated and difficult to tell strictly, but they are mainly based on ""constant-segment-info"" and ""day-mean"".
[constant-segment-info]
-To deal with constant outliers.
-Extract (nearly) constant segments with segment features.
-Examples of segment features are startday, endday, length, countofsegmentsinthetimeseries, countoflongsegmentsinthetimeseries, countofsegmentswithsamestart_day.
[day-avg]
-To deal with non-constant outliers.
-day-avg stands for Groupby([""buildingid"",""meter"",""day""])[""meterreading""].mean()
-Also used day-group-avg (group represents 15days interval)
-day-group with definitely and weirdly small values are removed.",122796,2019-12-25 00:59:34,Hiroyuki Namba
"thanks @hiroyukiex , I really think the outlier removal has made the huge difference, good luck",122796,2019-12-25 05:42:08,Firat Gonen
Congrats! How did you combine the ensembles' outputs? Mean? Median? ,122796,2019-12-23 14:48:31,Fernando Wittmann
"8 predictions combined like the figure.
-Simple mean in terms of 4 red predictions (multiply 1/4).
-w is optimied (Please read [Ensemble] section in details)

In fact, each red prediction is created from 4 models, but almost no effects.
I think only 8 models are enough rather than 32 to get near-optimal score.",122796,2019-12-25 00:22:26,Hiroyuki Namba
Got it! Nice approach! ,122796,2019-12-25 12:44:49,Fernando Wittmann
"Great job! Mean and std value of air and dew temperature also worked in my best single model, and I found that spliting data into month groups performs better than just KFold.  However, I found that 6 groups is better than 4 and 2, while your best single model is 2 groups.  Look forward to your code part in notebook!! ",122796,2019-12-23 12:26:49,Clancey_Lee
Great job. And how did you train the 32 LGBM ? What's the difference between each other ?,122796,2019-12-23 09:08:21,Changyi
"2×4×2×2=32
2 -> [1-6] train model or [7-12] train model (important)
4 -> 4 random seed average (effective)
2 -> Remove all suspicious outlier or remove only obvious outlier (slightly effective)
2 -> Modify site0 electric units or not (almost no effects)",122796,2019-12-23 12:01:16,Hiroyuki Namba
great solution,122796,2019-12-23 00:24:38,Neo Zhao
"I recommend reading page 345 from volume 7 upside down, I found a ton of information",112850,2019-10-18 13:15:54,Francisco de Abreu e Lima
"There are just 342 pages.
345 is back cover. Nothing new there.",112850,2019-10-18 13:27:34,Konstantin Yakovlev
"Haha, beautiful! ",112850,2019-10-25 18:56:30,Carlo Lepelaars
220$ book when Kaggle is free? :D,112850,2019-10-16 05:37:39,Adrian Zinovei
"Yo Ho, Yo Ho! A pirates life for me 😄 ",112850,2019-10-18 05:04:22,Rafid Abyaad
"Just to be clear, this is a joke. Please don't actually spend the $1000+ on the full set of books. They are phenomenal guides to everything in a building but you don't need any of it to compete in this competition. ",112850,2019-10-15 23:11:35,ryches
"Died when I got to ""so best to pace yourself.."" 😂 ",112850,2019-10-15 23:31:26,jlapaix
😂 ,112850,2019-10-17 23:38:05,Rizki Teguh Kurniawan
😂 😂 ,112850,2019-10-15 21:45:35,ilovescience
"I have implemented your temporal convolutional networks at my job in forecasting electricity usage, and it outperforms lgbm. I highly recommend people read his blogs & github. Thanks jeddy",113007,2019-10-16 18:37:23,CoreyLevinson
"thanks for sharing.
PS: there are no bad architectures, could be worse data science knowledge. That's why we are here. ",113007,2019-10-22 22:36:24,Adrian Zinovei
"Thanks a lot Joe, as every time !",113007,2019-10-22 20:07:13,mezoganet
Nice! Thanks for sharing!,113007,2019-10-21 13:52:32,Carlo Lepelaars
"Great.. Very Helpful
Thanks @aquatic ",113007,2019-10-16 16:49:14,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
"I read your github too, that is a great tutorial!",113007,2019-10-19 03:57:16,LongYin/杰少
So if I understand correctly this method does not use any explanatory variables other than the past values of the variable we want to predict? Do you think this could work in our case where the prediction horizon is longer than a year?,113007,2019-11-20 15:32:39,Benedetto Grillone
"@aquatic Thanks a lot for your tutorial.  
I also highly recommend checking out Sean Vasquez's model that was designed for this data set. He implements a customized seq2seq WaveNet architecture in tensorflow.  Would it be possible to make customizations in Keras as @seanjv did in TensorFlow?",113007,2019-11-06 21:59:28,Ad hoc
"Hi, your github toturial is great,  i'm curious about wavenet performance in this competition, would you share wavenet PB score？",113007,2019-10-30 07:19:41,EvilPsycho
Thanks for sharing :),113007,2019-10-23 02:21:13,ANAPARK
"As others have seen, most buildings in site 0 has either zero or small readings for the first 5 months. Because of the similar pattern and the same geographical site, perhaps the data is collected by the same party in a similar way.

I think it is likely that at about May 2016, they either start to take measure or start to use a new way of taking measure of electricity usage, which will persist into the future. So, the zero/small readings will not repeat next year.
If the small/readings are caused by any yearly periodic systematic regulation, I think it is not likely that the small period is the beginning 5 months of the year, since season/temperature does not typically starts at the beginning of the year.

Site 0, meter 0 (electricity), all buildings meter_reading:
",113054,2019-10-24 13:43:34,Buffalo Spdwy
"Thank you for pointing out.
I removed all those values (electricity of site_id 0 until May 20) from training data, and score improved a lot!
https://www.kaggle.com/corochann/ashrae-training-lgbm-by-meter-type",113054,2019-10-27 13:38:34,corochann
"I found the same insight in my notebook.

You can see this cluster of buildings down left.
There are not the only buildings that have the 0 phenomenon at the very same timestamps. There are at least big four other clusters of buildings that share the same 0 patterns.",113054,2019-10-28 01:38:25,Juanma Hernández
@corochann My score did too. It does take some guess work to decide what is pattern to fit and what is bad data to discard :),113054,2019-10-28 11:35:13,Buffalo Spdwy
@juanmah Yours is a very illustrative plot! Much more thorough than my eye ball findings. Will go to investigate those yellow blocks. Thanks!,113054,2019-10-28 11:44:42,Buffalo Spdwy
@juanmah That visualization is really nice to find out weird patterns!!,113054,2019-10-28 12:33:42,corochann
@barnwellguy Terrific visualization! Thanks! ,113054,2019-10-30 16:50:07,Manraj Singh
"Let me explain something about building meters - especially for chilled water, steam, and hot water.  Many of these are 'pulse' meters, meaning they don't record how much they consume at a given timestep interval.  Rather, they record every time they surpass a threshold.  For example, a chilled water meter may send a timestamp every time it hits 100 ton hrs.  This is tricky, and the thresholds are not necessarily well set up for a building.  A building may use 100 ton hrs of cooling in 45 minutes, or once every 6 months.  In this case, '0.0' values aren't null or zero consumption - they are a time when the pulse meter didn't record.
Per Dmitriy's comments, this is going to very difficult to pull out.  Not only do you have to estimate energy consumption at a given hour, but you have to estimate when the next pulse will occur to record the total for the test meter submission.",113054,2019-10-22 23:34:27,Matthew Dahlhausen
"@mdahlhausen Thanks for the domain knowledge, super helpful.
Just to confirm I'm understanding this right: a meter reading of, say 3.0 means that the meter has exceeded its threshold 3 times since the last interval (i.e. in this case 3 times in the last hour)? ",113054,2019-10-30 03:16:08,Louka Ewington-Pitsos
"The value is: Energy consumption in kWh (or equivalent).
The software normally convert the pulses to the energy unit.",113054,2019-10-30 15:50:46,Juanma Hernández
"Hello there,
When a (pulse) meter is mounted and configured for a certain measurement point, his sensitivity is chosen accordingly with the site consumption. That is, it should properly record each settlement interval (one hour in this case; the power is auctioned on an hourly base, or even quarterly in some countries). For example, if you want to weigh o piece of bread is not the best practice to use a meter that weighs trains.
There are very rare cases when a meter behaves like the ones with long 0 intervals. The site may be disconnected from the grid or the measurements are wrongly collected.",113054,2019-11-18 15:37:27,Sorin
"If 0.0 means that a reading wasn't taken, and then the next positive reading ""catches up"" on the incremental changes since the last positive reading, then it's actually a problem with the RMSLE metric.  
With a mean-preserving metric, this bunching would just create noise, but on average you would still get the right incremental value;  if you were supposed to have readings of ""100"" and ""200"" for the two hours, but you get ""0"" and ""300"" instead, either way you get 150 on average.  However, (log(101) + log(201)) / 2 amounts to a different number than (log(1) + log(301)) / 2; it's 4.96 vs 2.85, in this example.  The implication is that your prediction of the building's energy use should take into account the likelihood of the meter not updating every hour, implicitly or explicitly.  
Is the prediction for the building low because it's energy efficient, or because its meters get stuck more often?  The organizers won't know, so there will be a big question mark over any conclusion that they reach from the models trained to optimize the RMSLE metric.  This is the kind of nonsense that happens when you use a kludge metric, and don't think through what side effects you're introducing by using it.",113054,2019-10-17 22:23:44,Dmitriy Guller
"If they ignore 0-values when computing the scores, it'd take care of the problem.",113054,2019-10-18 05:11:32,authman
"Ignoring 0-values won't take care of the problem, it will make the problem different.  Instead of under-estimating the energy use for a building with a stuttering meter, it will over-estimate it.  Another problem is that we don't actually know whether zeroes are ignored or not, so how are we supposed to build the model intelligently?  
If I were to guess, I would say that the big CV-LB differences are due to either different behavior of zeroes in the test set, or different treatment of them.  This is why I'm staying out of this competition: the bad choice of metric is going to turn it into nonsense.  You're going to get a much bigger bang for the buck figuring out how to deal with zeroes than you will figuring out how to build an intelligent model.",113054,2019-10-18 13:01:34,Dmitriy Guller
wise words @dmitriyguller thank you for share your knowledge :),113054,2019-10-18 13:31:21,Nanashi
"The data description page does say

There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.

but we don't know if zeros are treated as gaps.",113054,2019-10-18 15:48:47,Dhananjay Raut
"My guess is that zeros are not treated as gaps, because in the training dataset, we do have gaps in observations, while the test dataset has an entry for every timestamp (to avoid leakage, I presume).  The observations that were added to complete the grid in the test set are probably going to be the only ones excluded from scoring.  ",113054,2019-10-18 19:41:36,Dmitriy Guller
"I think removing zeros (i.e. ""gaps"") solves the issue as they are not scored in the test set, one should not score in the train/validation either. 
Concerning the large CV-LB gap, I guess it depends on your CV schema ;)",113054,2019-10-25 12:36:32,bluetrain
"
I think removing zeros (i.e. ""gaps"") solves the issue as they are not scored in the test set, one should not score in the train/validation either.

Do you have evidence of this? My assumption is the same as Dmitriy's -- that 0s are handled differently than explicit gaps and would be scored in the test data. The competition details / sponsors haven't said anything to indicate that the test data is processed or represented differently than the train data, with the only exception being the artificial inclusion of timestamps that are missing to mask the true gaps. It seems like part of the problem is that meaningful 0s can't always be cleanly disambiguated from measurement gaps, so it doesn't seem possible to me that just excluding them would be the way to go. ",113054,2019-10-25 14:48:54,Joe Eddy
"""There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.""
I really think they could not have been more clear than this :)
Gaps are not simply zeros. Gaps are sequences of zeros, like the one that someone nicely plotted above. 
Find a logic that identifies these gaps and remove them from your training data. 
Good luck and happy kaggling!",113054,2019-10-25 15:59:53,bluetrain
"I agree with you that sequences of zeros may be reasonably interpreted as gaps, but there are plenty of explicit temporal gaps in the timestamps. For just one example, look at building id 1012, which has an explicit gap in August. It's possible that explicit gaps and sequences of zeroes are actually the same or similar things, but they are definitely represented differently in the training data. If we assume that the test data is processed/represented the same as train, then we could expect there to be sequences of zeros that aren't explicit gaps and therefore will be scored if we take that quote literally. 
I think that quote actually isn't very clear but rather leaves ambiguity because we're not told how to interpret zeroes. If those zero sequences are actual gaps, why wouldn't the sponsors just remove them? Or maybe to reframe the question, if those zero sequences weren't processed out of train, why should we expect them to be processed out of test for scoring purposes? My occam's razor view is that train and test were processed exactly the same way and then missing timestamps were added to test, and not that the sponsors have special processing code that will clean zero sequences from test that they didn't bother to apply to train as well if they know those data points are problematic.",113054,2019-10-25 16:29:11,Joe Eddy
"If the gaps are really missing records and not zeros, why bother specifying that they are not scored in the test set? If there is no data it's pretty clear that it will not be scored…
Also, from a logical perspective, it makes a lot of sense to exclude the zero-induced gaps from scoring, because they are clearly not predictable by any means.
However, I appreciate your point about the ambiguity and I agree that they could have cleaned the train set too, if my hypothesis was true.",113054,2019-10-25 16:40:10,bluetrain
"
If the gaps are really missing records and not zeros, why bother specifying that they are not scored in the test set? If there is no data it's pretty clear that it will not be scored…

I think they specify just because it's non-obvious once they fill in the missing timestamps and include weather data to obscure the gaps. They didn't want to just show no data for the missing timestamps in test because that would probably run the risk of being a major data leak, but they wanted to make it clear that they weren't going to impute those target values or something like that.

Also, from a logical perspective, it makes a lot of sense to exclude the zero-induced gaps from scoring, because they are clearly not predictable by any means.

I agree, scoring our models against obvious measurement errors that don't reflect real energy usage doesn't seem to make much sense, but unfortunately that doesn't guarantee that it won't happen. I actually do wonder if there's any predictability in the zero gaps -- could it be triggered by extreme weather conditions or be a recurring pattern for certain buildings? Could be an interesting thing to look into, but even if errors are systematic in some way I highly doubt we have enough data (or the right features) for a model to properly anticipate them.",113054,2019-10-25 18:09:38,Joe Eddy
"Maybe a good point would be removing only those zero values that are at the beginig of the ts, and leavo those ones between two blocks of meter_readings > 0",113054,2019-10-25 20:09:46,EnricRovira
This Comment was deleted.,113054,2019-10-29 00:22:04,No user
This Comment was deleted.,113054,2019-10-29 00:22:51,No user
"I get a .4 difference in score with(out) a zero discriminator, so I second your notion, but I'll ""compete""",113054,2019-11-02 12:22:25,Glimmung
"This data set is more tricky than it appears. There are green kernels that don't do any eda at the moment, but that will change soon. Many meters are missing entire months worth of data (forget 0.0 or np.nan values—I'm talking about no timestamp or sample recorded at all). Figuring out how to deal with these will be a game changer.",113054,2019-10-16 21:35:21,authman
"@authman @tiannnn @dhananjay3 @gunesevitan 
at my kernel Starter ⚡ Great Energy Predictor 
I explain this.
meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error.
Why is meter_reading 0?

Those buildings don't consume energy (weird).
The stuff they use for measuring was broken.
Missing data! See the next example with the building 0. Spoiler: They started to measure Building 0 at June 2016! (month=6)

If you check the builing 0 (building_id = 0) you will see:

there is no information between January - May 2016 (months 1 to 5). meter_reading==0.
From June 2016 (month=6) we have information! 

reason? 
In my opinion they started to measure the consume of that specific building in June 2016. 
In order to complete the database from 2016, they filled the other months with 0.
Note that they can do it because the meteorological variables aren't a problem, you can access to historical data from external sources,  and other variables about the building are constant like the year_built, how big is the building, floors etc… And that's why they could create the database and include new buildings since 2016 :)
So… would be amazing to know when they started to measure each building… I'll post more insights and cool information.
More information at my kernel, I'll keep updating everyday ;)",113054,2019-10-17 12:45:02,Nanashi
"Interesting. In your kernel we can see that some(e.g. 1099, 778) building have zero reading in between or after non-zero readings than this explanation is not valid in these cases. anyway we just want to know if to treat these values as missing values or not.",113054,2019-10-17 13:42:57,Dhananjay Raut
"good point I didn't realize about that, maybe is a mix of both: missing values and errors :)",113054,2019-10-17 13:47:00,Nanashi
"Yeah, what I was saying is that there are instances where we only have e.g. <500 samples for a building/meter. In the worst case scenario, there's a particular building-meter pair where we literally only have 2016 december transactions. The rest of the year is completely missing, as in no 0-fill, no np.nan, just no data period. There's more to this story than meets the eye ;-).",113054,2019-10-17 17:19:18,authman
0 gaps can be meaningful as well in my opinion. e.g. vacation in school building. ,113054,2019-10-18 19:07:57,Bhavesh Ghodasara
"IMHO my hypothesis is:

Zero readings mean zero energy consumption, but meter installed and working
Missing readings could mean a number of things: no meter installed, meter broke down, meter installed and running yet data unavailable…

Quick visualization to support my point:
https://www.kaggle.com/ganfear/missing-data-and-zeros-visualized?scriptVersionId=22110712",113054,2019-10-17 16:29:42,Ganfear
"as @ganfar mentionned, if you look at your house meters, the reading is on cumulative value not instantaneous value and a meter is almost never reset. For this competition, they most likely took substracted the previous value to get the consumption between two timestamp.",113054,2019-10-17 21:29:15,eagle4
"In this case it's necessary handle missing values and 0 gaps.✔️
But Which are the appropriate approaches to overcoming this problem?",113054,2019-10-22 21:59:20,CaesarLupum
"Are the zeros the gap?
From the data page:
'There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.'",113054,2019-10-17 15:28:05,FChmiel
"gaps most likely means absence of some timestamps.  
because we don't have all timestamps for each unique meter. can be seen in my EDA.",113054,2019-10-18 05:03:32,Dhananjay Raut
"I am not sure if this is taken from an actual dataset. But in practise, sensors can drift or malfunction. When that happens, it is possible for the reading to be different from the ground truth.",113054,2019-10-17 08:57:31,Daniel
"I think 0.0 probably means that building doesn't have that meter type. If the building has it, there has to be some energy consumption since it is real world data.",113054,2019-10-16 19:15:09,Gunes Evitan
"sometimes zero value occurs in between non zero values so ..
plus if a building does not have some meter type then that combination is not in dataset.",113054,2019-10-16 19:30:44,Dhananjay Raut
"I think that it could be that the zeros in the beginning of the measure are meters not yet installed.
And the zeros in between non zeros are broken meters that they replaced, so that after few months they installed new ones and it came back to meter.
Is it possible?",113054,2019-10-22 18:33:12,luis cascelli
"USA DOE likely has guidance on how to fill out survey's that they have conducted.  Since it looked like from my brief exam they wanted a count of number of PC is use, they likely had some specifics on what to do about meter readings.
If we can find that guidance that may give us a better understanding of the data collection process.",113054,2019-10-19 00:41:04,PC Jimmmy
This Comment was deleted.,113054,2019-10-24 13:43:31,No user
"Perhaps it is a site 0 characteristic. I zeroed out Jan-May site-0 meter_reading submissions to mimic the 2016 training set behavior, the score is worse. The zeros perhaps does not repeat each year.",113054,2019-10-24 13:51:56,Buffalo Spdwy
Thank you for the advice!,113249,2019-10-25 18:53:36,Carlo Lepelaars
"Thanks for sharing! @senkin13 
To anyone that doesn't know how to do a time series split, you can implement the time series split as shown in:
https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TimeSeriesSplit.html
",113249,2019-10-18 04:17:45,Daniel
"Hi @senkin13, I was wondering about that. For this competition the train set is for the year 2016 and the test set for the years 2017 and 2018. If we do a split by timestamp, for example, train on the first 10 months and validate on the last two months, wouldn't this deviate from the ""prediction reality""? 
What I mean is that during validation we can loose the effect of seasonality features. For example, the ""month"" feature will be useless since we will never predict for a month that we trained on, whilst when we predict on the test set this information can be quite important. My experiments with KFold and TimeSeriesSplit shows that the former underfits and the latter overfits.
Any ideas on how to approach cross validation for this problem?",113249,2019-10-18 09:47:20,Pedro Bernardo
"I agree with your viewpoint,since I have not worked deeply for competition,just talk one simple idea.
 The purpose to find a robust validation is for finding robust features,so you can create many features then use different cross validation to check which are robust,including kfold/timeseriessplit…",113249,2019-10-18 13:08:55,senkin13
Very Informative Share.. Thanks @senkin13 @tiannnn ,113249,2019-10-18 07:00:43,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
"
Don't trust public leaderboard too much ,because public and private are also splited by time.

@senkin13 do you have any evidence of it? I did split lb probe and seems it's random split. What makes you think it's timebased split? ",113249,2019-10-18 12:03:12,Konstantin Yakovlev
"wow @kyakovlev I'd like to see that. I think @inversion said:

We always split time-based data by time for Train/Test, and Public/Private. If we didn't do that, the Public leaderboard score would provide feedback (leakage) for future time points.

or at least I remember that someone posted that.",113249,2019-10-18 12:17:27,Nanashi
"sorry for posting link to other discussion
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112872#651685",113249,2019-10-18 12:20:37,Konstantin Yakovlev
"@kyakovlev Thanks for your probing, I think random split will lead to leaderboard leakage,so it's possible as you mentioned that 22% of first lines are private, or public and private are mixed with different month/week…",113249,2019-10-18 12:50:15,senkin13
"Some mix should be. I'm too dumb in stats to prove it knowing lb score. 
But I think it's possible to make calculation if you know lb score before and lb after excluding 22% last test data.
What we have:

All 0 predictions -> 4.69
Baseline - > 1.36
Excluding 22% -> 2.6

We need to calculate if 22% of 0s corresponds 1.24 RMSLE difference. If so - > random split, if not possible mix.",113249,2019-10-18 12:57:06,Konstantin Yakovlev
"4.69^2 = 22.00
1.36^2 = 1.85
2.60^2 = 6.76
(6.76 - 1.85)/(22.0 - 1.85) = 0.24
Given that last months are winter, 0.24 being a bit higher than 0.22 makes sense. So the split is probably not 78/22 by time.",113249,2019-10-19 19:13:08,Ahmet Erdem
"For anyone from the future, as of 28th Oct 2019, @geoffrey7697 probed the LB and identified that data between August 2018 to Dec 2018 is part of the Private LB. In fact there are evidences that it could be even more (may include July 2018 as well):
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113751#latest-659225",113249,2019-10-28 04:45:11,arnab
"Since Public LeaderBoard is much larger (1.5 years) compared to Train (1 year) or Private Leader Board (Possibly 0.5 years), should not we give value to the Public LB score along with Local CV score? Otherwise, it's fairly difficult to understand if the features capturing long term trends are helping or not. I suspect that this is the reason that Kaggle has limited the number of daily submission limit to 2. ",113249,2019-10-28 08:56:51,arnab
"IF it was I setting up the test split I would do it by building.  If time split like past Kaggle series, than 22% of two years would just be the first 6 months of 2017.
If done by building than the public test could still contain the full two years.
Assume that shortly someone will do the LB probe that lets us know.",113249,2019-10-19 00:24:54,PC Jimmmy
"Knowing the right location would help to use the holiday feature effectively. I earlier saw some discussion about using US holiday as feature. But the problem is, as we don't know the location we can't say for sure about the holiday list. I guess that's why my holiday feature failed earlier. Also maintenance day, natural disaster. festival day so many features to think about if the location is known .",114874,2019-10-31 02:57:16,Khairul Islam
"Thanks for posting this, as someone who has yet to incorporate anything about holidays into my modeling, it should be helpful",114874,2019-10-29 20:28:21,SteveKane
I got a better corr with Birmingham than loughborough. Check it out here: https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans,114874,2019-11-05 12:29:15,Silverback
"Could be. Loughborough is just based on the official Ashrae sites. 
But I couldn't find the Loughborough library building in our dataset so it could be wrong.
I see that on the Ashrae pdf you posted, that chapter is called Midlands UK and not  Loughborough, so Birmingham could fit.",114874,2019-11-05 15:55:40,S D
"Yer - it might well not be Birmingham as the corr isn't perfect. Birmingham Airport came up as a weather station in an ASHRAE report when referencing the ""Central England"" region in the PDFs, hence why I tried it. ",114874,2019-11-05 16:35:17,Silverback
Thank you for sharing. Thank you,114874,2019-11-01 03:19:09,jiruideng6
"Hi guys
Its my first competition I have question that if we find out the name of places related to site how could we use in modelling like what extra importance it will give to us.",114874,2019-10-31 06:00:29,boxxx
"You could use external data based on this knowledge as long as you post the source in the external data disclosure thread. As specified in the rules:

C. External Data. You may use data other than the Competition Data (“External Data”) to develop and test your models and Submissions. However, you will (i) ensure the External Data is available to use by all participants of the competition for purposes of the competition at no cost to the other participants and (ii) post such access to the External Data for the participants to the official competition forum prior to the Entry Deadline.
",114874,2019-10-31 19:35:59,tahaum
Damn this is so good. Thanks for sharing.,114874,2019-10-30 16:45:47,Manraj Singh
Terrific analysis! Thanks for sharing!,114874,2019-10-30 16:30:22,bluetrain
"striking way of finding hidden insights from data. Would be helpful to many for sure, who are yet to incorporate holidays in their features.",114874,2019-10-30 14:46:55,Atul Anand
"I don't know if if we'll help, but it's really impressive…",114874,2019-10-29 21:51:54,mezoganet
Identifying variation that happens systematically should definitely help assuming it continues into the future,114874,2019-10-29 22:33:11,SteveKane
"You should probably share this on the external data disclosure thread, even if you don't find improvement.",115256,2019-11-15 21:55:03,Rob Rose
"The impact of any magic with holidays, besides maybe Christmas/NYE but for this one needs no calendars, is likely to be marginal. Firstly there are buildings which seem to be fully immune to any holidays, secondly we are talking at best like 12 observations per holiday and then we have like 10-15 holidays per year outside of regular weekend/weekday pattern.
My point I would not waste time on playing with holidays at this stage of the comp, even though it is easier to dig in this direction then to figure out how to actually do CV in this comp :)",115256,2019-11-02 09:53:31,snovik
My thoughts exactly,115256,2019-11-02 09:54:39,Gunes Evitan
I've seen a small improvement encoding the holiday string as a category. Additionally I've seen a bigger improvement removing training rows that could potentially be a holiday altogether.,115256,2019-11-02 14:19:40,rcpeters
"@rcpeters, so you removed all the rows where ""IsHoliday""=1 and got significant improvement? By how much if you don't mind my asking? That is interesting.",115256,2019-11-02 23:04:18,YaGana Sheriff-Hussaini
At the time my score went from 1.25 to 1.24 but with only two decimal places who knows. ,115256,2019-11-04 02:10:37,rcpeters
"Gunes, thank you for this post with holidays in all 4 countries.
In a similar post Konstantin claims 0.02 improvement. I'd expect even better results with Ireland, UK and Canada added. Why is this not the case?",115256,2019-11-06 07:36:50,Poe Dator
"I believe Canada, UK and Ireland sites have less buildings than US sites. When we remove those timestamps, the data becomes even less and score doesn't improve.",115256,2019-11-07 05:52:20,Gunes Evitan
"As said and shown by different kagglers, there are mainly 4 countries US, Canada, England and Ireland in the dataset. And I have seen there are dicussions about F and C in air temperature but when I checked them empirically I found that Canada and US both have Celsius air temperature. Can anyone enlighten me? Thanks a lot.",115256,2019-11-10 18:20:15,Mukul Sharma
Thanks for sharing @gunesevitan.,115256,2019-11-01 11:54:28,YaGana Sheriff-Hussaini
This Comment was deleted.,115256,2019-11-12 10:56:06,No user
"@Sohier Dane  I appreciate the clarification you gave. But still, I believe, there are couple of things , which is making the competitiors lose their confidence on the ""COMPETITION"".

Allowing usage of external data is meant really to support a better modelling or better metadata build-up, which can assist in better model creation ,i.e external data that can ""help to find answers"". I know, it's not perhaps, written anywhere in the rules, but that is what the intention is/should be.

What I have read in the other  discussions about leakage,  it certainly shows that this is not an external data that can help to find answer, but this is something that is THE ANSWER.
So, I would really request even the kaggle team to clear their stand on this.

Assurance of using only the private set that is unavailable publicly is good for the competitiors. But , am I really wrong in thinking, that , using such public data which caters to exact building or site, actually makes it unfair on the part of other competitors , who  are practically working with only 2016 data and others who are working with leakage, are actually training their model on a bigger and better dataset?

Isn't that an unfair advantage?

Your last comment made me a little nervous. ""enough of the sites are unavailable online"" , means, 'some of the sites are available online'.  

May I have your opinion on the my following suggestions?

External data that involves meter readings of the buildings in the dataset should not be allowed for training. I hope my understanding is correct, that is what is happening here.
If above criteria is considered, I feel, clearing leaderboard could be helpful. Someone still may make submission with leaked data. But at least, we can hope that new submissions will meet the new criteria.
Extend the deadline, if possible.
",117357,2019-11-24 10:39:22,Leo Regulus
I hope that meter readings that can be publicly downloaded can be official train data.,117357,2019-11-15 08:36:42,daishu
"Hope so, but don't think it is possible. Leaked data from external resources can be used as data augmentation for the training set, which is not submitted. Thus, to make sure this not happen, one's model and training data should be checked as well (in my opinion). ",117357,2019-12-06 08:24:53,Ali_sher
"
We will ensure that any meter readings that can be publicly downloaded are not included in the private set.

@sohier Does that mean that you already know of other sites that can be scraped ? 
If so, this would be nice if somebody at Kaggle does the webscraping for us and make the data public. Otherwise, we may expect that such data will remain 'hidden' for some time before being published in the external data thread. ",117357,2019-11-15 15:40:50,FabienDaniel
Great idea! It is a great waste of brain power to let all Kagglers go through the nuisance of figuring out which site_ids have public test labels.,117357,2019-11-27 19:06:53,Carlo Lepelaars
"
To be clear, we have reason to believe that enough of the sites are unavailable online that there will be a meaningful final leaderboard.

May you tell us which sites are unavailable online? Leakages are frustrating.",117357,2019-11-21 04:01:19,daishu
I've spent so many hours cleaning up or looking at any site_ids that are now ready to download. I also used some submissions to test this. Looks like you will be penalized in this competition if you start too early,117357,2019-11-21 07:59:09,Oleg Knaub
"While I share the concerns of Leo Regulus and many others on this forum, let me propose the opposite solution. Could the organizers please make the leaked data explicitly available at this forum. 
Even if removed from score calculation, this data may help to train models. I just see no sure way to effectively ban use of leaked external data from However in many cases, getting data requires web scraping and repeated heavy use of websites / APIs of data owners. I noticed that some links posted in this forum are already broken. This means that data owners noticed unusual activity and restricted access. So we have a situation where those who got early access to leaked data have advantage. True, the sources are shared in the external data thread but scraping them again is not always repeatable for the above reasons. 
If the leaked data is available here it will put everyone into more equal position and support focus to ML part of the competition. ",117357,2019-11-25 18:39:24,Poe Dator
"You raise a fair point, and I was unaware that some of the links are down. With that said, are there any publicly available datasets that haven't already been rehosted in a notebook/dataset like this one? I'm happy to revisit this issue if that's not the case. https://www.kaggle.com/yamsam/ashrae-leak-data-station",117357,2019-11-26 18:00:33,Sohier Dane
"Dane, I have not found any extra datasets, however I have not checked personally if https://www.kaggle.com/yamsam/ashrae-leak-data-station is complete either. While Isamu Yamashita and other great Kagglers made a major effort to rehost leaked data, there is no 100% certainty that such rehosted datasets are accurately saved and processed. It may be best if such dataset comes from the organizers once the leaks are known. ",117357,2019-11-26 18:58:50,Poe Dator
"TLDR: I'm willing to review specific cases but can't make a sweeping commitment.
I think I would need a stronger case in order to publish additional data.

I don't want to poach upvotes from people who have published useful data
Some sites may not have a clear match and I need to be careful to avoid accidentally confirming unproven theories. Similarly, I need to be careful to avoid confirming if a site is not actually a match.
Some of the purported public sites only have partial data available; I need to be careful to release previously inaccessible data.
The data was acquired through a variety of methods so we often can't confirm if a link was ever valid. It would be easy for someone to set up a reasonable looking link that returns a 404 error.
For some sites the purported public data only covers some buildings for a subset of the relevant time period.  The forums and associated notebooks and datasets are constantly updated; it's not feasible for me to stay on top of all of them at the necessary level of detail real time. 
The burden is on competitors to ensure the data they use complies with the rules. If someone uses external data but the data doesn't meet all of the availability requirements specified in the rules after the data disclosure deadline, I can disqualify them.
",117357,2019-11-26 19:38:37,Sohier Dane
"Dane, Your arguments are fair, indeed.  Good to see that you and the other organizers are monitoring this situation.",117357,2019-11-27 19:01:20,Poe Dator
"In Data description one can read:

meter_reading - The target variable. Energy consumption in kWh (or equivalent). Note that this is real data with measurement error, which we expect will impose a baseline level of modeling error. UPDATE: as discussed here, the site 0 electric meter readings are in kBTU.

We can reasonably expect that some measurement error is present in the test set too. Are the measurement errors removed from the test set for evaluation?
In other words: do we have to predict meter readings or building performance?",117357,2019-11-30 09:26:48,lmssdd
"So I guess this is now a web scraping competition?  
I wish we could just have all the data beforehand, so that we don't have to do all the scraping",117357,2019-11-15 02:25:08,pocket
Now we just have to use the external data thread to download the training data 🤔 ,117357,2019-11-23 08:03:55,Etienne R
"Thanks for the clarification.
I'm satisfied with these rules.
Anyway, below 3 comments are useful.

The ranking in public leaderboard is useless.
The public leaderboard score is useful to check our cv system.
The thing we need to remember is ""trust your CV"".
",117357,2019-11-15 03:42:04,YouHan Lee
Good point! This leakage will lay even more importance on building a good local validation scheme.,117357,2019-11-21 19:14:33,Carlo Lepelaars
"@youhanlee - I agree with your observations. I have personally held off making any submissions yet. My approach is as follows:

Make a decent prediction model with a strong CV approach
Include no leaked data in the model design
Concentrate on a rich feature set, especially on missing building attributes like floor count; have more complete floor counts opens a number of critical energy factors like building volume, etc.
Enhance weather data with industry standard predictors like cooling degree days, cooling degree hours,  prior day, prior hour temperatures, etc.
Use the public leaderboard to help validate the model

Hopefully, this will create a solid predictor that will handle private test set without much chance of surprise.",117357,2019-11-30 01:33:10,Bill Holst
LB is likely useless as there is huge variation between years judging from the leaked sites. The only solid way to go forward is to trust your CV and even that one is kind of unpredictable. This is the worst data science setup possible.,117357,2019-12-04 21:47:19,snovik
why not just ban all external data?,117357,2019-11-19 23:27:46,AI Penguin
Agreed. The problem is that regulating whether someone has used leaked data or not is hard. I mean someone could quietly use it in their program and the organizers wouldn't know.,117357,2019-11-26 16:40:21,Ayush Goel
I wonder how do you find leak data exactly ? I think it's impossible if you already know all leak data from first. If so why do you open all leak data and restart this competition again.  ,117357,2019-11-15 08:35:09,Isamu Yamashita
"
We will ensure that any meter readings that can be publicly downloaded are not included in the private set.

So you will change the public/private split?
And I think December 12th is already too late for making the competition fair.",117357,2019-11-14 21:59:13,Ahmet Erdem
"Up, a deadline extension would be great, too much wasted time..",117357,2019-11-15 08:29:28,Arthur  Llau
That sounds fair but it makes the current LB of no use. Thank you for clarification. ,117357,2019-11-14 21:40:56,Vishal 
Thank you for the official explanation. I will continue this competition.👍 ,117357,2019-11-15 12:16:57,Mashlyn
"I have searched a lot of notebooks and threads but still a bit confused.
My question is about the timestamps on all of the datasets(train, test, weather train, weather test)
are they local time or GMT time? or some of them are local time, others GMT time?",117357,2019-12-08 08:48:02,Navid Hakimi
i think they are all local time.,117357,2019-12-20 03:34:29,时间之外的往事
plot by daily temperature for each site would be a good way.,117357,2019-12-20 03:36:06,时间之外的往事
"
We will ensure that any meter readings that can be publicly downloaded are not included in the private set. Update for clarity: shortly after the external data disclosure deadline, I'll review the disclosures and update the scoring system to ignore rows as needed. There will be a short blip on the leaderboard while existing submissions are reevaluated but the public scores will remain unchanged.

Does that mean that there is no reason to tune models for publicly downloadable segments? ",117357,2019-12-03 09:13:57,GeorgeGV
That sounds very fair ,117357,2019-12-03 07:56:12,Jelly's
"Does this means  when we upload our final data ,we can contain leak data?",117357,2019-12-01 01:23:07,jacobfrey
"Per the rules, all external data used for your final submission must be disclosed and available for all other competitions to use by December 12th (the external data disclosure deadline). We will, at a minimum, disqualify any team that uses undisclosed data.",117357,2019-12-02 17:27:30,Sohier Dane
"I hope the range of
""meter readings that can be publicly downloaded""
is declared at 12/12 (sooner is better if possible)
At least for me, it is not clear the range,
for example building_id 152 at site1 and 245 at site2.",117357,2019-11-21 12:55:08,Hiroyuki Namba
"Not sure organizers can. @sohier wrote they ""believe that enough of the sites are unavailable online"". He is not sure so I guess all or almost all are available…
Edited to try to be more constructive.",118375,2019-11-21 07:55:48,Bertrand P
The more interesting thing - is there anything than is not publicly available?,118375,2019-11-21 08:47:30,Roman
☝️ Would very much like to know this too!,118375,2019-11-22 19:15:56,Carlo Lepelaars
"As much as I agree with this, I don't think organizers would fulfill this request. We still have 4 weeks find all the data, disclose it on the external data thread and build the best model we can. Quite frankly, I stopped caring about the leaked data and just focused on modeling.",118375,2019-11-21 08:18:30,Tim Yee
I agree with @pdnartreb : I don't think the organizers are certain of what data is publicly available. My guess is they will remove the publicly available data from private LB at the end of the competition based on what is shared in the competition.,118375,2019-11-21 09:48:42,Vopani
So who scrapes the most and shares the least will win this competition,118375,2019-11-21 09:54:48,Gunes Evitan
"I think a week is enough time to adjust your models with additional training data provided others share their scraped data within the final week if it comes to that. So, that edge can't be that large.",118375,2019-11-21 10:09:20,Tim Yee
"What if someone has scraped and hasn't shared the same here , one can simply create a submission with that data and get a bronze easily. Who / How can it be ensured that All data which is publicly available is listed in the discussion forums?",118375,2019-11-21 13:32:19,AdityaVikramSingh
"I'm totally agree with you. 
According to the description of this competition, winner will be invited to an annual conference of ASHRAE next year, so now I'm really afraid that what winner will say in that event. I hope the winner will be more smarter than me.",118375,2019-11-21 08:31:32,Isamu Yamashita
"I second your request.
And I agree the attitude of the organizers, in particular Kaggle staff, has been really disappointing so far.",118375,2019-11-21 07:28:02,bluetrain
"Aaaaand here goes Google Analytics v2.0. Most stable plan atm - wait for entry deadline, get all data and try to predict something. Cheaters plan - scrap some more data, hide it and use indirectly for validating.",118375,2019-11-22 08:49:09,WispZero
"Thanks everyone for all the discussion and input around this concern. We are not planning to release any further data. The reason is that, while there are edge cases where the data could in theory be located, it is unlikely. Releasing data prematurely would unnecessarily degrade the utility of the public leaderboard. While also quite unlikely, it is technically possible that one of our data partners has published data without our knowledge since we last spoke to them, which would render a publication effort pointless.
There is still a range of scores on the leaderboard which would not be the case if it was determined solely by public data, so there is still an opportunity to assess your models there. As noted here, we will ensure that any meter readings that can be publicly downloaded are not included in the private set regardless.",118375,2019-11-21 17:22:20,Sohier Dane
"I think I know what you meant by edge cases. Site 4 Berkeley public data is very close to competition data, but it's not same. There is no direct advantage of using it. I'm pretty sure there are lots of smart people who are capable of reverse engineering the transformation. It looks like you are still underestimating competitors here.",118375,2019-11-21 18:27:26,Gunes Evitan
That is not what I was referring to.,118375,2019-11-21 18:46:40,Sohier Dane
"@sohier What we/I can/could ear… The pipeline to win this competition is: scrap the more, probably you will have to perform some kind of (black hat in this case) hacking to find all the data (I guess these are the edge cases you are referring to and I could illustrate that), don't share and pray that no one find/share them, build a model (I already have feature engineering ideas) that overfits the 2018 (private) data you have downloaded but don't use them explicitly…
Sharing is from my point of view the biggest asset of Kaggle.
Edited to try to be more constructive.",118375,2019-11-22 07:57:02,Bertrand P
"@claytonmiller @chrisbalbach, could you please give your insight regarding data leakage issue? Is it acceptable to your team to receive solutions with high percentage model generalization based on web scraping and/or leakage result?",118375,2019-11-22 13:24:58,devai01
"I apologize for the words that I used. Reading once more what I wrote showed me that they were not translating my thoughts, not relevant and not constructive so I edited my posts. I believe that organizers are doing there best to purpose an interesting competition. Frustration and disappointment are not easy to deal with…",118375,2019-11-23 09:37:41,Bertrand P
:-),118375,2019-11-23 09:53:02,Vopani
"I'm very consistent with you. But some bad things is happened. I think I have find a site .maybe it's in the Private LB. I decided to open source the data this weekend, Because I want to confirm the correctness of the data. The open source data found on the website may be a little different from the data in the competition.",118375,2019-11-21 08:06:50,Zhang Yunfei
1,118375,2019-11-21 08:05:49,Frank Lawrence
"Hi everyone,  congratulations to all the fantastic Kagglers on here! Apologies for the newbie question but this is my first competition. 
I don't understand what additional information do these data leaks exactly bring to the models? Whilst I understand that these leaks may help to boost public LB score would they still be useful on private LB if the site_ids are different? In fact would these leaks not be bad for generalising models and cause overfitting to train set?",118375,2019-11-21 10:24:50,spenot09
I want to be far from top LB. stop working on improvement aww….,118375,2019-11-21 09:38:30,Hanjoon Choe
"You can check my kernel Reducing Memory Size for Great Energy Predictor
",112917,2019-10-16 10:44:44,Tarek Hamdi
Upvoted your kernel,112917,2019-10-16 18:53:49,Gunes Evitan
"Thanks @gunesevitan , I hope it's useful",112917,2019-10-17 04:24:23,Tarek Hamdi
Thanks for this!,112917,2019-11-01 12:50:58,Bryan
Thank you :) ,112917,2019-10-17 14:37:03,Y Assous
"RMSLE, especially the plus one version  of it, is easily my least favorite metric on Kaggle.  It's a kludge metric that you use when you don't know how to deal with skewed non-negative response variables.  One problem with that metric is that it penalizes the models that actually give you an unbiased estimate of the target.",113064,2019-10-16 21:33:55,Dmitriy Guller
"This problem it's like a 'forecasting time series models', RMSE may be better than RMSLE
and so good measure of errors that can serve as a loss functions to minimize.
What do you thing about ?
RMSE: It is symmetric and quadratic, which is suitable for Gaussian noises.
More generally, minimizing RMSE finds an approximation for the conditional expected value of the next observation (to be predicted) given the explanatory variables (the past in time series).
See this: https://www.quora.com/Why-we-use-Root-mean-square-error-RMSE-Mean-absolute-and-mean-absolute-percent-errors-for-forecasting-time-series-models",113064,2019-10-22 17:12:16,CaesarLupum
"RMSE is not a good metric either for skewed distributions, because the noise is unlikely to be Gaussian  with constant variance.  If you use RMSE on a skewed response variable, your fit statistic is going to be mainly driven by the observations of the highest magnitude.  
We have this problem all the time in insurance applications.  Consider a typical model to predict auto insurance lost cost:  the mean may be something like $500, the vast majority of observations would be $0, but some could go into $100,000 area or higher.  We don't model log1p(auto insurance losses), as that would result in a model that's distorted in all kinds of ways.  We typically use a Tweedie distribution to fit the GLM model, and a Gini index to evaluate it.  
Gini index is a non-parametric measure, and the advantage of it is that it is agnostic to what you assume about the distribution of your target variable.  Most of the other metrics, including RMSE, RMSLE,  RMSL1pE, MAE, MAPE, etc., are not.  When you have a non-negative response variable, you can never go wrong with a Gini index, but you can easily railroad your models to go off into stupid directions if you make a wrong choice of a distance-based metric.  RMSL1pE in particular is a wrong choice for applications where you want your model to make sense and be useful.",113064,2019-10-22 17:48:48,Dmitriy Guller
Thanks for helpful response sir !✔️ ,113064,2019-10-22 18:17:44,CaesarLupum
Thank you for the explanation @dmitriyguller! I've learned a lot from it!,113064,2019-10-25 19:00:52,Carlo Lepelaars
"We typically use a Tweedie distribution to fit the GLM model, and a Gini index to evaluate it.
@dmitriyguller, thanks for your insights.  Is Gini index the same as the Normalized Gini Coefficient?",113064,2019-11-01 10:55:30,YaGana Sheriff-Hussaini
"Yes, that's the same thing.",113064,2019-11-01 12:23:00,Dmitriy Guller
"I'm probably missing something, can someone please explain why all the published notebooks have the RMSLE of validation set ~0.7, but that is so different from their scores against the test set ~1.4?",113064,2019-10-17 14:25:22,Nerijus Areska
"@nerijusareska because the lgbm model's use RMSE not RMSLE.

https://stats.stackexchange.com/questions/56658/how-do-you-interpret-rmsle-root-mean-squared-logarithmic-error/56659",113064,2019-10-17 14:34:34,Nanashi
Plus the published notebooks haven't found a good way to cross validate yet,113064,2019-10-17 15:01:12,Gunes Evitan
"I think most of the public kernels use RMSE of the np.lop1p which should be equivalent to the RMSLE, i.e.:
sqrt( mean( ( lop1p(y) - lop1p(pred) )**2 ))
right?
About the LB vs. CV, well, you are training and validating on the same year of data, but trying to predicting the next 2 years after that!
That's a quite different task, so the scores should barely correlate at all.",113064,2019-10-17 15:29:03,Henrique Mendonça
Yea I think they missed out using a custom loss function for lgbm,113064,2019-10-17 17:29:42,Daniel
"
I think most of the public kernels use RMSE of the np.lop1p which should be equivalent to the RMSLE, i.e.:
  sqrt( mean( ( lop1p(y) - lop1p(pred) )**2 ))
  right?

Yes, it is right! One could simply check that by using sklearn's MSE and MSLE:
y = np.random.rand(10)
y_hat = np.random.rand(10)
print(np.sqrt(mean_squared_log_error(y_hat, y)))
print(np.sqrt(mean_squared_error(np.log1p(y_hat), np.log1p(y))))

Of course we need also apply np.expm1() to convert the predictions back…
So, I guess the gap between CV and PublicLB is due to 2 reasons:

CV setup. I'd definitely try TimeSeriesSplit (build consistent validation is always tough in time-series comps 😄 )
Train vs Test differences
",113064,2019-10-18 09:59:50,Federico Raimondi
you can also use np.log1p and np.mean instead of the math.*  … just saying :),113064,2019-10-16 22:44:12,Henrique Mendonça
Thanks for pointing out this.,113064,2019-11-01 09:25:39,LongYin/杰少
"TLDR version;
RMSE is a kin (very very  close) to the standard deviation. RMSLE is identical except you take the log of the value first. 
This means while in RMSE you are comparing actual values (call this value y), in RMSLE you are comparing the exponents (call these x … the comparison is y = e  ^ x  (e to the power x)).  in short RMSLE compares not the value but the powers.
Or perhaps an easier way to visualize this, it's like comparing how many digits it takes to make a number vs the number itself. … so two 8 digit numbers have about a  gap of about 0. and the gap between an 8 digit number and a 7 digit number is about 1 which is about the same gap between a 3 digit and 2 digit number. RMSLE is adjusting for relative scale of a particular result. ",113064,2019-10-24 23:34:56,j_scheibel
I can only assume they wanted the score in this manner as they feel buildings produce consistent variation but the variation is scaled up as the buildings power usage gets bigger. ie a 5 story sky scrapper vs a 50 sky scrapper are nearly identical. It is just 1 is a strict multiplication in power consumption compared to the other. the highs and lows scale identically.,113064,2019-10-24 23:35:59,j_scheibel
"The buildings better produce constant variation of energy use on a log scale (or more accurately, the meter reading + 1 should have a constant variation on the log scale).  One fun fact about RMSLE is that the bias of the optimal estimate is a function of the standard deviation of the target variable (on the log scale).  If meters readings don't have a constant standard deviation on the log scale, then a less efficient building on the scale you get billed for by utilities may actually appear to be a more efficient building on the log scale.",113064,2019-10-24 23:49:15,Dmitriy Guller
"
the most unique property of the RMLSE that it penalizes the underestimation of the actual value more severely than it does for the Overestimation.> 
  This shall help a lot if kept into consideration.
  Thanks!
",113064,2019-10-17 13:39:09,Mohammad Sohaib
"Nice, thanks! This will come in handy!",113254,2019-11-07 19:27:29,Carlo Lepelaars
"It's very strange, because the consumption has two very differentiate periods:

The first half of the year it has a constant consumption of 40 kW and a working days consumption, and late evening consumption in the weekends.

In the second half of the year, the constant consumption is about 6 kW, all days has high late evening consumption. 

You can see the interactive graph in my ASHRAE - Interactive building energy browser notebook and choosing the building 363.",113254,2019-11-01 10:55:06,Juanma Hernández
"After seeing your post, the person responsible for monitoring that building ",113254,2019-10-18 19:34:02,Khairul Islam
"had a look on that. Reproduced, confirm
Found 8782 observations, all of them on building 363, 3-rd site, meter 0
Meter 0 is an electricity. It means, that building could use electricity while development
However: there is no data after 2016 🙏 ",113254,2019-10-18 19:34:02,Dmytro Samchuk
"Your opinion is reasonable.
I think that energy consumption should be different before and after construction, so it seems to be difficult to predict.",113254,2019-10-19 05:24:53,Yoshi
"P.S.
7 buildings (building_id = 28, 45, 46, 99, 103, 409, 567) are built in 2016.
We should be careful about these buildings, because energy consumption may differ between train period ( = development period) and test period (= after built). ",113254,2019-10-19 05:36:33,Yoshi
Nice work - those buildings might also have some use to understand state of the art for energy savings compared to similar buildings in same and other sites.,113254,2019-10-19 07:11:24,PC Jimmmy
Good catch! Thanks for sharing.,113254,2019-10-18 06:50:42,chmaxx
"That's ingenious! 
but…
X['sin_' + period] = np.sin(2 * np.pi * value/X[period] )    ?
Thanks for sharing.",113784,2019-12-06 16:02:11,seabiscuit2019
"Other machine learning algorithms can be a lot of strong towards raw cyclic features, notably tree-based approaches. However, deep neural networks stand to learn from the sine and cos transformation of such features, notably in terms of aiding the convergence speed of the network.",113784,2019-10-22 17:40:17,Shashi Prakash Tripathi
"Thanks, but I think Series.dt properties are easier for cyclical datetime features
https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetime-properties",113784,2019-10-22 09:38:11,Gunes Evitan
"Maybe :)
But this approach allows you to implement features with a more cunning choice of the period, in contrast to just seconds, minutes, etc.",113784,2019-10-22 09:55:00,Lyalikov Artyom
"From my understanding, this sin/cos transformation can be assumed as a part of fourie transformation, which has following advantages:

can adjust trade-off between smoothness and number of variables (when apply one-hot encoding) by considering the number of sin/cos pairs
can handle multimodal periodicities

Please refer following for more detail:
https://otexts.com/fpp2/useful-predictors.html
https://otexts.com/fpp2/complexseasonality.html
I'm not sure this variable transformation is effective for tree-based ensemble modeling, but believe that effective for linear regression.",113784,2019-10-24 08:17:01,statefb
"Simplest way to get year, month, day, hour, minute and seconds
pd.todatetime(data['timestamp']).dt.year
pd.todatetime(data['timestamp']).dt.month
pd.todatetime(data['timestamp']).dt.day
pd.todatetime(data['timestamp']).dt.hour
pd.todatetime(data['timestamp']).dt.minute
pd.todatetime(data['timestamp']).dt.seconds",113784,2019-10-24 03:57:47,Prashant Kolkur
It's from sklearn import base in kaggle kernel. Thanks for sharing.,113784,2019-10-23 19:30:53,Tim Yee
wow very fast approach thanks @timon88 ,113784,2019-10-23 13:18:21,olaleye eniola
"Ah, great. Thanks! I already encountered this idea in a different context and totally forgot about the concept. 🙈 
Have you already tried this out and if so, how did it affect your submissions?",113784,2019-10-22 11:53:21,chmaxx
"While I didn’t check, I think I would check with a bundle of features",113784,2019-10-22 13:55:08,Lyalikov Artyom
Great. This can be very useful. Thanks for sharing!,113784,2019-10-22 15:22:00,Adrian Zinovei
"Helpful for me, thank you!",113784,2019-12-07 02:35:25,lonelycat
"Very interesting, thanks for sharing",113784,2019-12-06 22:01:38,python10pm
"For unknown reason, I am unable to install numpy 1.17.3. Any idea why ?  
",114614,2019-10-28 19:14:50,siimondele
"I don't know why is showing the current numpy version, instead of the installed numpy. But the numpy installed is working good with LightGBM.
And what work in runtime, could not work when committing. Or vice versa, the numpy version is the correct one when committing.",114614,2019-10-28 20:04:58,Juanma Hernández
"7z is compressing more than gzip.  But you have to issue the  command manually on the saved cvs, I don't think it is a pandas option.",114782,2019-11-14 19:57:10,CPMP
pics or it didn't happen,114782,2019-10-29 08:59:44,SteveKane
"
I went full on Billy Gibbons! ;)",114782,2019-10-29 09:07:32,Scirpus
Super like!,114782,2019-10-29 09:09:29,arnab
"I just like ""Blue Jeans""… One of the best song, ever.
Did play it one century ago…
Bill Gibbons is seldom seen here, anyway 👀 ",114782,2019-10-29 18:25:45,mezoganet
"If you set a chunksize=25000, the gzip compression will go a bit faster.",114782,2019-10-29 12:40:42,authman
That's a good suggestion. Thanks a lot 👍 ,114782,2019-10-31 17:10:16,Tejash Shah
good one!!,114782,2019-10-30 15:07:15,Dibya Ranjan Sahu
"Even better, add .round(1) for meter_reading
Pretty sure after 2nd decimal it's overkill…",114782,2019-10-29 12:26:54,bluetrain
"It might even help your score with those positive ""near-zero"" values…",114782,2019-10-30 10:37:47,Silverback
"if you think so, keep all of them :)",114782,2019-10-30 16:05:19,bluetrain
Thanks @scirpus. I don't look good having beard.,114782,2019-10-30 16:43:39,Manraj Singh
"Nice poetry ! Really !
My English courses a long sooooo long time  ago.
But not sure""meter_reading' rymes with 'compression='gzip''
Or being French I did miss something ?
Always welcome !",114782,2019-11-07 21:33:08,mezoganet
"A possible explanation is that Site 1 is in England and some buildings were historically measured in square feet? 
Some of the Site 1 buildings are present in the  Building Data Genome Project Database - see my post for details. I wonder what your analysis would show for Site 1 after removing the buildings with known square meter measures?",115783,2019-11-06 06:40:30,Poe Dator
"Interesting. You can connect that with my finding that some temperatures were measured in F and converted to C, while others were directly measured in C.
1,5,7,11,12 being the sites using Celsius. 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114483
Also, I believe 7,11 are in Canada, 1,5 in England, 12 in Ireland.  (7 and 11 use the same weather data btw)
Canada uses officially the metric system, but would still use square feet a lot. So you might see a mix, which might explain that 11 is not metric.
Maybe something similar is happening in site 1.",115783,2019-11-08 14:37:45,S D
"As said and shown by different kagglers, there are mainly 4 countries US, Canada, England and Ireland in the dataset. And I have seen there are dicussions about F and C in air temperature but when I checked them empirically I found that Canada and US both have Celsius air temperature. Can anyone enlighten me? Thanks a lot.",115783,2019-11-10 18:19:39,Mukul Sharma
"Ottawa and Montreal are close to the border and looking at the weather they can be confused with the city from the USA
",115783,2019-11-10 18:28:52,Stanislav Poryadnyi
"Thanks, very clever method.
And the result agrees with the discussion in @poedator 's post
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115698#latest-667385
where 0,5,12,7 are the only sites outside US.",115783,2019-11-08 14:21:15,Buffalo Spdwy
"Sites outside US are: 1 (London), 5(UK), 7 and 11 (Ottawa - weather data match), 12 (Dublin)",115783,2019-11-08 16:49:04,Szilárd Kálosi
interesting finding. Look mixed unit existed as well as mixed timezone in the dataset.,115783,2019-11-05 13:47:44,WangYong
very useful finding! Thanks for sharing,115783,2019-11-05 13:18:40,LeiZhou
"It depends on what basis did you divide it. If there is no logic behind it, I think it must be a coincidence.🙄 ",115851,2019-11-26 17:23:07,Shahules786
"The importance of the holiday list is questionable because holidays vary by nation and to some degree by regions within nations. The US does not celebrate the Lunar New Year which is a very big deal in Asia. The seasonal split theory is valid only if all buildings are in either the northern or southern hemisphere.
 I got a score of 1.10 using Vopani's kernel and I show up about 20 places higher than him on the LB (which seems unfair but I did upvote the kernel!). Looks like lots of folks have scores of 1.09 and 1.10.
Kickback",115851,2019-11-10 15:19:14,kickback
How did this work? Must be an accident! ,115851,2019-11-12 16:49:36,Shahules786
"It's actually madness.
I saw it yesterday just after making a submission where I trained on the whole dataset before submitting. So I went back and did the exact same process except I trained two models on one half each (just like kxx), averaged them and resubmitted.
My score (1.17) stayed exactly the same (though when I ""sort"" my scores the half-half version goes above the original one, so presumably it's a tiny bit better). 
Point being: IN GENERAL half-half doesn't seem to make a big difference. My guess is that there is something specific about these kernels that makes half-half good. 
Which is a very bad sign, because there doesn't seem to be any logical reason for that.",115851,2019-11-05 22:55:34,Louka Ewington-Pitsos
"update: I resubmitted the python kernel by @rohanrao exactly the same except with the bad site 1 readings removed and scored 1.10 but it was one of the higher 1.10's according to the leader-board, so I think its safe to say that this is an improvement. 
So that's sliiiightly good news because it means that doing something sensible increases your score by a small amount, but I still have no idea what the hell is going on in this comp.",115851,2019-11-06 02:00:56,Louka Ewington-Pitsos
I can confirm this observation,115851,2019-12-01 15:47:32,pspenano
"shooting in the dark is my vote - my current best model does a nice job of predicting the training data - 2016 with no strong indications of over fit.
My leader board score still a long ways to go - I think prediction of special events in 2017 and 2018 will be key and therefore lots of shots in the dark.",115851,2019-11-05 22:54:14,PC Jimmmy
"oh god. 
We had better not have to search for like, power outages in dublin or some cancer. ",115851,2019-11-05 23:28:20,Louka Ewington-Pitsos
"Allways look on the bright side of life !
https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=2ahUKEwip8ePq49_lAhWQHRQKHae4B64QtwIwAHoECBMQAQ&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DVOAtCOsNuVM&usg=AOvVaw2nXYSDLwmFznNo6CvwiV4x
Some things in life are bad
They can really make you mad
Other things just make you swear and curse
When you're chewing on life's gristle
Don't grumble, give a whistle
And this'll help things turn out for the best
And
Always look on the bright side of life
Always look on the light side of life
If life seems jolly rotten
There's something you've forgotten
And that's to laugh and smile and dance and sing
When you're feeling in the dumps
Don't be silly chumps
Just purse your lips and whistle, that's the thing
And
Always look on the bright side of life
(Come on)
Always look on the right side of life
For life is quite absurd
And death's the final word
You must always face the curtain with a bow
Forget about your sin
Give the audience a grin
Enjoy it,…
Quite this competition, da ya think?
This is what I feel right now. ;-)",115851,2019-11-10 13:51:00,mezoganet
"Yes, it's interesting and worth to investigate more. If the main reason is the 2 x 6months CV or not. Also why the OOF score is CV1: 1.33 and CV2: 1.50. I've similar imbalanced local RMSE results with 4 x 3months CV.
It could also mean that, as you said, we're shooting in the dark, if so then big shake up is expected at the end. ",115851,2019-11-05 21:14:49,MPWARE
4×3months CV method has a bad result in LB score like 1.21 or so,115851,2019-11-06 07:05:04,Dxx
"Thanks @mpware I was going to try 4×3months CV method today. You saved me a bit of time.
I also observed a similar imbalanced local RMSE results in some of my models and they seem to give a better public LB score. So I do not trust any of my CV methods so far.",115851,2019-11-06 07:44:03,YaGana Sheriff-Hussaini
"Have you tried to make separate models for each 6 months. I will try to make it. I hope that will improve the score  a little bit 🙏 . If you had try it, please save me time.",115851,2019-11-07 09:29:30,Aymen Khelifi
"The more I remove features the more LB score improves, it's totally negatively correlated to my local CV. I admit that some features I added could generate overfit so I'm removing them progressively.",115851,2019-11-07 20:12:51,MPWARE
"@mpware When you say ""2 x 6months CV""/""4 x 3months CV"", that's not enough information:

Do you mean LGBM, with a rolling timeseries split (like ""Half-and-Half"" notebook), or just a simple K-fold split (stratified or non-stratified, and if stratified, by what? building_id? And of course, turn date-shuffling off.) It looks like you meant ""simple K-fold split"", thus ""simple 2-fold""/""4-fold""
Did you not use RMSLE rather than RMSE as evaluation metric? (""I've similar imbalanced local RMSE results"") And what did you use as loss function in training? If you used the wrong functions, then sure your RMSLE results would be worse.
Third, the imbalanced OOF scores (CV1: 1.33, CV2: 1.50) are not necessarily bad. Rerun with RMSLE. And best to quote your CV-LB offset, rather than just CV numbers. A classifier with a so-so LB score, but tight CV-LB correlation, typically performs better than the opposite. Whereas overfitting is poison.
",115851,2019-11-16 01:25:47,Stephen McInerney
"@rohanrao 
I created the same thing in which I divided my train data according to months 11,12,1,2,3,4 and 5,6,7,8,9,10 but I did't get good results. Is this the same thing you are doing or which approach you used to divide the data.??",115851,2019-11-21 01:56:48,boxxx
@rohanrao Could you please let us know your thought as well?,115851,2019-11-12 10:58:40,arnab
This Comment was deleted.,115851,2019-11-05 18:38:39,No user
"@raphael1123 the kernel you linked already had 1.11 using the exactly same strategy mentioned in this topic! lol
Have a look at that kernel again, there's a print of the folds and a link to the best version
By using the unshuffled KFold @kimtaegwan gets half training and half validation splits by date, because the dataset is sorted by timestamp.
fold = KFold(n_splits=folds, shuffle=False, random_state=42)
--------------------------------------------------
2 -fold
fold : 0
val_set_range :  2016-01-01 00:00:00 2016-07-04 17:00:00
fold : 1
val_set_range :  2016-07-04 17:00:00 2016-12-31 23:00:00
--------------------------------------------------
",115851,2019-11-06 09:27:53,Henrique Mendonça
"That's a similar strategy👍  @hmendonca 
In another kernel of mine, I tried Group2fold (January to June, July to December) and achieved 1.10.  What's going on? haha",115851,2019-11-06 11:37:28,Taegwan Kim
@raphael1123 I didn't find the code for stratified by building_id. Could you please point me to that?,115851,2019-11-12 09:08:13,arnab
"1.05 LB without any external data.
I hope no more leakage😥",116309,2019-11-19 03:29:43,Hiroyuki Namba
1.07 LB without any leak,116309,2019-11-28 13:11:43,InterDog
"Without any leak, using Catboost 5 fold, lb = 1.06 and cv = 1.2073",116309,2019-11-27 14:16:29,HarshitMehta
1.09LB,116309,2019-11-18 00:23:57,Shahules786
1.016 cv 1.07 lb,116309,2019-11-08 11:26:02,Oleg Knaub
"@oleg90777 , impressive result!
Are you focused on FE or outlier detection strategy?",116309,2019-11-08 12:15:11,Kostiantyn Isaienkov
Your CV seems very close to LB. Are you using non-shuffle kfold ?,116309,2019-11-09 01:08:40,Antoine
Yes,116309,2019-11-09 08:10:45,Oleg Knaub
0.998447158 cv 1.07 lb。 but i don't know how to improve its up to 1.06;,116309,2019-11-10 07:51:53,Zhang Yunfei
@pp2file very impressive. Are you doing FE for weather data?,116309,2019-11-10 16:51:34,Vishal 
I test a lot weather FE，but it's not improve score. I just use normal weather. don't add lag.,116309,2019-11-11 00:21:21,Zhang Yunfei
"nice work!
what is ur CV method?",116309,2019-11-12 12:00:00,jerrywu
"just split two data. I think it's not a good method, Now i work to find a good cv method.",116309,2019-11-12 13:22:48,Zhang Yunfei
"@pp2file  Congratulations  on this jump :) , is there any cv method that you recommend from your side ?",116309,2019-11-19 06:05:07,AdityaVikramSingh
"Built model by meter type using 5 KFold (shuffle = False),using:
https://www.kaggle.com/nz0722/aligned-timestamp-lgbm-by-meter-type
cv = 1.2406, lb = 1.04
P.s : Including leaked data too",116309,2019-11-18 05:13:51,HarshitMehta
NO UCF DATA  ?,116309,2019-11-18 05:49:09,ADITYA KUMAR
without leaked data? It is impressive.,116309,2019-11-18 05:49:58,Jonny Lee
Sorry for the confusion. I used leaked data,116309,2019-11-18 06:24:41,HarshitMehta
"1.07 CV, 1.09 LB",116309,2019-11-12 07:24:56,Chau Huynh
"I I suspect that the nature of this data means that single models won't end up being super effective. We'll probably have to combine a bunch of different models predicting different subsets to get best results. 
my best lb score is 1.10, but it's just the half-half kernel with the usual site_id 0 rows dropped, and a tiny bit of FE. ",116309,2019-11-09 06:24:01,Louka Ewington-Pitsos
1.10LB,116309,2019-11-08 11:43:19,Jie Lu
1.05 cv 1.09 lb,116309,2019-11-08 11:30:59,Davide Stenner
1.07 lb ,116309,2019-11-08 13:25:40,Isamu Yamashita
"Hi, @yamsam , perfect!
The same question, are you focused on FE or outlier detection strategy?",116309,2019-11-08 13:30:36,Kostiantyn Isaienkov
I have no magic feature and don't use  ucf data yet,116309,2019-11-08 13:49:40,Isamu Yamashita
"dang, you must be pretty good at this",116309,2019-11-09 06:29:12,Louka Ewington-Pitsos
"sorry for stupid question, what is  ucf data ?",116309,2019-11-09 10:06:24,Jonas Matuzas
https://www.oeis.ucf.edu/buildings,116309,2019-11-09 12:47:40,Isamu Yamashita
Update. LB 1.03 with UCF data,116309,2019-11-15 08:53:22,Isamu Yamashita
my LB 1.06 I am modifing your notebook with UCF,116309,2019-11-15 12:32:02,Jonas Matuzas
Isn't the ucf data kind of already providing the target variable results ?,116309,2019-11-15 14:42:20,DietHard
"Yes. but take care, next wave is incoming.",116309,2019-11-16 01:02:56,Isamu Yamashita
@yamsam  Using UCF data(site 0 ) my cv   and lb score is increasing by approx 0.01. Also i am splitting  train /validation(UCF data). Are you training with given data and making change in submission file by replacing directly site 0 meter reading  ? ,116309,2019-11-16 06:19:15,ADITYA KUMAR
YES. But I have not found the best CV with UCF data. I am just trying to find it in my public kernel.,116309,2019-11-16 06:34:01,Isamu Yamashita
Thank you for clarification.,116309,2019-11-16 07:08:13,ADITYA KUMAR
Very helpful thread,116309,2019-11-30 13:20:10,NikhilGupta
LB1.06  CV0.81   I don’t known why the cv score is so low,116309,2019-11-29 04:10:07,Dxx
your model is overfit,116309,2019-11-29 07:12:01,InterDog
"LB 1.10 , hyperparametes tuning helped to gain 0.01, I'll keep searching for the optimal ones",116309,2019-11-22 21:37:12,Igor Dudchenko
1.09 LB,116309,2019-11-13 23:10:34,Yagiz Tumer
1.0015554 cv 1.07 lb no ucf data,116309,2019-11-12 17:21:43,ADITYA KUMAR
"@negi009 what's the secret sauce☀️ 😜 , some sophisticated FE or using normal data?",116309,2019-11-12 17:52:25,Aadil Srivastava
What CV are you referring to?,116309,2019-11-12 22:17:09,Antoine
normal data very few FE # how you split data can give you better model. ,116309,2019-11-13 13:34:01,ADITYA KUMAR
@negi009  do you recommend a time based split for various meters ?,116309,2019-11-19 06:06:01,AdityaVikramSingh
@avikrams  yeah split should be time based to avoid leakage. ,116309,2019-11-19 07:03:47,ADITYA KUMAR
1.09 LB,116309,2019-11-10 16:48:06,Vishal 
"LB 1.10
I am curious how many of the high score single models are a tree model",116309,2019-11-08 19:58:53,Ad hoc
1.09 LB,116309,2019-11-08 14:21:29,Kangyu Chen
1.08LB,116309,2019-11-08 13:50:12,Mr Loke
that is amazing. i have tried but its hard to get better from 1.1LB,116309,2019-11-09 10:44:59,Yixinchen
"Thanks a lot for sharing this. I think we need some clarity on how this will be handled.
""Do not use for training or scoring"" is not a straightforward solution as it can be exploited in other ways. Post-processing can be done to overfit the values to closely match the actuals. Certain insights can be used hidden in the code to tune parameters or outputs using the actual labels.   
Whether or not this data was used then becomes very subjective.
Organizers should acknowledge publicly and allow the use of this data in any way to make it fair. Ignoring Site 0 rows for scoring in private LB is a good option and easy to do since they already mention about not scoring some rows.",116763,2019-11-11 12:11:30,Vopani
"
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115698#665239",116763,2019-11-11 12:45:19,Konstantin Yakovlev
I bet he was talking about way different things 7 days ago.,116763,2019-11-11 16:43:41,Roman
"He's not clearly saying that we can use this data directly in the submission.
And he told nothing about the consequences of using such data leak either in the model or in the submission.
I just hate that after two months' hard work at last being disqualified.
Need some official crew to clarify this.",116763,2019-11-11 16:50:56,Jie Lu
"
My concern is UCF - as meter readings are publicity available.
Predicting that at least public LB will be ruined soon if this data won't be excluded.

answer is 

We're aware, not to worry. Just remember to keep the external data thread up to date as needed.

links were posted
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112841#665274
I don't see any reason for disqualification.",116763,2019-11-11 17:09:59,Konstantin Yakovlev
I wonder that whether using test labels is allowed?😂 ,116763,2019-11-11 10:29:18,daishu
"Competition organizers and Kaggle Team should disclose ""using"" in this case. It shouldn't be used in training or while making predictions, but it should be okay for EDA in my opinion.",116763,2019-11-11 11:20:01,Gunes Evitan
Thanks for sharing it! This also explains why public/private split is 78/22. I have never seen larger public than private before. So they put all the leaky ones in public probably. ,116763,2019-11-11 19:25:49,Ahmet Erdem
"@gunesevitan, thanks for sharing. 
I could not see ""meter reading"" column in site0 files or am I looking in the wrong place?
UPDATE:
Never mind seen it.",116763,2019-11-11 11:26:03,YaGana Sheriff-Hussaini
"Well, it looks like Google Analytics competition now. We need clarification from competition host ASAP.",116840,2019-11-11 21:01:09,WispZero
"Anyway, Google analytics competition has finished as ML competition, not scraping competition.
I hope this competition finishes as ML competition. ",116840,2019-11-12 02:31:22,YouHan Lee
Me too. I have spent two weekends in this competition. So sad.,116840,2019-11-12 03:56:47,daishu
"If you mean ""ML competition"" by ""submit all 1e-4 values to get bronze"" then it does. Hope this one will find another way.",116840,2019-11-12 07:30:26,WispZero
"You are both right…
It was more of a true-forecasting classification challenge.

Predict, in November 2018
Log of Sum of revenue in microdollars in Dec 2018, Jan 2019
For band of 300k users seen in May-Sep(?) 2018

The revenues in microdollars were very large numbers - after log() they turned into ~20.
Statistically, only a few hundred would return in that window, so nearly all the targets were going to be zero, making zero or 1e-4 very strong baselines…
(Many could be very confident predictions of not returning, essentially 0. For those more likely to return it was better to hedge & predict ~1 or lower; whatever the ML said…)
In terms of fairness - it was like all true-forecasting challenges, virtually leak-proof…
The problem was that the setup above was entirely different to competition launch time (It was a bit odd, an improvement, but it voided a lot of early-starters efforts.) The rules changed: the majority deserted it, but Kaggle chose to keep the original competitor numbers, meaning hundreds of medals for the zero or 1e-4 baseline :/
You can see the scores for medalists here (relevant plot below 😮). The zero benchmark score was about 0.888. Comparing to the other plots in that Notebook, it is quite an outlier among competitions.
",116840,2019-11-16 13:30:52,James Trotman
It seems that more of the test labels are becoming known. site 1 was discovered but not shared so it's only a matter of time all the test labels will be shared and revealed slowly. It is becoming a scraping competition… I find myself looking through university building energy data instead of machine learning.,116840,2019-11-11 19:37:45,Tim Yee
"Thanks, I'll pass then. Good luck to you, you're bound by your good LB rank I'm afraid.",116840,2019-11-11 19:51:02,CPMP
"""Is this a webscraping competition?""
Oh Yes It seems more than illusion !
Thanks once again for your remarks, your warns, and your advices…",116840,2019-11-15 20:54:42,mezoganet
"I can confirm it is possible to be 2nd on the LB with a simple override of UCF data.
I don't know if all publicly available test labels are known or shared but I am confident that these would be shared as the community here is focused on learning by sharing. So I hope and think the challenge is still fair even if the leak is disappointing… Moreover there is no proof that private test data is inline with public LB ones.
@cpmpml I noticed web scraping is not a skill you want to exercise… I showed that I am able to use this skill and, I hope so, some others too. I am here to learn ML. This challenge is still, in my opinion, very… challenging and interesting and I guess you love that! I would enjoy so much to work with you to purpose a ML solution to this challenge. Of course I would do the web scraping (and more) if needed, you would not have to do it! Please contact me to discuss on how we could cooperate if you are interested. Everybody here would win/learn if you participated in this challenge.",116840,2019-11-12 16:21:14,Bertrand P
That's a generous offer but I'll pass.  Web scrapping will influence final ranking too much for my taste.,116840,2019-11-13 11:35:51,CPMP
"web scraping is not a skill you want to exercise = Strongly agreed.
I am here to learn ML = Strongly agreed.
no proof that private test data is inline with public LB ones = Someone already said that they jumped up to 1st place after they replace meter_recording with the leaked data.
My faith do not use these data and some externel datasets, It is unfair. 
Yes, someone says life itself is unfair. nope!! Not at all. Strongly disagreed.",116840,2019-11-13 17:17:51,Hanjoon Choe
"@cpmpml This is not a generous offer ;) My goal is to learn ML and I am sure you would be a fabulous prof/mentor. You would have shared more than I would/could have. I know that doing is the best way to learn and I think that learning by doing beside you would have been even better. I already wanted to purpose to work as a team in Microsoft challenge but I was sure you would have not consider my offer. I tried this time to not regret. My offer is still and will ever be opened! I am disappointed that you have chosen to not be part of this challenge because I would have learned more. I just respect it. In the same time I feel proud you did not just ignore my offer. Maybe you even considered it…
Kaggle is full of disappointments for me… ""Microsoft Malware Prediction"" challenge still does not make sense to me (https://www.kaggle.com/c/microsoft-malware-prediction/discussion/83946#490008) and I still think that I deserved a better reward to my investment. As a lot of the great competitors that gave all… Once again in ASHRAE challenge I am disappointed. I come to learn ML and I have to web scrape… I wonder if Kaggle is the better place to spend (a lot of) my time… But I am sure Kaggle team try to do the best, ASHRAE too, and every participant here seems to try to do the same. I benefit a lot of what is shared here. I benefit a lot of what you (and many others) shared and share. I am grateful to all of you for that… So I am going to keep fighting. I gonna fight with ML tools. I will, as I have done this weekend, just exploit external data shared by other as it is part of the challenge/rules. I do not like it but I have to do it. Narcism!
@hanjoonchoe ""Someone already said that they jumped up to 1st place after they replace meter_recording with the leaked data."" I jumped too to 1st place by replacing UCF data in a submission that scored 1.06 without exploiting the leak. It scored 1.04. I did that to verify (probe) if that data were used in the public LB. I was happy. Narcism! In the same time I was disappointed: this is a huge leak and mean more. The data can probably be used to more than replacement… Is it still a real world problem? A real ML problem? Are we just competing? But it does mean nothing about the data used in the private test. As far as I understand… This is to me the motivation to continue. I try to trust everyone: Kaggle team is going to make the choices to a fair (I think it exists ;)) competition and the competitors gonna play fair and share external data. Not too much narcism!",116840,2019-11-13 19:53:13,Bertrand P
"@pdnartreb Don't be disappointed by Kaggle.  Kaggle competition are hard, many of the best datascientists come and compete.  
In Malware you should have asked.  I see you did well there, being one of the few top 100 public teams to stay in top 100 in private.",116840,2019-11-14 04:19:12,CPMP
"
Rules clarification from Kaggle:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117357#latest-673345",116840,2019-11-14 21:45:03,Vishal 
"I am afraid it might end up being one - although to be fair, if you provide super detailed info like measurements for energy and weather but obfuscate the actual locations, you're practically asking for it …",116840,2019-11-11 19:29:20,Konrad Banachewicz
"Thanks, I'll pass then.   Good luck to you, you're bound by your good LB rank I'm afraid.",116840,2019-11-11 19:50:40,CPMP
"Thanks :-) To be honest, using the scraped public data as part of your training set is not that hard - so I was actually surprised more people have not done it so far. ",116840,2019-11-11 20:52:33,Konrad Banachewicz
"Maybe not hard, but not a skill I'd like to exercise ;)",116840,2019-11-11 22:48:17,CPMP
+1 ,116840,2019-11-14 21:21:07,Vishal 
"I have not been following this competition so I don't know what is going on, but I would like to remind everyone that Kaggle just added a new Dataset tier to everyone' profile (info here). Perhaps there is opportunity in this competition to scrap data and make Datasets to share with other Kagglers. You can learn data scraping and start acquiring Kaggle Dataset medals.",116840,2019-11-13 15:11:00,Chris Deotte
"
scrap data and make high quality Datasets to share with other Kagglers

Is that legal? I mean we'd need consent of the website owners, no?",116840,2019-11-13 15:23:50,Rishabh Agrahari
"Good question, I don't know the law. But if you read an online news article (or other public information), I think you are allowed to write and/or tell a friend what you read. ",116840,2019-11-13 16:04:22,Chris Deotte
"Most of the time those websites have robots.txt in their index. It is a standard used by websites to communicate with web crawlers. The standard informs the web crawler about which areas of the website should not be crawled. It is good practice to obey robots.txt.
Some websites even have sitemap.xml in their index which makes it easier for you to scrape data. If a website doesn't want you to take their data, they already have implemented a bot protection system, so don't worry.",116840,2019-11-13 17:27:48,Gunes Evitan
Like anybody cared about the dataset tier…,116840,2019-11-13 20:05:30,bluetrain
Some apparently do :P,116840,2019-11-13 20:19:54,Psi
"
Is that legal? I mean we'd need consent of the website owners, no?

A very relevant question indeed.  From the Terms page (link at the bottom of any Kaggle page):

You are responsible for all Content you contribute to the Services, and you represent and warrant you have all rights necessary to do so.

Usually, sites with data have a License page somewhere.  That's the best way to see if you can reproduce their data.  If there is nothing that says it is open source or public domain then copyright applies, and you must ask permission.",116840,2019-11-14 04:12:15,CPMP
"IANAL, and this is only the relevant US law, but databases/datasets have a separate copyright from the data they're scraped from. While downloading the web pages and uploading them to Kaggle would violate copyright, just scraping the data would not, as the database you create is its own copyrightable work. Additionally, publicly available data is perfectly legal to scrape, at least according to the rulings currently on a lawsuit involving LinkedIn.",116840,2019-11-15 21:37:44,Rob Rose
"The suit you point to isn't about copyright, it is about computer fraud and abuse act.  What you missed is that LinkedIn users agreed to make their data publicly available, i.e. make it public domain.  Therefore scrapping that data is fine.
I stand by what I wrote, and I advise anyone having doubt to consult with an attorney at law.",116840,2019-11-15 22:01:41,CPMP
"I want to, how many of you are only using features provided by the kaggle? I mean I have tried everything with those features but I was unable to get anything. Is it external features that do the magic???",116840,2019-11-19 14:34:41,Mukul Sharma
"On the other hand, taking out any possibility of getting close to a medal because who cares about scraping more than others, there is plenty of space to try out some more creative, most likely less performant, approach. 
Win-win. :)",116840,2019-11-11 19:45:40,Luca Basanisi
"There are other competitions where you can exercise ML skills and still get to the top if you're lucky and skilled.  Here it seems you need to get into web scrapping, from the other answers I got, which means that I will pass.",116840,2019-11-11 19:50:26,CPMP
"It is really a shame that they didn't notice such a clear flaw in the competition design indeed.
What I meant is that, hopefully, many people are competing only to learn and that aspect is not compromised by the fact that this competition is ruined beyond recovery.",116840,2019-11-11 20:06:09,Luca Basanisi
Tensorflow 2.0 competition seems promesing for learning. At my work i use tensorflow for productive modeling which is great for distributed training.,116840,2019-11-12 04:01:51,ragnar
I agree. It is an unacceptably large data leak that does not benefit the hosts of the competition.,116950,2019-11-12 12:05:44,Carlo Lepelaars
"My frustration doesn't come from the leaks (which are a common currency in Kaggle these days) but by the fact that the cv strategy is going to shift to a traditional 1 year of training and 2 years of validation with detection of seasonality, in the middle of the competition. I guess this is part of the game…",116950,2019-11-12 13:19:50,eagle4
"
1
",116950,2019-11-12 12:03:12,ragnar
plus 1,116950,2019-11-12 12:03:31,ragnar
"Kaggle just posted about clarification
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117357#latest-673345
",116950,2019-11-14 21:43:14,Vishal 
This Comment was deleted.,116950,2019-11-12 17:12:44,No user
I hope they'll find a better solution than this one ;),116950,2019-11-12 17:26:01,FabienDaniel
+1,116950,2019-11-13 21:54:36,Mashlyn
"Potentially, I am starting to suspect that all sites might be publicly collected at this point. :( Is it intentional to screw up the nature of kaggle?
━━━━━┓
┓┓┓┓┓┃/ . `(와장창)
┓┓┓┓┓ : . ＼○ノ
┓┓┓┓┓┃ ` /
┓┓┓┓┓┃ ノ) .
┓┓┓┓┓┃ ,
┓┓┓┓┓┃
┓┓┓┓┓┃
┓┓┓┓┓┃
┓┓┓┓┓┃
┓┓┓┓┓┃",118331,2019-11-20 20:05:00,Hanjoon Choe
Initial kernel I've published found 49 buildings but currently 74 buildings exactly matched in both UC Berkeley and ASHRAE data sets. This is equal to 1.286.941 instances in submission. Notice that submission set was 41.697.600 instances. This means that site 4 leak data is 3.08% of all submission data.,118331,2019-11-24 16:31:27,Sefik
"I suggest adding this to the ""data"" page's description for visibility. This is a big change. ",119261,2019-12-03 11:07:01,Dan Ofer
"When I read your message I understand that something wrong on site_0 - but a tiny bit confused.
Option 1 - readings for meter types 0  and 1 are all wrong.
or
Option 2 - only readings for meter type 0 are wrong.
I think your saying Option 2 - is that correct?
If it's option 1 - they were both wrong - than what conversion is needed.  
While the data description indicated that all 4 meters were in kwh - the information from the leak sites does not seem to support that - certainly looks like the meters are in different units - lbs, tons, whatever.   
Rather than simply pointing to the data description (which has now proved to be wrong) it would be nice of you did a deep dive on your data and supply a more convincing statement that all sites have been checked and all meter readings (expect site 0) are in kWh.  Because I think I see many that make no sense if truly kWh.",119261,2019-12-01 05:38:02,PC Jimmmy
"It seems like readings for meter  type 1 are wrong, isn't it?",119261,2019-12-02 03:02:42,Qinmeng_Guo
"yes - it seems to me that type 1 is wrong and type 0 is ok - but he used the word ""electric"" which I think is type 0.",119261,2019-12-02 21:08:05,PC Jimmmy
"
type 0 is ok

Nope. Based on the table of the referenced post, type 0 (electric) is in kBTU while the other sites are in kWh. Hence, type 0 should be updated to kWh. Regarding type 1 (Chilled), the unit for site 0 is kBTU while for sites 2 and 15 is Tons (1 ton (refrigeration) = 12000 Btu (IT)/hour).",119261,2019-12-11 19:12:24,Fernando Wittmann
Could you please explain why it's necessary to multiply by 3.14118 for scoring? Am I right that the evaluation is in  kWh?,119261,2019-11-28 20:54:40,Elvira
"I assume it is because the same error is in the test data as well, i.e. for site 0 are values are in kBTU unit in both train and test data.
Hence, if we convert train data to kWh, we need to convert back in our predictions.",119261,2019-11-29 08:30:12,Martin Elfstadius
"Martin Is following what you want to say if I want to do a code correction?
def correcterrormeter0modeluse(meterreading,metertype):
    meterreading = (meterreading(metertype == 0).astype(int)0.2931) + (meterreading+(metertype != 0).astype(int))
    return meterreading
def correcterrormeter0scoringuse(meterreading,metertype):
    meterreading = (meterreading(metertype == 0).astype(int)3.4118) + (meterreading+(metertype != 0).astype(int))
    return meterreading
So, to summaries, we will use correcterrormeter0modeluse for our train and valudation dataset and will use  and will use function **correcterrormeter0scoringuse   ** function  for our outcome in validation ",119261,2019-12-04 07:38:01,yogi
"yes, that's it.
I did it like so:
train.loc[(train['siteid'] == 0) & (train['meter'] == 0), 'meterreading'] = train[(train['siteid'] == 0) & (train['meter'] == 0)]['meterreading'] * 0.2931
Same same",119261,2019-12-10 14:02:28,Martin Elfstadius
Am I right that if you do modelling building-wise  or site-wise this changes essentially nothing?,119261,2019-11-28 13:17:52,Larry D.
"Think so, yes. I you have modeling elements that do cross site comparisons, you might have a problem",119261,2019-11-29 08:31:14,Martin Elfstadius
"Actually, buildingwise no, since it only applies to electric meters, absolute values within a building between meters will be affected. So, meter wise model requires no handling, building-wise does, IF you use relation between meters within building =)",119261,2019-12-10 14:05:31,Martin Elfstadius
Evaluation Exception: Submission must have 41697600 row,119261,2019-12-18 04:18:13,Amit
come on!,119261,2019-12-16 17:00:01,zeze
"So, this only affect 'train.csv'? Is that correct? Please confirm the meter reading units from original and desired. What then is the 'test.csv' supposed to predict in 'kWh' or 'bBTU'??",119261,2019-12-11 20:56:27,S_DS
"I multiply 0.2931 the electric meter readings for site 0 and then log1p before tranning, exem1 and then multiply 3.4118 before submit, but get very worse result, what's wrong with my understanding?",119261,2019-12-06 01:04:29,Jiang Siyuan
@djongo  did you consider the meter type? He refers to type 0 as of my understanding.,119261,2019-12-06 06:39:43,Shahules786
Yes， I considered the electric meter.,119261,2019-12-06 10:01:53,Jiang Siyuan
"Interesting, mine did not go worse or better, pretty much the same. From the CV I see they're different on the 5th digit",119261,2019-12-12 23:13:59,Jie Wu
Nice! Thanks!,121074,2019-12-12 19:24:21,Carlo Lepelaars
sounds great~,121074,2019-12-13 09:14:10,hardSnail
"Is ""business hours"" mean 8h each day and then 24-48 hours = 3-6 days?
",122296,2019-12-22 02:27:39,Mukharbek Organokov
@muhakabartay I've forgottent about Christmas holidays…,122296,2019-12-22 15:22:10,Roman Trotsyuk
"So, 48h left, thus we need new updates. 
",122296,2019-12-22 00:11:20,Mukharbek Organokov
"I've kicked off the process of rescoring submissions to remove leaked data from the private set. The public leaderboard will look odd while this is underway, but will return to its previous state. I didn't want to kick this off earlier in the week to avoid disrupting submissions that were already underway.
The final private set includes roughly 2.4 million data points. I will most likely be unable to respond to questions about the process until it is done as I need to focus on getting it and the cheat review screen completed.",122296,2019-12-20 01:18:31,Sohier Dane
"Can we re-select submission?  I want to choose my last submission as one final result, but it wasn't scored before end of competition. If that breaks the rule, please ignore it.
--update
I think kaggle should allow us to choose it as the final result before the end of competition even though the submission is in scoring.
--update 
My private score of last submission is 1.2413097349 which can get a gold. So sad.
",122296,2019-12-20 02:47:27,daishu
"
most of the teams now  😄 ",122296,2019-12-20 13:07:14,sandy1112
"Scoring update: our original rescoring job was taking longer to run than expected so I've moved the rescoring to a separate system. It's going much faster and is currently on track to finish over the weekend. Once that's done I will need to review and debug some edge cases. The main downside is that the rescore progress will no longer update in real time; there will likely not be any progress displayed until Monday regardless of how far we actually get.
Thank you all for your patience! Rest assured that we want to see leaderboard finalized before the holidays as much as you do :)",122296,2019-12-21 04:03:33,Sohier Dane
@sohier why not rescoring only the selected submissions?,122296,2019-12-21 04:07:20,Ahmet Erdem
"I'd guess that not everyone will have manually selected 2 submissions, so they still have to rescore all of them.  
I don't know what percentage of the teams preselected two, so some sort of hybrid system that does the teams with selected submissions and then grinds through all the submissions of those teams who didn't preselect may or may not really help much.",122296,2019-12-21 04:30:10,Bruce Young
"for the teams who didn't select 2 submissions,the private lb may be simply rescore with the  best two before competitions end,instead of rescoring all and choose best two,so what you worry about may be not a problem at all",122296,2019-12-21 05:20:31,Wu_Yiqun
"@mutantspore 
Selecting 2 submissions is a test I did once and I missed a silver medal… I will not play with that anymore.",122296,2019-12-21 07:36:26,mezoganet
"I think kaggle rescore every submission to detect the cheaters too. New accounts can just upload the submissions alone and if nobody checks how would kaggle know they are clones in case they use a different pc and ip address? This is my hypothesis only, but I have some sorts of evidence.",122296,2019-12-21 16:18:24,phi
"@pfichou 
what do you mean? Is it a bad practice to select submission before the competition ends?",122296,2019-12-21 16:31:13,Taco
"No, not a « bad » practice, but there is a risk of missing a good performance in which you did not believe. 
Science, technique, computation… they don’t need hope but proofs.",122296,2019-12-22 08:44:07,mezoganet
"@sohier 
Is there a chance that we'll see the progress displayed today?",122296,2019-12-23 10:55:59,Stanislav Blinov
Hopefully it'll be just in time for Christmas :),122296,2019-12-23 19:00:35,Wei Hao Khoong
"@addisonhoward 
Thank you for rescoring. But, I think rescoring all submits will take a while.
Why not calculate in this order?
private -> public(final sub) -> public(other)",122296,2019-12-21 01:56:38,Yust
Very much helpful ,122296,2019-12-22 04:01:31,Saikot Roy
"Hold the LeaderBoard until December 24 at night and it is Santa Claus who discovers the Private Leaderboard, don't you think?",122296,2019-12-19 12:27:06,Manuel Campos
You think Santa has time for any other competition than his own right now? ,122296,2019-12-19 16:44:57,Addison Howard
It doesn't look like it is progressing.,122296,2019-12-20 21:55:16,Ahmet Erdem
"I might be mistaken, but I think its rescoring all submissions for everyone, not just the selected ones. that will take a while",122296,2019-12-20 22:07:45,Tim Yee
"exactly, over all the submissions we made as a team, only one (first one) has been calculated.",122296,2019-12-21 09:19:17,Firat Gonen
"private 22%(until yesterday) -> 11%(now)
why?????",122296,2019-12-20 04:49:51,Yust
because they removed the leaked data from Pvt test set.,122296,2019-12-20 05:02:58,sandy1112
"Is it officially announced?
( I do not think it ’s good to change the rules later😣  )",122296,2019-12-20 05:08:08,Yoshi
Organizers had confirmed (on discussion posts) when the leaks came out that they will remove the leaked data from the Pvt. Test set,122296,2019-12-20 05:19:25,sandy1112
"I only used a 2016 training set and did not include leaked data during model training that's why I struggle to improve my score. Anyone who used leaked data as part of model training had a great advantage of improving their score in Private LB. Good luck to all of us. Anyway, the Public (from 78% to 89%) and Private LB dataset distribution (from 22% to 11%) was not officially announced.",122296,2019-12-20 05:41:14,devai01
I think the admin should have removed the leak data and restarted the competition.,122296,2019-12-20 05:51:51,Yoshi
"
Private LB dataset distribution (from 22% to 11%) was not officially announced

It was.",122296,2019-12-20 10:17:12,Stanislav Blinov
"So, does you mean we can't see the private leaderboard today?",122296,2019-12-19 15:40:41,Yoshi
"There were 3000+ teams, but now they are only 1800+. what happened?",122296,2019-12-20 10:12:07,Navid Hakimi
banhammer (seems like a lot of participants used LB probing with fake accounts),122296,2019-12-20 15:06:23,Nikita Detkov
"""our anti-cheating removals"" - what do you understand by term ""cheating""? Private code sharing or something else?",122296,2019-12-20 18:50:27,Roman Trotsyuk
Based on @detkov comment it seems they can detect fake accounts that were made for the purpose of LB probing. Quite shocking and interesting that more than half of the participants were actually just some fake accounts. But not sure what other sort of cheatings could be detected!?,122296,2019-12-23 13:28:57,Novin Shahroudi
"According to    Addison Howard:  > Don't fear! We didn't remove 2,000 of you as cheaters 😉 , but we've encountered a few snags in the meantime.",122296,2019-12-23 15:46:07,Masoud
Why am I removed from the public Leaderboard?,122296,2019-12-20 05:12:30,Zhang Yunfei
"You are not removed mate, kaggle is evaluating submissions and team who have been evaluated with 1 or both submissions are placed at the bottom as of now.
Scroll down you will see your team 😊",122296,2019-12-20 05:24:01,HarshitMehta
"waw, thank you . it's shocked me/👍 ",122296,2019-12-20 05:31:26,Zhang Yunfei
"I wonder how the final submissions will be selected for teams who didn't chose them manually. 
If the automatic selection will be based on public LB score after re-scoring, then some people may be uncomfortable with this. It will choose different submits than we expected at the moment of competition end.
If the automatic selection will be based on public LB before re-scoring, then private LB can be calculated without waiting for re-scoring of (much bigger) public one. ",122296,2019-12-21 04:56:56,Josef Slavicek
"It must be chosen from the best public LB submissions before rescoring (which is already known). I'm not sure why the public LB data / rows should change at all.
Quoting @sohier 

The public leaderboard will look odd while this is underway, but will return to its previous state.

So probably the original public LB will be back which is best.
IMO, just rescoring all submissions to the new private LB is all that's required.",122296,2019-12-21 05:12:17,Vopani
Does Private leaderboard include only 11% data or Combined (89% public + 11 % private data),122296,2019-12-20 11:07:13,Utkarsh Tripathi
Just the 11%,122296,2019-12-20 11:08:54,sandy1112
Does it mean that the Pvt LB won't be avilable before Tuesday next week? or will you be putting up a preliminary Pvt LB tomorrow and the final version will come  after a few days?,122296,2019-12-19 17:22:31,sandy1112
it doesn't make sense to put a preliminary one ,122296,2019-12-19 17:31:50,Jie Lu
@jielu0728  it has happened in past where a preliminary LB was released at first (e.g. last Santander Competition) and the medals were awarded after 2 weeks once the LB was finalized.,122296,2019-12-19 17:34:31,sandy1112
This is all I would say. You got to have patience man.,122296,2019-12-20 13:42:08,YaGana Sheriff-Hussaini
"Only the final private LB will be released, no preliminary.",122296,2019-12-20 16:42:50,Sohier Dane
"Thanks for sharing. I really liked that you split each building's meters in half instead of just splitting entire training data in half. Of all the things I tried, this one managed to slip by me. One of the better splits I tried was splitting by hour, so first 12 hours and last 12 hours of the day, except I never included this model in my final blend. It seemed crazy at the time, but surprisingly the validation of both training and leaked test labels (sites 0,1,2,4,15) showed that it indeed performed better than normal half/half split. Reading your post makes me realize why it performed better and why I should have trusted my cv as I never submitted that model to LB.",122863,2019-12-23 14:10:48,Tim Yee
"Thanks for this really good explanation and amazing solution. Could you please elaborate on how you looked for outliers in building's readings? What I mean is: There are over a 1000 buildings, so it's rather impractical to plot one by one and determine in each one what to remove. Did you automate this process by any chance? If so how did you do it?",122863,2019-12-23 16:38:16,Thiago Preischadt
"
Did you automate this process by any chance?

Well, I was using a biological neural network. My own :)
I was manually cleaning the data. To decide what might (and should) be cleaned I looked at every plot for every building. It really takes less than a second per plot to realize if there is anything wrong with data.",122863,2019-12-24 08:49:00,Roman
"I did the same. I figured if doing the hard and less attractive work meant achieving a better score, I just had to bite the bullet. Some meters could be easily cleaned by looking at heat maps, those were easy to clean, but the rest required going through each individual building meter plot. There was diminishing return as well. ",122863,2019-12-24 11:30:24,Tim Yee
"I understand, thanks a lot for clearing that up! Also really liked your validation technique.",122863,2019-12-24 17:12:42,Thiago Preischadt
Thanks for the detailed explanation ! CONGRATS ! ,122863,2019-12-26 16:06:12,CaesarLupum
Thanks for the detailed explanation. Plese elaborate how you looked for outliers.,122863,2019-12-24 08:16:05,Vishnu R
Nice work!👍 ,122863,2019-12-23 15:22:39,Oussa
"Thanks for this. Re CV strategy, therefore you built a model per building and meter?",122863,2019-12-23 15:10:58,Jonathan Mallia
Thanks ,122863,2019-12-23 14:25:10,Debanjan Sarkar
"Leaks:
0: https://www.kaggle.com/gunesevitan/ashrae-ucf-spider-and-eda-full-test-labels
1: https://www.kaggle.com/mpware/ucl-data-leakage-episode-2
2: https://www.kaggle.com/pdnartreb/scrap-asu-data
3: https://media.githubusercontent.com/media/buds-lab/island-of-misfit-buildings/master/data/raw/DGS_322_Buildings-15m-By_Building-DST-gap-filled-3-2-18-508pm.csv
4:https://www.kaggle.com/serengil/ucb-data-leakage-site-4-81-buildings
15: https://www.kaggle.com/pp2file/ashrae-site15-cornell
Trends:
14: http://energytracker.fm.virginia.edu/index.html   (can see overall trends using graphs)
University Terms
Site 0: University of Central Florida 
http://cf.smartcatalogiq.com/en/2015-2016/Catalog/About-College-of-Central-Florida/College-Calendar
http://cf.smartcatalogiq.com/en/2016-2017/Catalog/About-College-of-Central-Florida/College-Calendar
http://cf.smartcatalogiq.com/en/2017-2018/Catalog/About-College-of-Central-Florida/College-Calendar
http://cf.smartcatalogiq.com/en/2018-2019/Catalog/About-College-of-Central-Florida/College-Calendar
site1: University College London
https://www.ucl.ac.uk/estates/roombooking/calendar/1516.pdf
https://www.ucl.ac.uk/estates/roombooking/calendar/1617.pdf
https://www.ucl.ac.uk/estates/roombooking/calendar/1718.pdf
https://www.ucl.ac.uk/estates/roombooking/calendar/1819.pdf
site2: Arizona State University 
https://catalog.arizona.edu/calendar/2015-2016-academic-calendar
https://catalog.arizona.edu/calendar/2016-2017-academic-calendar
https://catalog.arizona.edu/2017-2018-academic-calendar
https://catalog.arizona.edu/calendar/2018-2019-academic-calendar
Site4: University of California Berkeley
https://registrar.berkeley.edu/sites/default/files/pdf/UCB_AcademicCalendar_2015-16.pdf
https://registrar.berkeley.edu/sites/default/files/pdf/UCB_AcademicCalendar_2016-17_V7.pdf
https://registrar.berkeley.edu/sites/default/files/pdf/UCB_AcademicCalendar_2017-18_V3.pdf
https://registrar.berkeley.edu/sites/default/files/pdf/UCB_AcademicCalendar_2018-19_V4.pdf
site5: University of Southampton 
https://www.southampton.ac.uk/assets/imported/transforms/content-block/UsefulDownloads_Download/A4EE46BFA84746158B25E5CD179BE5DE/term-dates-201020.pdf
Site 7 and 11: McGill
https://www.mcgill.ca/importantdates/key-dates/past-dates/key-dates-2015-16
https://www.mcgill.ca/importantdates/key-dates/past-dates/key-dates-2016-17
https://www.mcgill.ca/importantdates/key-dates/past-dates/key-dates-2017-18
https://www.mcgill.ca/importantdates/key-dates/past-dates/key-dates-2018-19
Site 9: University of Texas
https://registrar.utexas.edu/calendars/15-16
https://registrar.utexas.edu/calendars/16-17
https://registrar.utexas.edu/calendars/17-18
https://registrar.utexas.edu/calendars/18-19
site10 : Weber State University
https://www.weber.edu/Registration/Class_Schedule_Archive.html
Site 13: University of Minnesotta
http://www.d.umn.edu/calendar/academic_cal_15-16.html
http://www.d.umn.edu/calendar/academic_cal_16-17.html
http://www.d.umn.edu/calendar/academic_cal_17-18.html
http://www.d.umn.edu/calendar/academic_cal_18-19.html
site14: University of Virginia
https://www2.virginia.edu/registrar/calenold.html (old)
https://www2.virginia.edu/registrar/calendar.html (current)
site15: Cornell
http://courses.cornell.edu/mime/media/26/11593/2015-2016+Academic+Calendar.pdf
https://registrar.cornell.edu/academic-calendar/2016-2017
https://registrar.cornell.edu/academic-calendar/2017-2018
https://registrar.cornell.edu/academic-calendar/2018-2019",112841,2019-12-12 22:51:22,KublaKahn
"I use the leaked data in the following kernels
0:https://www.kaggle.com/gunesevitan/ashrae-ucf-spider-and-eda-full-test-labels
1:https://www.kaggle.com/mpware/ucl-data-leakage-episode-2
2:https://www.kaggle.com/pdnartreb/scrap-asu-data
4:https://www.kaggle.com/serengil/ucb-data-leakage-site-4-81-buildings
15:https://www.kaggle.com/pp2file/ashrae-site15-cornell
(sorry for repeating)
【JUST MY THOUGHT】

We can use the above data.
(I think it is not necessary for each of us to declare the exploitation of these data in this thread. Because these data already meet the external data condition written in rules 7C)
There may be other leak, but at least for me there seems to be no other leaked data that meet the condition of external data written in rules. 
",112841,2019-12-11 11:07:44,Hiroyuki Namba
@sohier Do you already know which sites are not scored in private?,112841,2019-12-13 08:16:51,Oleg Knaub
"https://github.com/dr-prodigy/python-holidays
https://www.officeholidays.com/countries/
https://www.timeanddate.com/holidays/
https://www.gov.uk/bank-holidays
https://blog.holidaylettings.co.uk/
https://www.edwardbryantschool.co.uk/docs/term_dates/term-dates-2018-2019.pdf
https://www.kaggle.com/c/ashrae-energy-prediction/notebooks",112841,2019-12-12 09:12:42,Jonny Lee
https://www.worldweatheronline.com/,112841,2019-12-10 03:47:18,suda
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-09 11:44:48,Dmitry Maslov
"Quite many of the buildings in the dataset are located a university campuses, even many of those with types 'office', 'lodging', 'parking'. It may help to use the academic calendars to predict consumption patterns related to school breaks (these shift a bit from year to year).
For starters: 
site 9: 2015-16 academic calendar for UT Austin https://registrar.utexas.edu/calendars/15-16
site 15: 2015-16 academic calendar for Cornell http://courses.cornell.edu/mime/media/26/11593/2015-2016+Academic+Calendar.pdf
Notice for example, that spring breaks do not coincide between these two. 
I presume that accounting for academic events (breaks, exams) may improve model results …",112841,2019-12-05 18:04:57,Poe Dator
"There is a potential data leak for site 9 (Austin, TX):
Many of the buildings are located within main campus of UT Austin. This may be confirmed by cross-checking competition data vs their buildings list here: https://utdirect.utexas.edu/apps/campus/buildings/nlogon/facilities/UTM/
The University seems to have a comprehensive site with energy management information https://utilities.utexas.edu/efficiency/energy-management. Since this site is only available to the campus residents on intranet/VPN, it is not possible to use/access for most of us. However, given the size of UT, it may not be ruled out that some competition participants from U. of Texas may get an advantage. Could someone verify this?",112841,2019-11-27 22:00:07,Poe Dator
https://www.kaggle.com/yamsam/ashrae-leak-data-station,112841,2019-12-10 11:38:51,lmssdd
https://www.kaggle.com/yamsam/ashrae-leak-data-station,112841,2019-12-10 05:58:42,LJH
"It is revealed that site 4 is UC Berkeley. The university provides it energy consumption data publicly. The following kernel extracts the historical energy consumption data for several buildings in the college.
https://www.kaggle.com/serengil/ucb-data-leakage-site-4",112841,2019-11-20 19:44:07,Sefik
"Site 1 UCL building metadata and meter readings
https://platform.carbonculture.net/communities/ucl/30/apps/assets/list/place/",112841,2019-11-15 14:38:51,Gunes Evitan
"OK, another one revealed ? As I see you climbed up in the LB 😄 ",112841,2019-11-15 16:21:03,Jie Lu
"Thanks for this, but it is terrible news! One more site data publicly available!
It´s a pity, this could have been a very interesting competition…",112841,2019-11-15 17:01:35,Panos
Another one? OMG…😭,112841,2019-11-17 05:42:33,PGiN
"Has anyone planned to make UCL dataset and to make it public ?
If no one, i will try it next week.",112841,2019-11-17 10:42:41,Isamu Yamashita
"@gunesevitan , may I ask if cm.asu.edu is scrapable ? if yes, how do you proceed ? I can't find the actual table or a way to query the website.",112841,2019-11-17 14:38:34,eagle4
"It is actually very easy to scrape http://cm.asu.edu/. It took me 15 minutes to write the spider and you don't even need Scrapy.

Open the website on chrome and copy your session cookie because server won't return a valid response without it
Find the request that returns building ids from XHR tab in network, and collect the building ids (use session cookie in headers)
Find the request that returns meter readings and pass building id to it while looping through scraped building ids and accumulate the meter readings in a dataframe (use session cookie in every request)

One big problem is the server returns the response in approximately  3 minutes, so it'll take too much time to scrape it completely",112841,2019-11-17 14:52:56,Gunes Evitan
"@yamsam  Yes, https://www.kaggle.com/mpware/ucl-data-leakage-episode-2",112841,2019-11-17 15:40:40,MPWARE
Thank you @gunesevitan @mpware . You are so great!! I will update my kernel.,112841,2019-11-17 21:14:45,Isamu Yamashita
Organizers should have provided the link to scrape the data rather than sharing train/test set :D,112841,2019-11-18 15:51:26,HarshitMehta
@gunesevitan can't you multi-thread a webscrape? can't each thread request information independently?,112841,2019-11-19 05:28:23,Tim Yee
"You can send multi-threaded requests, but there is no point of doing it. You still have to wait for server to send you response back. The time consuming operation is waiting for response.
That's why web scraping frameworks like Scrapy are single-threaded, but they utilize concurrency with coroutines. Non-blocking IO is way better than multi-threading in web scraping.",112841,2019-11-19 05:46:23,Gunes Evitan
"Guys can you please move this valuable discussion on Best Practices on Web Scraping to a separate thread, out of this dataset thread? Other things that you would have to worry about with commercial sites are rate-limiting, rotating IP addresses, etc.",112841,2019-11-19 07:38:01,Stephen McInerney
@sohier We are asking to clarify using web scrapped data as predicted values. That is clearly a data leak.,112841,2019-11-11 21:05:45,WispZero
"Site 2 is not posted here yet, so let's make it official
http://cm.asu.edu/",112841,2019-11-19 09:54:49,Gunes Evitan
,112841,2019-11-19 10:19:47,Jie Lu
sorry somebody was faster than me…,112841,2019-11-19 13:30:13,eagle4
"I think we would be better finding all the leaks as soon as possible, so we can focus on modeling.
Attached are the exact locations / stations that provided the weather information for the competition.
If one look for universities near by those airports there is a high likelihood of finding the source of buildings energy consumption. I do not think all of them are possible to be scraped.
High probability that site 15 is Cornell and data can be scraped at  http://portal.emcs.cornell.edu.
Others are just initial guesses.
site,station,city,state,country,lat,lon,altitude,timezone,tz
0,ORLANDO INTERNATIONAL AIRPORT,ORLANDO,FL,US,+28.434,-81.325,+27.4,-5,US/Eastern
University of Central Florida
1,HEATHROW,LONDON,,UK,+51.478,-0.461,+25.3,0,Europe/London
2,PHOENIX SKY HARBOR INTL AIRPORT,PHOENIX,AZ,US,+33.428,-112.004,+337.4,-7,US/Mountain
Arizona State University
3,RONALD REAGAN WASHINGTON NATL AP,WASHINGTON,VA,US,+38.847,-77.035,+3.1,-5,US/Eastern
4,METRO OAKLAND INTL AIRPORT,OAKLAND,CA,US,+37.721,-122.221,+1.8,-8,US/Pacific
UC Berkeley
5,CARDIFF,CARDIFF,,UK,+51.397,-3.343,+67.1,0,Europe/London
?? Cardiff University
6,CHARLOTTESVILLE-ALBEMARLE ARPT,CHARLOTTESVILLE,VA,US,+38.137,-78.455,+195.1,-5,US/Eastern
?? University of Virginia
7,OTTAWA CDA RCS ONT,OTTAWA,ON,CA,+45.383,-75.717,+79.0,-5,Canada/Eastern
? University of Ottawa
8,ORLANDO INTERNATIONAL AIRPORT,ORLANDO,FL,US,+28.434,-81.325,+27.4,-5,US/Eastern
??
9,AUSTIN-CAMP MABRY,AUSTIN,TX,US,+30.321,-97.760,+204.2,-6,US/Central
? University of Texas at Austin
10,OGDEN-HINCKLEY AIRPORT,OGDEN,UT,US,+41.196,-112.011,+1362.5,-7,US/Mountain
?? Weber state university
11,OTTAWA CDA RCS ONT,OTTAWA,ON,CA,+45.383,-75.717,+79.0,-5,Canada/Eastern
??
12,DUBLIN,DUBLIN,,EI,+53.421,-6.270,+73.8,0,Europe/Dublin
?? Dublin City University 
13,MINNEAPOLIS-ST PAUL INTERNATIONAL AP,MINNEAPOLIS,MN,US,+44.883,-93.229,+265.8,-6,US/Central
?? University of Minnesota
14,TRENTON MERCER AIRPORT,TRENTON,NJ,US,+40.277,-74.816,+57.9,-5,US/Eastern
?? Princeton University
15,REGIONAL AIRPORT,ITHACA,NY,US,+42.483,-76.467,+335.0,-5,US/Eastern
Cornell University",112841,2019-11-21 03:29:45,uortolan
"I agree. I can confirm Site 15 is Cornell University. It requires some form fuzzy matching (sqft) for 70 of the buildings. The other 54/124 are exact match. Link to building metadata. That should save you guys some time. Don't forget to upvote my dataset ;)
edit: to anyone looking to scrape/compile/match all the cornell data, you'll need to average the meter readings within each hour (sum and divide by 4) because they are in 15 minute increments, so 

00:00:00 = (00:00:00 + 00:15:00 + 00:30:00 + 00:45:00) / 4. 

And you'll have to deal with the missing readings so good luck.",112841,2019-11-21 05:05:27,Tim Yee
"Someone tried Princeton energy data (site14 potentially)? It is available online at:
https://tiger-energy.appspot.com/data-download
But it does not seem to match.",112841,2019-11-27 22:37:48,MPWARE
"Site 15 revealed
http://portal.emcs.cornell.edu/GannettHealthCenter
refering to https://www.kaggle.com/pp2file/ashrae-site15-cornell",112841,2019-11-24 19:38:00,Jie Lu
"for a number of buildings / sites it is possible to find information in the Internet using area and construction year as a key.  I made a more detailed post here.
Below is a list of sources that mention buildings from a few areas: 
0: Orlando, FL
https://www.oeis.ucf.edu/buildings
2: Tempe, Arizona 
https://www.asu.edu/purchasing/bids/pdfs/231906_ex_6A.pdf
3: DC
https://dgs.dc.gov/sites/default/files/dc/sites/dgs/service_content/attachments/Attachment%20A-%20Municipal%20Facilities%5b1%5d.pdf
8: Orlando, FL
https://www.orlando.gov/files/sharedassets/public/initiatives/bewes/090418_cityoforlando_bldginfo.pdf
9: Austin, TX
http://www.thecb.state.tx.us/reports/xls/7559.xlsx
13: TwinCities, MN
https://mn.b3benchmarking.com/Report?OrganizationID=1337&ReturnTo=Higher+Education
14: Charlottesville, VA
https://www.fm.virginia.edu/docs/conditionReport/2018-19FacilitiesConditionReport.pdf (has year built data)",112841,2019-11-04 20:35:31,Poe Dator
"hey so just like… based on that I'm gonna go scrape all ucf's energy data… That's within the rules right? I'm allowed to just go get that?
doesn't feel right…",112841,2019-11-08 09:59:33,Louka Ewington-Pitsos
"Site 3 only building metadata, no meter readings
https://opendata.dc.gov/datasets/building-energy-benchmarks/data",112841,2019-11-23 16:45:46,Gunes Evitan
have，data，just you don't find it.,112841,2019-11-25 00:08:49,Zhang Yunfei
"calendar of each year
https://www.timeanddate.com/calendar/",112841,2019-11-11 04:27:18,Draconda
"There is this Building Data Genome Project Database  and mention of Southampton University as data source in the related article of Miller, Meggers
Judging by sq ft and year built numbers it has about 250-300 overlaps with this contest's buildings table. The possible matches are for site_ids 1,2,4,5,6,13,15. ",112841,2019-11-04 17:11:20,Poe Dator
"Pain in the neck to get data - pretty much only one city at a time for me.  But should be some useful info for the heat load part of energy usage.
https://www.nrel.gov/gis/data-tools.html",112841,2019-10-18 17:40:07,PC Jimmmy
"@pcjimmmy cheers for sharing, I assume you're mainly talking about the ""solar data"" stuff here?  https://www.nrel.gov/gis/tools.html, specifically, ""Federal Energy Management Program Screening Map""?",112841,2019-11-08 07:58:41,Louka Ewington-Pitsos
yes - solar stuff - tried just adding a few cities at different latitudes - no help - so probably need to know the specific cities for each site.,112841,2019-11-08 15:23:30,PC Jimmmy
"dang, thanks for dropping that nugget of info jimmy! I've done a decent amount of web scraping before so maybe I'll see if there's a way to grab it en-masse and have a go with more data. ",112841,2019-11-09 06:20:00,Louka Ewington-Pitsos
"Pretty confident on a number of the sites for what city is what  - but have the solar data low on my to do list.  My current features do a very good job of making predictions to match the training year but have made little progress in getting leader board improvements to match.  Pretty sure the solar data will help predict the training year, but not sure it will help for the test years.  
When I do get to add solar data than orlando will be the first city I grab - it's two sites and  I have lots of confidence that the two sites are really orlando.",112841,2019-11-09 06:52:34,PC Jimmmy
"Hourly solar data time series for US & Canada:

https://maps.nrel.gov/nsrdb-viewer/?
Choose ""PSMv3"" (Physical Solar Model) for satellite-based solar irradiance data
SELECT YEARS: 2016-2018
SELECT ATTRIBUTES: select all attributes
SELECT DOWNLOAD OPTIONS:
Include leap day
UTC (don't convert to local time), si its easier to synchronize with the
yes to half hour intervals. Use only the half-hour values for solar data, and the whole-hour values for the terrestrial parameters (e.g. temperature) to fit missing data.
Note that you have to check synchronization. I noticed that site0 data should be moved back approx 1.5 hours to synchronize temperature best.

UK data:

I used terrestrial data. Register for a user account at UK CEDA (Centre for Environmental Data Analysis), then download specific sites via:
https://catalogue.ceda.ac.uk/?q=solar&sort_by=relevance
The sites measure generally only GHI hourly.

Note: Both sources require appropriate attribution when used",112841,2019-12-11 19:54:55,Peter G. Schild
"historical weather data in Ireland and London.
https://www.kaggle.com/conorrot/irish-weather-hourly-data
https://www.kaggle.com/jeanmidev/smart-meters-in-london",112841,2019-11-03 03:39:35,marshi
https://www.kaggle.com/selfishgene/historical-hourly-weather-data,112841,2019-10-23 12:40:31,Isamu Yamashita
"https://github.com/buds-lab/the-building-data-genome-project
https://github.com/buds-lab/building-prediction-benchmarking",112841,2019-10-22 14:52:01,KALE
"https://www.eia.gov/
Not sure why so many of the datasets only available in monthly chunks but good data here for energy consumption",112841,2019-10-20 19:04:35,PC Jimmmy
https://www.kaggle.com/mimoudata/ashrae-2-lightgbm-without-leak-data,112841,2019-12-19 15:31:19,Huizi Ren
Was anyone able to fill in yearbuilt and floorcount using these leaks? ,112841,2019-12-14 00:40:07,SDanielZafar
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-13 04:02:02,Mr Nik
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
https://www.kaggle.com/selfishgene/historical-hourly-weather-data",112841,2019-12-12 22:55:23,Alan Clappis
Airport data from OpenFlights.org available at: https://openflights.org/data.html (and directly from https://raw.githubusercontent.com/jpatokal/openflights/master/data/airports.dat ),112841,2019-12-12 20:32:37,Andre
weather information - https://www.ncei.noaa.gov/data/global-hourly/archive/csv/,112841,2019-12-12 16:50:19,KyeongHwanKim
"We have used below kernels and data from external website : 

 https://www.kaggle.com/wuliaokaola/ashrae-maybe-this-can-make-public-lb-some-useful
https://www.kaggle.com/yamsam/new-ucf-starter-kernel
https://www.kaggle.com/mpware/ucl-data-leakage-episode-2
https://www.kaggle.com/pdnartreb/asu-buildings-energy-consumption
https://www.kaggle.com/serengil/ucb-data-leakage-site-4-81-buildings
https://www.kaggle.com/pp2file/ashrae-site15-cornell
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115040#latest-667889
https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature
https://power.larc.nasa.gov/data-access-viewer/
",112841,2019-12-12 12:41:40,Salai Madhavan
"I use the leaked data in the following kernels
0:https://www.kaggle.com/gunesevitan/ashrae-ucf-spider-and-eda-full-test-labels
1:https://www.kaggle.com/mpware/ucl-data-leakage-episode-2
2:https://www.kaggle.com/pdnartreb/scrap-asu-data
4:https://www.kaggle.com/serengil/ucb-data-leakage-site-4-81-buildings
15:https://www.kaggle.com/pp2file/ashrae-site15-cornell
https://www.kaggle.com/yamsam/ashrae-leak-data-station
(sorry for repeating)",112841,2019-12-12 11:32:51,Alexander Deineha
"https://pypi.org/project/meteocalc/
For weather features",112841,2019-12-12 10:10:21,Gunes Evitan
"https://www.kaggle.com/gunesevitan/ashrae-ucf-spider-and-eda-full-test-labels
https://www.kaggle.com/mpware/ucl-data-leakage-episode-2
https://www.kaggle.com/pdnartreb/scrap-asu-data
https://www.kaggle.com/serengil/ucb-data-leakage-site-4-81-buildings
https://www.kaggle.com/pp2file/ashrae-site15-cornell
https://www.kaggle.com/yamsam/ashrae-leak-data-station",112841,2019-12-11 23:23:55,b15d030
"As far as i know, those platform can be used to get hourly energy data (and are publicly available data)
http://cm.asu.edu/
https://www.oeis.ucf.edu/
http://portal.emcs.cornell.edu/
https://platform.carbonculture.net/
https://engagementdashboard.com/ucb/ucb/
https://engagementdashboard.com/ucsb/ucsb
https://engagementdashboard.com/sd36/sd36
https://tiger-energy.appspot.com/data-download",112841,2019-12-11 18:50:34,thomasdelattre
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-11 16:48:02,firstbloodY
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-11 05:11:02,Apiao
holidays library,112841,2019-12-10 17:35:00,Maxisoft
"https://opendata.dc.gov/datasets/building-energy-benchmarks/data
http://portal.emcs.cornell.edu/GannettHealthCenter
https://platform.carbonculture.net/communities/ucl/30/apps/assets/list/place/
https://www.kaggle.com/selfishgene/historical-hourly-weather-data
https://www.data.jma.go.jp/gmd/cpd/monitor/climatview/frame.php
https://www.kaggle.com/selfishgene/historical-hourly-weather-data
https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/numerical-weather-prediction https://www.orlando.gov/files/sharedassets/public/initiatives/bewes/090418_cityoforlando_bldginfo.pdf http://www.thecb.state.tx.us/reports/xls/7559.xlsx https://mn.b3benchmarking.com/Report?OrganizationID=1337&ReturnTo=Higher+Education https://www.fm.virginia.edu/docs/conditionReport/2018",112841,2019-12-09 07:52:20,Debu
"Hi Dane,
Your post on the modified rules says: ""Using publicly available meter readings is within the rules, assuming the general restrictions are also followed such as posting a link to the data in the external data thread by the entry deadline (December 12th)."" So, every participant has to publish the list of external data sources?
Instead, since you will be doing a review after the deadline of December 12, may I request that you publish a list of accepted External data sites? In that way, we do not have to bother about which external link cannot be used.
At this point (thanks to the enterprise of the relevant active Kagglers), I am using the external data sites mentioned in the following Kernels:
ASHRAE - UCF Spider and EDA (Full Test Labels) v3
UCL: Data Leakage (Episode 2) v1
ASU train and scraped test data v7
UCB: Data Leakage (Site 4) v14
ASHRAE-site15-cornell v3
Thanks to the initiative of  @gunesevitan, @mpware, @poedator, @pdnartreb, @serengil @IsamuYamashita and @pp2file",112841,2019-12-09 07:40:34,Debu
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission
And data that other kagglers announced",112841,2019-12-09 07:08:09,Yue Zhang
https://www.officeholidays.com/ for holiday info.,112841,2019-12-08 03:10:40,Shiu-Tang Li
"I used doc2vec with wikipedia dumps https://dumps.wikimedia.org/enwiki/
It is probably not relevant to what the thread is meant to be but just in case…",112841,2019-12-07 13:17:22,Nathan Zhai
"used leaked data from sites 0,1,2,4,15",112841,2019-12-07 07:46:53,Mohammad Farzanullah
"Hi @sohier  As per my understanding site 0,1,2,4 & 15 data has been leaked. Does it mean that these sites will be excluded from private leaderboard test data?",112841,2019-12-07 05:43:25,PrasunMishra
Would formulas (involving constants only) be considered External Data and must be disclosed?,112841,2019-12-06 16:00:39,Peter KK
"There's no need to worry about something trivial that you might find in a textbook, like unit conversion factors. However, I can't give a blanket pass since ""a formula with constants"" could contain almost anything. If that doesn't answer your question feel free to send me a direct message.",112841,2019-12-06 22:18:13,Sohier Dane
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-06 07:31:20,Davide Stenner
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-05 06:45:01,jacobfrey
"https://www.kaggle.com/yamsam/ashrae-leak-data-station
All these leak data stations used in my submission",112841,2019-12-03 02:47:04,Clancey_Lee
so far. maybe have 6 site energy  data  has be leajed.,112841,2019-11-25 00:13:50,Zhang Yunfei
Will publicly available values be excluded from the final validation or are there plans to jitter them such that competition is made more fair in the interest of intellectual honesty?  Doing so could encourage honest Kagglers to continue in earnest…,112841,2019-11-21 06:43:37,jzking
See https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117357,112841,2019-11-21 15:13:05,Sohier Dane
"Not sure how useful this is going to be, but I'm planning on experimenting with https://www.esrl.noaa.gov/gmd/grad/solcalc/ and pysolar.",112841,2019-11-15 22:42:42,Rob Rose
https://www.timeanddate.com/worldclock/,112841,2019-11-14 21:05:17,Bill Holst
"https://en.wikipedia.org/wiki/List_of_cities_by_average_temperature
https://www.wunderground.com/
https://github.com/buds-lab/island-of-misfit-buildings",112841,2019-11-13 03:23:55,TongLi
"Lots of weather data on NOAA website:
https://www.ncdc.noaa.gov/data-access/model-data/model-datasets/numerical-weather-prediction
https://www.ncdc.noaa.gov/data-access/radar-data/noaa-big-data-project
https://www.ncdc.noaa.gov/data-access/land-based-station-data",112841,2019-11-11 19:50:51,uortolan
"https://www.sports-reference.com/cfb/years
college football schedule - 2016 to present
high consumption of beer - assume the same for energy but TBD",112841,2019-11-09 00:24:57,PC Jimmmy
https://darksky.net/dev,112841,2019-11-03 08:54:30,marshi
"https://www.data.jma.go.jp/gmd/cpd/monitor/climatview/frame.php
It will help to find site id location",112841,2019-10-31 08:41:33,Isamu Yamashita
"https://www.ncei.noaa.gov/support/access-data-service-api-user-documentation
https://www.ashrae.org/communities/regions",112841,2019-10-25 07:32:10,LeiZhou
This Comment was deleted.,112841,2019-12-08 05:41:56,No user
This Comment was deleted.,112841,2019-12-08 05:39:29,No user
This Comment was deleted.,112841,2019-11-16 22:20:40,No user
"Am I missing something?
 I thought the point of this challenge was to build an estimation-model to show what the energy consumption of a building would have been without energy saving changes that has been made to it. Thus it is used restrospectively to compare the actual energy consumption, and the estimated one if changes had not been made (our model)…. Therefore actual historical weather data for the period we are interested in would be available…. was my thought.",112887,2019-10-16 09:59:16,Magnus Aunevik-Berntsen
"@magnusab I think you are correct. From the description of the competition:
Developing energy savings has two key elements: Forecasting future energy usage without improvements, and forecasting energy use after a specific set of improvements have been implemented, like the installation and purchase of investment-grade meters, whose prices continue to fall.",112887,2019-10-16 11:00:02,arnab
"@magnusab - you maybe right, I'm not quite clear on exactly what the sponsors are trying to get out of this competition. The description says:
Developing energy savings has two key elements: Forecasting future energy usage without improvements, and forecasting energy use after a specific set of improvements have been implemented, like the installation and purchase of investment-grade meters, whose prices continue to fall. One issue preventing more aggressive growth of the energy markets are the lack of cost-effective, accurate, and scalable procedures for forecasting energy use.

In this competition, you’ll develop accurate predictions of metered building energy usage in the following areas: chilled water, electric, natural gas, hot water, and steam meters. The data comes from over 1,000 buildings over a three-year timeframe.

With better estimates of these energy-saving investments, large scale investors and financial institutions will be more inclined to invest in this area to enable progress in building efficiencies.

I'm not sure what they mean here, or they hope to achieve it by having us predict meter readings:
Developing energy savings has two key elements: Forecasting future energy usage without improvements, and forecasting energy use after a specific set of improvements have been implemented, like the installation and purchase of investment-grade meters, whose prices continue to fall.
I still think my point stands. Clearly they have the actual meter data from 2017-2018 which they will be using to score our submission. What is valuable to them is being able to have models that predict the energy consumption for 2020 and beyond for buildings with and without the improvements. If that's what the goal then creating models that use actualized weather data will overestimate their predictive power.
Please correct me if I'm missing something…",112887,2019-10-16 13:47:32,Rob Mulla
"Re reading your post again and when you say this:

Thus it is used restrospectively to compare the actual energy consumption, and the estimated one if changes had not been made (our model)

If this were the case - how would they score our submissions? If the changes were made then they have no ""true"" measurement if the changes were not made.",112887,2019-10-16 14:10:16,Rob Mulla
"They have 100% real data before change.
They made changes let's say for 50% of buildings
50% without changes went to test set to evaluate model.
If model is good they will predict 50% with change and see consumption difference to prove energy saving and eco transformation strategy.",112887,2019-10-16 14:30:14,Konstantin Yakovlev
"@kyakovlev I see what you're saying, but I don't follow the logic. If that were the case it would be a classic A/B test. Why would they need our models to predict anything? They already have the meter data for the 50% without changes (control group) and 50% who made changes (treatment group) for the years we are predicting.
I've asked this question to the sponsor in their discussion thread here. Hopefully they can clear things up.
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112985#latest-650506",112887,2019-10-16 14:40:25,Rob Mulla
"Ohh, it's easy.
Money))) 
They go to NY municipality

Let's do eco/energy transformation
But we don't have money for it
Give us consumption historical data
Here it is
Our model predicts SUCH HUGE savings for future two yeas
WOW!! No more words, take our money, do the job!

So now we are predicting consumption for next two years with changes our without we don't know.
Probably in dataset are similar buildings with and without change, probably only with change, or only without. ASHRAE people want to know how good we can predict consumption for such long periods. 
Of course it's not about money itself - Money is a key to make decisions by people how not care about making world better. Less brutal earth consumption will be - more ""Grand"" prefix for grandgrandgrandson will be real.",112887,2019-10-16 14:47:45,Konstantin Yakovlev
"Hi Rob, 
I  try to explain your confusion:

You motivation is: predict the future energy usage.
ASHRAE's motivation is: after install new building equipments, what is the potential energy reducing if without the installations? This is different from A/B testing,** since they never saved 50% data without changes.** They have renewed all equipments! What they want to is to find the 'counterfactual' model to calculate the energy reducing.
",112887,2019-10-28 04:48:55,Doggy
"@robikscube 
When it comes to scoring us. Dont they just score our model on how well it estimates for that point in time on actual data….. Then to utilize it in practice you train it on data up until you did the change on the building, then you feed it with data for the period after making the change… you compare the ""estimated energy expenditure without change"" with actual expenditure and you put a dollar sign on how much eneergy the change saved you?
Im in a hurry, so didnt think this properly through, but please point out where I went wrong anyway :) Just so I get it clearified :) ",112887,2019-10-18 17:29:06,Magnus Aunevik-Berntsen
"I agree with your points on weather and the short time-frame of the data. I do, however, appreciate that the competition is looking at this from a long-term perspective and energy efficiency. EE occurs behind the meter so it’s not easy to really know it’s impact. Usually, there are profiles on hypothetical demand for appliances, etc. But these are generalized shapes… not something that really sells to individual customers on the fence with spending money on EE.
The benefit of this competition as opposed to real-world is that the historical load demand and weather have been provided. If we had to also forecast weather, most of our models would do much poorer. Most utilities rely on ""simple"" approaches such as 20 year averages. This approach actually didn't pan out well for the past few years where heat waves has driven temperatures higher than usual.. you can image how that affected our forecastsl However, the sub-seasonal RODEO forecast competition is promising… maybe one day. Anyway, the point is that weather forecasting in itself is time consuming (see RODEO competition) and would eat up a lot of time in this competition…
… which in the end a hypothetical approach to estimating EE. Utilities moves at a glacial pace. Throwing in something like, “we estimate your savings to be ___ using our ML/AI models” would make almost anyone’s head explode when talking about utilities and energy savings.",112887,2019-10-16 10:39:35,Thomas Yokota
"IMHO the use case should have been the following: 
Based on today's data[ X(day=today) ], predict the energy consumption N days in future [Y(day=today+ N Days)]. 
Being a competition organized by Kaggle, I am finding the current goal to be little fishy. :-)",112887,2019-10-16 09:38:44,arnab
"Amazing Summary..
Thanks for Sharing Thoughts… @robikscube ",112887,2019-10-16 06:16:41,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
Indeed! It will to be a lot of fun!,112887,2019-10-16 03:17:26,Mohammed Tayor
"I totally agree with you (especially on the aspect on the part about using weather data) @robikscube.
Microsoft recently released their building energy saving solution which uses multiple machine learning models (1 of which is a weather forecasting model).
https://www.microsoft.com/en-us/itshowcase/microsoft-uses-machine-learning-to-develop-smart-energy-solutions",112887,2019-10-16 08:10:44,Daniel
"An interesting comment and thread.  I'm new to Kaggle and enjoyed the challenge, but I also had questions about the utility of any models developed in this process.  I've have experience reviewing building energy performance, including simulation models.  A few thoughts below. Would be interested in other perspectives. 

The challenge goal was to accurately predict energy use that under a hypothetical scenario would have occurred if not for the installation of energy savings measures.  The difference in observed post-installation energy use and the predicted energy use would be deemed the realized energy savings. Energy Service Contacts, were a building manager only pays for realized energy savings, carefully consider how energy savings will be determined and documented. More accurate ways to establish ""what if"" energy use assuming no energy savings measures were installed would be useful. 
Accurately predicting ""annual"" energy use would be useful as it is strongly correlated with annual energy costs. There may be errors in predicting each month, day or hour but as long as the negatives offset the positives then the sum over a year can be accurate. I had models predicting annual energy use for 2017 & 2018 that were very close to the actual energy use.  But, these models only ran in the middle of pack using the RMSLE.  I wonder if using the RMSLE results in higher error when predicting annual energy use as opposed hourly energy use. The RMSLE would bias toward lower energy consumption hours, while a simple mean error would bias toward the higher energy use hours and I would think be more consistent with a simple annual sum for energy. 
I did not check on how well my models did for peak periods, i.e. the highest energy use during each month, or the annual peak.  I also suspect a RMSLE fitted model might be more likely to under-predict the energy use in these periods as opposed to say using a simple mean error. These peaks which can set distribution , transmission or capacity costs would be important to get right.
In real life the first thing a modeler would do I think would be to clean up the data: establish units (kBTU, therms, kWh..) and remove any data that doesn't pass the laugh test. In my case I found data cleaning of limited value given (1) the Private data set also had data issues, so cleaning up the Train data set could actually result in a worse score, and (2) I chose not to view or use the leaked data as doing so would seem to violate the spirit of the challenge.  Seems like the goal should be to take sloppy data and make clean predictions with it -- which would suggest the Private data set should be cleaned up and made as close as possible to actual energy use.  Taking ""garbage"" data and intentionally making ""garbage"" predictions doesn't seem like it would be very useful. A model that could take garbage data and make ""clean"" predictions, on the other hand, could be very useful. 

Good luck with your Private scoring",112887,2019-12-19 17:43:11,Dave Beavers
"[update 26Oct]
Competition by DataDriven Power Laws: Forecasting Energy Consumption  
@willkoehrsen was top3 there, you can top3 solutions in the official github.",112958,2019-10-26 05:39:20,Nanashi
Do you know how to get the data of  ‘ Power Laws: Forecasting Energy Consumption’？,112958,2019-10-26 07:20:00,jajaja
"I don't think it's possible. Generally in DrivenData, data remains available only during the competition.",112958,2019-10-26 09:58:02,arnab
@jesucristo  Thanks for this useful resource!!,112958,2019-11-01 09:10:49,Pradeep Muniasamy
"Thanks, you have my upvote.",112958,2019-10-17 11:07:20,Kenechukwu
👍 ,112958,2019-10-17 10:53:32,jajaja
Thanks @jesucristo ! This is really useful 😄,112958,2019-10-16 11:02:48,Federico Raimondi
"from pandas.tseries.holiday import USFederalHolidayCalendar as calendar
dates_range = pd.date_range(start='2015-12-31', end='2019-01-01')
us_holidays = calendar().holidays(start=dates_range.min(), end=dates_range.max())

df['is_holiday'] = (df['timestamp'].dt.date.astype('datetime64').isin(us_holidays)).astype(np.int8)

You can use pandas datetime for it. The problem here is that we don't know site_id geo location (US or not)",113286,2019-10-18 12:16:04,Konstantin Yakovlev
"Nice job - I agree with your guess that the data is USA cities.  While a world wide organization now,  data matching this has been collected in USA for last decade.
Would recommend some changes however if your working on USA assumption you should go all the way!
Days with high energy use possible.
Day after Thanksgiving.
Superbowl Sunday.
Other Days with Low.
Christmas eve.
Any four day long weekend connected with holidays.
Days where lots of folks still work and might not get much of an energy drop.
Columbus Day
Here in Pennsylvania over one million folks out in woods hunting deer on the Monday after Thanksgiving. ",113286,2019-10-18 23:09:06,PC Jimmmy
"Hi @pcjimmmy , 
Very useful remarks)
Will try it is in my code.
I am not an expert in US holidays just took days that I found. Thanks for help!",113286,2019-10-18 23:16:57,Kostiantyn Isaienkov
Another low energy - day before new years.  ,113286,2019-10-19 00:14:02,PC Jimmmy
"I plotted the average air temperature yearly groupby month.
A lot of the yearly temperature can be found in US.
check this out -> https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113772#latest-654627",113286,2019-10-22 07:21:07,CC Joshua 
"Cheers @isaienkov 
I couldn't find where the data is coming from. Are you sure it's US [only]?",113286,2019-10-18 10:27:16,Henrique Mendonça
"@hmendonca , I took data from here http://www.calendarpedia.com/holidays/federal-holidays-2016.html.
But you are completely correct, I am not sure that it is only US buildings.  ",113286,2019-10-18 10:35:51,Kostiantyn Isaienkov
"It is probably US only since the ASHRAE stands for The American Society of Heating, Refrigerating and Air-Conditioning Engineers",113286,2019-10-18 10:52:00,Gunes Evitan
"@gunesevitan , 
That's why I did my hypothesis. ",113286,2019-10-18 10:56:10,Kostiantyn Isaienkov
"Its probably USA because studies with this exact data breakdown have occurred multiple times in last decade.  I can find summary reports for those years, but yet to find any actual data to supplement modeling.",113286,2019-10-19 07:08:52,PC Jimmmy
"@gunreddy @isaienkov @pcjimmmy 
Fair point. But a quick google search will review that ASHRAE has a strong global presence.

https://www.ashraeindia.org/
https://midlands.ashrae.uk/
https://ashrae.org.sg/
http://www.ashrae.org.hk/
In fact, many building authorities follow ASHRAE guidelines with regards to HVAC.
https://www.researchgate.net/profile/Sam_C_M_Hui/publication/2410830_Review_of_building_energy_standards_and_implications_for_Hong_Kong/links/543fb7d30cf2be1758cf51cb/Review-of-building-energy-standards-and-implications-for-Hong-Kong.pdf
https://www.jstage.jst.go.jp/article/shasetaikai/2014.6/0/2014.6_17/_article/-char/ja/
http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.549.6457&rep=rep1&type=pdf
",113286,2019-10-19 18:17:49,Daniel
"Did not check your links yet - but do any include annual summary reports that contain the ASHRAE buildings, etc ?  
I was very modestly involved in data collection several years ago for a building in DOE survey.  It's a decent investment to collect the data, standardize things, etc for the building owner and data collector.  Someone has to pay for that survey effort - I know it's been done in USA by DOE.  You can find reports on USA for summary of the years where surveys made.
Next point - plot the temperatures over the 3 years.  They are certainly north hemisphere values.  For example, not going to match Hong Kong weather patterns.
Last point - it's your model.  Do it your way :)",113286,2019-10-19 21:21:14,PC Jimmmy
"The data description mentions: ""The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world."" 
The data might be from different parts of the world as well? Thoughts?",113286,2019-10-21 02:37:25,Pratik Gandhi
Which one are considered long weekends? :))),113286,2019-10-18 14:51:24,Adrian Zinovei
This Comment was deleted.,113286,2019-11-05 22:51:27,No user
"LSTM solution
https://oar.a-star.edu.sg/jspui/bitstream/123456789/2386/3/ADMA_62.pdf
""PowerLSTM: Power Demand Forecasting Using Long Short-Term Memory Neural Network""


",113544,2019-10-22 15:06:14,Heng CherKeng
"i suppose the deep network framework is like this?
",113544,2019-10-21 08:50:58,Heng CherKeng
"
not sure if ""forward-backward translation"" would work ????",113544,2019-10-21 09:10:23,Heng CherKeng
"predict the bounds too (for post processing, etc)
",113544,2019-10-21 09:20:57,Heng CherKeng
"http://www.ibpsa.org/proceedings/BS2017/BS2017_142.pdf
http://cs109-energy.github.io/
http://harvardcgbc.org/cgbc-launches-online-gaussian-processes-forecasting-tool-to-analyze-building-energy-consumption/
""Existing tools normalize energy consumption by weather using simple regression methods, while the center utilizes machine learning with the Gaussian process which requires fewer training data points and higher prediction accuracy compared to other machine learning techniques. ""
but i think link is dead :(

demo video: https://www.youtube.com/watch?v=NkETYwz81W8&feature=youtu.be",113544,2019-10-21 14:07:23,Heng CherKeng
"useful
Energy Basics Part 2 -- The Baseline Model
https://www.youtube.com/watch?v=R7zo6SwDwis
Energy Basics Part 3 -- Energy Performance Tracking
https://www.youtube.com/watch?v=Xny7OjCiCtU",113544,2019-10-21 13:37:34,Heng CherKeng
Thanks a lot for sharing. Will be following your work.,113544,2019-10-28 15:55:36,arnab
"https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/43795
https://www.kaggle.com/c/web-traffic-time-series-forecasting/discussion/39370
With great github repos (the second one served as reference for winning solutions on other competitions )",113544,2019-10-20 12:56:22,Serigne 
"Well I am new to data science and neural networks. I formed a keras model, having 5 layers with 512 nodes (relu activation, normal initialization) and 1 layer each as input and output. Trained the model multiple times (used adam optimizer, mae loss, 500+ epochs with 5 to 6 digit batch size) and it achieved 1.216 RMSLE in the submission. I used following notebook for feature engineering (the author achieved 1.08 using GBM that too without leaks!!): https://www.kaggle.com/aitude/ashrae-kfold-lightgbm-without-leak-1-08
I think Heng's idea of considering time series feature and using sequence models like LSTM is a good one.
I'd like to know which NN architecture could achieve better result. Anyone tried LSTM or any sequence models?",113544,2019-12-18 13:19:12,フセイン
"I made a detection code.
import numpy as np
import pandas as pd

train_df = pd.read_csv(""../input/train.csv"")

for building_id in range(1449):
    train_gb = train_df[train_df['building_id'] == building_id].groupby(""meter"")

    for meter, tmp_df in train_gb:
        print(""building_id: {}, meter: {}"".format(building_id, meter))
        data = tmp_df['meter_reading'].values
        splited_value = np.split(data, np.where(data[1:] != data[:-1])[0] + 1)
        splited_date = np.split(tmp_df.timestamp.values, np.where(data[1:] != data[:-1])[0] + 1)
        for i, x in enumerate(splited_date):
            if len(x) > 24:
                print(""length: {},\t{}-{},\tvalue: {}"".format(len(x), x[0], x[-1], splited_value[i][0]))
        print()
",113774,2019-10-22 14:26:33,marshi
"Have you made any progress on this , i noticed the same thing today ? . Any help will be appreciated ",113774,2019-11-03 07:36:16,AdityaVikramSingh
"I am not worried about the flat regions, they say they won't score them and I believe they will. But in my plots I see a lot of weird stuff going on (like random offsets suddenly added for a few weeks,  not-looking random oscillations around the average value..), which look strange, but almost undetectable. That worries me, not sure I want to spend time on such a competition. ",113774,2019-11-05 23:53:52,andrea cazzaniga
"Looks like potential Null values… 
From the official data description: ""There are gaps in some of the meter readings for both the train and test sets. Gaps in the test set are not revealed or scored.""
Looks like we have some work to do cleaning the data!",113774,2019-10-26 19:03:32,S D
"For real life, we usually meet the same problem, missing data due to some connection error if the data is collected through network, or forget to record the data if the data is collected by people. etc.",113774,2019-11-01 09:23:04,LongYin/杰少
"Instrument failure ?
Maybe or Kaggle noise just for us as a gift….",113774,2019-10-26 18:14:28,mezoganet
"
marshi wrote:
  If 2017-2018 (test) data have time series like this, we have to predict this failure and it is difficult problem.

What if a failure is solved? Do you want to predict also when a failure will be solved?",113774,2019-10-22 16:36:25,Davide Stenner
"I just tried that in another competition several days ago, and the performance is mmm… 
We'd better not expect too much from a new coming library 👀 ",115079,2019-10-31 21:32:03,Jie Lu
We’ll wait and see next future…,115079,2019-10-31 22:29:32,mezoganet
"Hmm ok, thanks for letting us know. Our team will experiment with it and maybe publish a public kernel using it.",115079,2019-11-01 20:09:10,Carlo Lepelaars
"As far as I understand the focus is on providing uncertainties for the predictions, so it could be useful in some contexts even if the performances are not the same as the usual boosting libraries",115079,2019-11-01 21:50:39,bluetrain
"I quickly read the paper(I am not an expert however.) It approximates distributions in which the expectation of targets are the highest. Using proper scoring rule(for example MLE or CRPS), It gradually/sequentially update parameters and scalings for each targets and each base learners by small change in divergence between after update and before update in such a way that it maximize scoring rule. That is what I understood.(It might be not exactly correct :P)",115079,2019-11-02 02:13:49,Hanjoon Choe
"
The results are promising , but it is still in development. So many of the features we need are not implemented yet. There will be small control over tuning the model.",115079,2019-10-31 06:12:10,Khairul Islam
I will try NGBoost in the next comp and see the results.,115079,2019-11-03 05:41:41,Prashant Banerjee
"Yes, its looks very promising! Haven't seen many public Kaggle kernels yet using it, expect for this kernel.",115079,2019-11-01 20:08:22,Carlo Lepelaars
Maybe good for the NFL contest,115079,2019-11-02 17:56:36,manu
"look amazing, I will look the paper. Thanks for the information. ",115079,2019-11-02 13:12:30,RodrigoVillalonga
Will it be the new XgBoost ? ,115079,2019-10-31 20:30:41,mezoganet
"Amazing & Awesome
Thanks for Sharing…!! @nroman ",115248,2019-11-03 12:14:25,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
awesome thanks for sharing! 👍👍,115248,2019-11-02 03:57:52,YGSEO
This is great! Thanks for sharing! Are there any buildings that stand out?,115248,2019-11-01 20:06:28,Carlo Lepelaars
There are plenty of them.,115248,2019-11-01 20:39:55,Roman
👍 Do you see a common theme among buildings with weird behaviors? It seems that there are some power outages and/or meter malfunctions in some buildings. Our team mostly did analysis on the meters and sites. ,115248,2019-11-02 12:25:57,Carlo Lepelaars
did you perform clustering analysis on building and build validation strategies based on identified clusters?,115248,2019-12-24 11:42:55,devai01
"No, I wasn't doing any clustering-based analysis.",115248,2019-12-24 12:31:20,Roman
Really helpful. Thanks @nroman 👍 ,115248,2019-12-24 08:09:47,Vishnu R
Amazing!!,115248,2019-12-08 21:09:38,flyingpiggy
Thanks for Sharing…!!,115248,2019-11-13 11:48:37,Chanhu
"I have created three notebooks capturing the absolute value of the meter reading for all the buildings across. Tried to create one notebook initially, but it was not getting rendered at all. Hopefully, it would be helpful. 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115850",115248,2019-11-05 17:24:54,arnab
I built one.  Takes forever and no improvement.,116330,2019-11-08 17:45:09,Larry Schuster
Thank you for the information! What would you think of building a model for every building? Would it make sense to do that?,116330,2019-11-08 13:58:56,Carlo Lepelaars
Building level is too granular and there isn't sufficient data points for a good stable model. My experiments with building level models didn't give good results so I dropped the idea.,116330,2019-11-09 07:22:40,Vopani
Our team has built very simple building level models and it is performing decently on lb 1.12 but the only bottle neck is that we are out of features which can help us improve,116330,2019-11-09 11:37:22,AdityaVikramSingh
In your code you are uploading leak files.  Can you share those files?  What was the source?,116330,2019-11-26 16:39:58,Alexander Konstantinidis
"Done. Those are just compiled from all the public kernels that share leak: Sites 0, 1, 2, 4, 15",116330,2019-11-26 16:59:04,Vopani
"Got 1.12 LB score with site wise model, thanks a lot! ",116330,2019-11-16 16:21:07,Yunxiao Song
"If we do try to create per building models, the feature set that remains constant for a particular building needs to be dropped. For instance, attributes like site_id, year_built, floor_count etc will remain the same throughout a particular building_id. Most of the learning from these features will happen across the building_ids. So, per building model might not be that big an improvement.",116330,2019-11-12 21:20:47,Arjun Sehajpal
They seem to be very rare so they will be washed out by rmsle.,116589,2019-11-10 16:18:31,watermelon man
"Seriously? When are we going to get a clean competition again???
Who is putting a kernel with all the override that I will upvote for sure?",116589,2019-11-10 15:09:27,eagle4
2 submissions ;-),116589,2019-11-10 12:47:16,authman
I notice the same reading also but it doesn't make sense how can meter reading be in negative value.,116589,2019-11-10 10:26:26,Mr Loke
"My theory is they are random power outages. That's why meter suddenly outputs a huge negative number (when the electricity goes off). When it comes back, there is a huge positive number.",116589,2019-11-10 10:34:07,Gunes Evitan
"Or perhaps you just have to change sign ? 
I really don’t know, but this could be an option..
But the magnitude is strange anyway. Unless you divide it by 10 ?",116589,2019-11-10 12:13:10,mezoganet
What happens to the test performance when you remove the positive outlier and the negative row?,116589,2019-11-26 18:21:07,neeraj
"The difference between them seems to be the true reading for both readings. For example (1479-1209)/2 = 135 kWh, which is the value of their closest neighbors. ",116589,2019-11-26 15:59:05,Fernando Wittmann
"You might have negative readings if power were being produced on site, but that seems unlikely in this case.",116589,2019-11-12 17:09:37,Marcie Wallace
"Are you talking about this data for building 77? 
https://www.oeis.ucf.edu/buildings/68",116589,2019-11-10 13:40:20,MPWARE
"Nope, it is from this building.
https://www.oeis.ucf.edu/buildings/131",116589,2019-11-10 13:46:12,Gunes Evitan
"Thanks, in fact I had 4 candidates for this building:

I was able to retrieve and join on [""squarefeet"", ""primaryuse"", ""year_built""] but I have a lot of buildings with two or more candidates. Did you figure out how to select the correct one? Download and compare data?",116589,2019-11-10 16:28:43,MPWARE
Yes I compared them manually and merged them to the original test set. Maybe I'll share the kernel later.,116589,2019-11-10 16:36:15,Gunes Evitan
"It's a bit weird that a few test data reading_meter are available. Impact is limited to site 0 currently, did you check if site 0 is in the 22% (with score improvement just by copy/paste the data)? They should be discarded in final score so it will just help us to improve model, otherwise it should be considered as data leakage.
Update: 52 buildings matching on UCF, 7 more to check manually.
[  0,   2,   3,   4,   6,   7,   8,  10,  11,  16,  18,  20,  22,
        23,  24,  29,  30,  31,  33,  34,  35,  40,  51,  52,  54,  56,
        57,  58,  59,  60,  61,  62,  63,  68,  69,  70,  71,  72,  73,
        74,  75,  76,  77,  78,  86,  87,  88,  91,  94,  97,  98, 103]

Update: Also @gunesevitan did you notice the 0.2843 coefficient to match Chilled water?",116589,2019-11-10 16:50:44,MPWARE
Did you check the impact on LB of replacing the predictions from your models by the data you scrapped from ucf ? It's a pity if such a leakage indeed exists ..,116589,2019-11-10 20:34:56,FabienDaniel
"I'm going to publish a notebook with this leakage but I'm not using it yet. 
Update: Done. https://www.kaggle.com/mpware/ucf-data-limited-leakage",116589,2019-11-10 21:56:00,MPWARE
"@fabiendaniel his team very suddenly and jumped to first place. I'm about 99% sure the scraped data was at least partly responsible. I'm planning to add it myself later this evening, so if I'm suddenly in silver you'll have your answer. ",116589,2019-11-11 06:27:11,Louka Ewington-Pitsos
"Since this data leak became public in external data thread several days ago (site 0 = UCF) we tried to check whether parsed data perfectly match those readings in test set, and yes it is:
https://www.kaggle.com/mpware/ucf-data-limited-leakage/comments#670060",116589,2019-11-11 06:54:48,Fred Navruzov
This Comment was deleted.,116589,2019-11-10 13:38:47,No user
"Thank you very much for sharing, as well as for your considerations about the meaning of fairness. I believe that many people (including me) who participated in this competition to improve their skills and enhance knowledge are glad to know that despite the fact that we all competing with each other, there is a great feeling of mutual help, respect and support.",118047,2019-11-20 01:45:56,Stanislav Lykov
"Thank you for your sharing. I verified your data. It can improve the public LB about 0.05 (1.03 -> 0.98).
So there are three sites(0, 1, 2) have been found and opened. I wonder to know which is the next.",118047,2019-11-19 16:02:46,Jonny Lee
"To be complete, building_id 245 is missing.
Hopefully this is the last leak and we can now focused on ML…",118047,2019-11-19 17:25:47,Bertrand P
"excellent job.
Btw, in order to learn something new in this competition, can you please disclose the code (tricks) to download the data ?",118047,2019-11-19 14:56:38,eagle4
"Thanks!
The local notebook that I used is very dirty… Moreover, it is the first notebook that I try to publish and I encountered some problems. But I will try to answer to your request. Either here or on a notebook. I just need some time. I hope you saved some time by my share but I did not ;)",118047,2019-11-19 15:46:23,Bertrand P
"It is a very simple spider.

First, you find your session cookie from request headers in Chrome. It looks like this. You use it in the headers while sending requests to next urls.
Cookie: __cfduid=d148f1f668892c657356c1b09cdd123171572891266; DG5SESSION=96B48C7EE436665D2277CACD5283BFAA
Second, you find the ajax call which returns the building ids and store the building_ids in memory.
http://cm.asu.edu/dgdb?db=VES&amp;query=%5Bcm%5D.%5Bdbo%5D.%5BpCM_Select_Building_List_By_Campus%5D+%40selCampus+%3D+%22Tempe%22%2C+%40selOrderBy%3D%22bldgname%22%2C+%40selAscDesc%3D%22ASC%22%3B
Finally, fetch the building meter readings from this url.
'http://cm.asu.edu/dgdb?db=VES&amp;query=%5Bcm%5D.%5Bdbo%5D.%5BpCM_Retrieve_Utility_Data_By_Campus_Building%5D+%40selCampus+%3D+%22Tempe%22%2C+%40selBldg+%3D+%22{}%22%2C%40selPeriod+%3D+%22Custom+Dates%22%2C%40selInterval+%3D+%22Hourly%22%2C%40selBeginDate+%3D+%222016-01-01%22%2C%40selEndDate+%3D+%222018-12-31%22%3B'.format(building_id)
",118047,2019-11-20 05:38:45,Gunes Evitan
@chabir I have updated the kernel (https://www.kaggle.com/pdnartreb/scrap-asu-data) to show how I did the scraping. It took (probably too much) time to me so I hope you will learn something!,118047,2019-11-20 12:34:33,Bertrand P
merci beaucoup ! I learnt something !,118047,2019-11-20 14:34:34,eagle4
"Thank you for your sharing. You have done great work !!
I will make new highway kernel soon",118047,2019-11-19 21:33:13,Isamu Yamashita
This Comment was deleted.,118047,2019-11-21 12:31:59,No user
Possibly the worst Kaggle competition ever?,118777,2019-11-24 19:35:09,bluetrain
Might be !!! Might be !,118777,2019-11-25 21:31:18,mezoganet
Google Analytics Customer Revenue Prediction is the worst game,118777,2019-11-26 08:27:07,Neo Zhao
"When I initially started this competition, the best was around 1.15. Now its around 0.96. It has become more of who is able to identify the leaks /from which site and share the data to the competitors in this competition.",118777,2019-11-24 14:15:50,Manoj Prabhakar
"""more of who is able to identify the leaks "" 
Yesss, this is what about leaks ! Not a Competition, just leaks.
A pity !",118777,2019-11-25 21:33:16,mezoganet
"1.08 with no leaks is pretty easily achievable. I'm not sure how much better folks have achieved, but I'd guess 1.05 would be a reasonable target.",118777,2019-11-26 00:35:39,Robert Stockton
It is not the case for me though. ㅠ_ㅠ,118777,2019-11-26 04:46:02,Hanjoon Choe
You did some nice stuff if you achieved 0.99 without leaks.,118777,2019-11-26 05:34:55,Tejash Shah
"I started this competition in order to learn about time series but with the time here, the competition is more about finding leakage and exploiting it. All the recent kernels talk about leaks. I have already lost interest and just come here to check new techniques.",118777,2019-11-25 05:12:50,Tejash Shah
"Same with you. Just for finding skills dealing with time series. However, leaks everywhere😂 ",118777,2019-12-02 12:06:35,Orig1n
"This competition should be renamed as ""Leaks only"" competition!😂 ",118777,2019-11-27 02:57:50,Shahules786
That is why people can not improve their score by using the existing data anymore.,118777,2019-12-05 05:49:06,Ali_sher
"@alisherianvar yeh,I know.I am facing the same situation.",118777,2019-12-05 06:43:45,Shahules786
You did some nice stuff if you achieved 0.99 without leaks.,118777,2019-12-17 15:55:01,zeze
"any competition where to win you have to spend lots (or really any) time cleaning or hand identifying bits of data…  is a competition where you are doing their work for them to win.  training and picking algorithms or trying to create features is one thing. becoming an analyst / expert in their data (to find bad/missing data) is another. I realize many of the competitions are won that way. probably most but I'm never gonna do that. So I quit a while back too. 
i did make a new change to my algorithm that lets me fit GBM sets of training/test data based on groups in the data  that then expand and get larger (in a cascading sort of way once improvements cant be found) works pretty well, does tend to fit to that data which might be an overfit if the data is noisy (like this competition) but i'm happy with it. so there's that. :D  (but to that point my local score was like .91 the LB score was 1.22 .. likely cause of data cleanup issues that i'm like ""yep, not gonna deal with it"")",118777,2019-12-12 20:02:29,j_scheibel
"
any competition where to win you have to spend lots (or really any) time cleaning or hand identifying bits of data… is a competition where you are doing their work for them to win. training and picking algorithms or trying to create features is one thing. becoming an analyst / expert in their data (to find bad/missing data) is another.

Here's where we differ. From the standpoint of ""clean competition"", you may be right, but from the standpoint of ""real-world data science"", this is much more real than the average Kaggle dataset. In my time as an ML professional, the vast majority of my efforts were in obtaining clean data. Feature engineering was secondary, and algorithm selection was almost a non-issue. I've had to become a domain expert in diapers (as a confirmed bachelor), automobiles, and yams just in order to get to the ML part of my job.
I'm not a fan of the leaks and the web-scraping, but I consider the data cleanup to be quite a legitimate aspect of this, and any other, competition. On the other hand, there's lots of competitions out there, and it's good that you can choose not to bother with this one.",118777,2019-12-13 02:11:07,Robert Stockton
"The only real world data science i do routinely is with stocks.  And my data comes in as clean as it can be but of course it costs money too. So other than purchasing more data and trying to select potentially relevant data, data work begins and ends there. (Which is really how i like it.)
The fun part for me is writing an improved algorithm for getting the best angle on the data you have. That might be auto feature creation (i dont generally make my own features) or algorithm to tell me weighting or something that actually does slicing and dicing of the data in a new way (some new method or improvement on existing expert systems).  but you are right most alogirthims are 6 of 1 half dozen of the other. And if you have a perfect view of the data there is only so much you can do before you are in the weeds as t were.",118777,2019-12-14 20:43:30,j_scheibel
Leaving the competition,118777,2019-11-28 16:37:53,TripleLift
Good decision Scirpus !,118777,2019-11-25 21:30:30,mezoganet
"without a leaked building ids:
meter 0: 1413 - 296 = 1117
meter 1: 498 - 122 = 376
meter 2: 324 - 0 = 324
meter 3: 145 - 55 = 90",119209,2019-11-28 12:21:48,Aleksandr Kosolapov
I'm afraid that too. It will be the BIG controversial issue after this competition🤒 ,119209,2019-11-27 12:28:54,Isamu Yamashita
Thank you for comment. I respect your success in kernels and public LB😄,119209,2019-11-27 12:39:05,Hiroyuki Namba
"I can't really share any concrete information on this subject right now, but I can say that our intent is to err on the side of caution.",119209,2019-11-27 21:44:03,Sohier Dane
"Thank you. I understand the difficulty. Good to know ""our intent is to err on the side of caution"".",119209,2019-11-28 12:07:11,Hiroyuki Namba
Does anybody have site-15 data for 2016?,119209,2019-12-05 01:28:31,Masoud
how much does lb improve from site 15?,119209,2019-11-27 10:42:43,Oleg Knaub
0.97(without site15) -> 0.95(with site15),119209,2019-11-27 12:18:03,Hiroyuki Namba
"Wow, ok. Thank you :)",119209,2019-11-27 12:22:19,Oleg Knaub
@hiroyukiex  Thank you so much for information    but  i got 0.98(without site 4 and 15) and with site 4 and 15 0.97 # can you plz tell me how much change from site 4 if possible(or you are not using site 4 ?  ) ,119209,2019-11-27 12:53:23,ADITYA KUMAR
"0.95(without site 4) ->0.96(with site 4)
Site 4 is useless, or I need to look into site4 more deeply (but I do not want to spend time reverse engineering).",119209,2019-11-27 13:06:30,Hiroyuki Namba
Thank you :) i Think i need to check my submission then ,119209,2019-11-27 13:12:38,ADITYA KUMAR
That's probably because site 4 has the best score among other sites and site 4 leaks are mess,119209,2019-11-27 13:47:02,Gunes Evitan
"Another team jumped to the 37th place with 4 submissions.  Check this thread and you will understand what is going on right now.
https://www.kaggle.com/c/ieee-fraud-detection/discussion/110778",122203,2019-12-18 17:58:22,Gunes Evitan
"By checking the top 100 team (not single participant) on the LB which have few (less than 10) submits, I think you might be right.",122203,2019-12-18 18:36:33,Changyi
"I see another one in top 100 who used only 4 submissions and that account was created 9 days ago.
Legendary.",122203,2019-12-19 10:33:42,Jie Lu
"We lost close to 150 positions in the last week.   
I'm going out of the medal……",122203,2019-12-18 19:57:56,ysuzuki
"Haha, I feel you…Just went out of the medals the last day…Have dropped more than 100 positions in the last 3 days without any1 posting any better-scoring kernels. C'est la vie! (btw I m not french, I hope I got it right)",122203,2019-12-19 20:20:13,Georgios Sarantitis
"Yes. Faisons de notre mieux!! (btw I m not french too, I hope I got it right)",122203,2019-12-20 03:26:28,ysuzuki
"
Faisons de notre mieux! 

Lol, yes we are actually exactly at the same place! Let's see where we both end up!",122203,2019-12-20 08:48:55,Georgios Sarantitis
"I also notice that someone just join kaggle for few days and their submissions is lower than 10,but their ranks is in 100,this makes me too confuse",122203,2019-12-19 08:20:31,Linyan
We lost close to 70 position in last few days. I am certain that we missed the trick.,122203,2019-12-19 00:45:47,Ashish Gupta
Yesterday night i made improvements by 7 positions and this morning we slipped again by 16 positions. There is something very strange,122203,2019-12-19 13:55:09,Ashish Gupta
"I agree. Thanks to your work I was between 60 and 70 yesterday but now I am out of 100. The only high score published recently is the R code ""ASHRAE 2*lightGBM without Leak Data"" but I got no improvement blending with that…",122203,2019-12-19 14:17:34,Changyi
Slipped by another 20 in 3 hrs. ☹️ ,122203,2019-12-19 17:39:43,Ashish Gupta
ASHRAE 2*lightGBM without Leak Data in R is a last minute submission. I guess people are blending a lot with that model to improve their scores.  ,122203,2019-12-19 20:44:27,Ashish Gupta
"@reneiw : 
beside the fact that you could have dragged your colleague in the money, can you explain more about your overall method. I understand that you build a method using the leak data to calculate the behaviour of individual buildings over 3 years: may I ask how you did that ? did you calculate trends, find particular patterns, set up particular feature engineering ?",122203,2019-12-19 21:55:47,eagle4
"I agree with @lucamassaron , this competition has been a complete disaster. The only positive thing is that it ends in a day.
The leaks destroyed the interest of many to participate.",122203,2019-12-18 17:13:56,python10pm
"Yesterday was 3 days before deadline. It could be a coincedence - if many teams at once started to blend final submissions. But also it could be not.
What I noticed - @reneiw jumped in the green zone with just 11 submissions. Might be that he's very talented and just didn't made any submissions. A lot of scientists work hard for a couple final results and don't use every submission for checking along the way. Just no history at all on the 3-years-old account is a bit suspicious. But I do not call names and give labels.
Anyway, let's not start witch hunting and hope that Kaggle algorithms will do the work.",122203,2019-12-18 13:33:33,Stanislav Blinov
I don't think it is possible to place in such high rank with that many submissions in this competition. My experiments costed me at least 60 submissions. ,122203,2019-12-18 13:41:03,Gunes Evitan
yes … I mean it can happen but in this competition you cannot rely only on local validation you do need to submit to understand better what is the best strategy etc ….,122203,2019-12-18 13:45:39,Pietro Marinelli
"@gunesevitan it's your experience, and mine too. But maybe this guy has a great experience, how could we know?
It's been known Uncle @cpmpml jumped in the gold zone with the first submission, for example.",122203,2019-12-18 13:47:34,Stanislav Blinov
There are several users of the wiener family (reneiw upside down) also 3 years old without any activity. Will they all be a cluster of magic accounts?,122203,2019-12-18 19:43:07,Manuel Campos
Ok now I agree that it's fishy.,122203,2019-12-18 19:47:11,Stanislav Blinov
"interesting,  reneiw's name become evidence of conspiracy,  i started using reneiw ten years ago when starting my study in control engineering, difficult to imagine what Wiener would think. btw, curious about the glorious deeds of the wiener family…
3 years old without any activity is can't be more normal for me, i dislike pure competition at all, even join in this competition is due to highly related to my main work but not for competition itself, evaluation of building energy saving efficiency is a real problem and simultaneously a hard problem, in my experience even the AB test split by time is sometimes difficult to correctly evaluate the energy saving effect due to many non-control non-constant factors, evaluation based on the environment variables is an important direction, the huge amount of data in the competition can be beneficial to my work research.
if just to explain the two questions above, i think I have no reason to make this explanation, the main reason for typing so many words to reply is, the reason your guys to doubt is reach high score with few commits, this can't stand, isn't this really reasonable performance? Do you think we should get high scores through a large number of submissions to be healthy? seems most people think so, that's also the reason I don't like competitions. No one has the energy to try a hundred models or methods, it is nothing more than taking a small number of methods to test online and then constantly using small tricks to correct and then test again, full of guess guess guess collision collision collision ensemble ensemble ensemble routine, take so much time and effort and commits, no real industrial value was created in the end, just to cater to this batch of test sets. Obtaining relatively good and stable results through a concise method originally does not need to submit tests so much times, if get the right method, 20% time can reach the front row, and the remain 80% time is spent from the front row to the top, which industrial added value is relatively small and unstable, i was lucky to find a concise way to complete the first step. With all due respect, except for pure learning purposes, if your 50 submissions are not as good as 20 of others, give up early, if you still want to win, find other competitions to try, there are always competitions where you can come up with better solutions.
Too much pessimistic talk about the phenomenon of competition, turn back to this competition now, actually the current score doesn't explain anything, i used leak data in depth, and the rapid improvement in results is also due to leak data, this very likely overfitting to the leak data or better than other people who have no use for leaking data Is an illusion, i mainly want to share the interesting thing that why suddenly decided to use the leak data in depth(for row level train), because i realized that this leak data is not a true leak, it has practical useable industry value. It is difficult to correctly evaluate the energy impact of a building renovation based on historical data alone, because energy use may change by time even if the building is without any renovation, but if there are buildings that definitely known without any renovation, and their energy data can help to predict other buildings energy use in the same period if they have no renovation also, then we can more accurately to evaluate the energy effect of building renovation, and the leak data of this competition act as the role of buildings data definitely known without renovation for evaluation period.",122203,2019-12-19 17:11:21,reneiw
How do you explain other 100 novices from the same country in top 200 with very few submissions? Are their job also highly related to this competition?,122203,2019-12-19 17:26:54,Gunes Evitan
"Pay attention to your words, i have no responsibility to ""explain"" for 100 Chinese novices, I can directly tell you that there are two accounts associated with me, one of my own, and one of my colleagues, the remaining 98 don't look for me. my colleague started early then me and i actually started modeling this week begin with use some organized data from my colleague about his 10th submission, then we model separately from this point, yes, you read it right, i use colleague‘s initial data but did not merge team with him, if foul delete all my data and grades, i have no interest in studying kaggle data usage rules, hopefully deleting my account will allow you to enter money region.",122203,2019-12-19 18:07:33,reneiw
"Disclaimer: I have not entered this competition, not even looked at data.  I just want to react to what looks like a witch hunt.
@reneiw 

Do you think we should get high scores through a large number of submissions to be healthy?

You're right questioning this.  There are famous examples of people jumping high in LB with 1 submission, including the famous @speculation win with 1 sub only.  Small number of subs plus good LB rank aren't suspicious in principle.

i use colleague‘s initial data but did not merge team with him

Well, that's against competition rules.  But you know it given you write:

i have no interest in studying kaggle data usage rules, hopefully deleting my account will allow you to enter money region.

If you know how it will end then why did you enter?  If it is only to see how your model(s) will fare on private test data, issue is that you won't see your submissions at all if you're removed.  I know because this happened to me after my team was removed because a team member broke some rules and was caught.",122203,2019-12-19 18:17:13,CPMP
"Because I only have time to realy started modeling in this last 3 days, if merged team will take up the submission opportunities of my colleague for the last few days, so I decide to keep my own account 
without in-depth investigation of kaggle rules last week. can't see the private score is really a problem.",122203,2019-12-19 18:30:23,reneiw
,122203,2019-12-19 18:31:46,Timon Gurcke
"I don't care about the money. I wasn't accusing you with anything at all. I just asked about your opinion about this situation, but you misinterpreted my question.  Actually, I wasn't expecting you to do private sharing since you are at the very top, but you just said it yourself.",122203,2019-12-19 18:40:23,Gunes Evitan
"Beside the main topic, I hope you post your solution and how you used leak data for predictions. would be interesting and good to study ;-) Thanks for your long explanation btw.",122203,2019-12-19 18:58:28,Hanjoon Choe
"emmm, looks like there are differences in language habits… It is a pity that I have no opinion on this, i have not followed user details for leaderboard.
I think private sharing is fine since the competition is almost over so does not substantially affect others, hope to see the private list first and then deal with foul, help me to verify whether it is stable to use some buildings data to help predict other buildings. Unfortunately it seems even the highest online score is too big so not applicable at all in real world, in actual projects, even if more precise data is used to make more accurate prediction models, customers still choose AB test",122203,2019-12-19 19:18:38,reneiw
"
Because I only have time to realy started modeling in this last 3 days, if merged team will take up the submission opportunities of my colleague for the last few days, so I decide to keep my own account without in-depth investigation of kaggle rules last week

You realize that doing this will also result in your colleague removal as he shared privately with you?",122203,2019-12-19 19:56:29,CPMP
"
I think private sharing is fine

No, private sharing is never fine,  it is unfair and it is against the rules.",122203,2019-12-19 19:58:27,CPMP
"
I think private sharing is fine
i have no interest in studying kaggle data usage rules

You come to the community @ you follow the rules, mr. Toxic.",122203,2019-12-19 20:32:24,Stanislav Blinov
"you misunderstand what I mean, rules are of course important for platform operations, ""I think private sharing is fine"" means i think this is fine when i write this comment when all our submissions are over, not in the competition progress, i misunderstood the comment above means that in my repost body said the use of leak data to improve performance belongs to private sharing… I did n’t share any method or data with anyone during the competition procedure include my colleague, otherwise he can get good grades too.

You realize that doing this will also result in your colleague removal as he shared privately with you?

this really don't know, and he didn't actually do anything to shair data for me, we are in different city branches and no daily sharing, just when i start modeling i don’t have time to rearrange large amounts of competition data especially leak data so i download colleague's collated data set directly from company server which he use, he didn't get any benefit from my actions or purposely to private help me, it's my fault if it makes him foul too, fortunately his grades are not very good thus don't hurt by me too much.",122203,2019-12-20 02:10:01,reneiw
"Interesting. You can set anywhere as your location in your profile, it doesn't mean anything. Cheaters should be removed, that's definitely true, but others (from the same Country) are innocent. One (or a group of) people cannot represent the whole Country, correct your words please. ",122203,2019-12-20 02:30:17,Venn
@gunesevitan I don't think words that promote outlandish discrimination are welcomed in this community. Certainly made me feel uncomfortable.,122203,2019-12-20 03:07:59,A.L.
"@yiheng @alexanderliao 
I wasn't trying to make any discrimination. Sorry for making you feel uncomfortable. I know there are innocent people among those top teams. I'm not accusing everyone with cheating. 
I thought having too much novices with very few submissions and all of them being from the same country is a little bit weird. I have no intentions of discriminating anyone.",122203,2019-12-20 04:27:36,Gunes Evitan
"some unexpected things happened this week cause some trouble to others including myself, since i am a passer of kaggle and maybe disappear another three years from next week so will not see the discussion later, i think i should make some notes lastly,

I did use data from colleague which can't access by other participate, and it ’s very important for me to get a high score in few days, he spent  lot of effort collecting, cleaning, and calibrating competition-related data on forums, this allows me to skip a lot of preprocessing work and go straight to the modeling process. This violates the rules and i am happy to accept punishment to clear all my results. This is my unilateral mistake, hope this will not affect others.
Although using other's initial data can greatly reduce the time spent, it's different from submission counts, this is the key reason I decided to reply, it ’s healthy to get high scores with a small number of submissions, relying on a large number of submissions to get high scores is usually because did not find a suitable method or did not use the same level data (such as this leak data). 
Back to the technical problem itself, @hanjoonchoe @chabir  , the key issue to using leak data to improve prediction performance is transfer information from some buildings to some others, so learning of leak data cannot rely on building ids in categorical state, i have tried several methods, such as learning training data using neural networks(thus learn the building id embedding simultaneously), and then fine-tune on leak data by freeze the embedding to force learn the new years characteristic by other environmental features; or extract the embedding of building id in nn to use as features substitute categorical buliding id in normal lgbm models; or use lgbm to learning on training data and then ignore the building id feature to learn another model to fit the gap between lgbm prediction value and the leak value. sorry for i can't tell you which one performs better in the end, because for each method i didn't do it fine enough, and I don't think I have solved this technical problem, i don't want to mislead you,  believe other excellent groups will give more interesting methods and conclusions.
",122203,2019-12-20 11:14:46,reneiw
"
i download colleague's collated data set directly from company server

This is private sharing of data.  ",122203,2019-12-20 11:24:54,CPMP
"We lost close to 67 positions in the last 3 days. We were 54 when this week started, currently at 121st..  Not even a single discussion thread on the improvement.. ",122203,2019-12-18 14:54:01,Manoj Prabhakar
"wow, this is quite strange as well … don't you think so?",122203,2019-12-18 14:57:13,Pietro Marinelli
how many position on average did you loose during the competition .. I mean given a position how long would it last without submitting? at least one or two days ?,122203,2019-12-18 14:57:59,Pietro Marinelli
"Hardly one in 2 days even if we don't submit anything. But this time around, everyday on an average 15 to 25 spots down. There are so many particpants with very less submissions in top 100… That seems to be more strange as well.. ",122203,2019-12-19 00:00:05,Manoj Prabhakar
maybe some teams found an additional last minute leak?? not sure…will wait and see what happens when Pvt LB is up. ,122203,2019-12-18 14:19:21,sandy1112
It's either that or private sharing.,122203,2019-12-18 15:07:03,Gunes Evitan
they should share it don't they?,122203,2019-12-18 15:10:00,Pietro Marinelli
"@pietromarinelli I would not bet on everyone sharing a last minute leak. @gunesevitan - agreed it could be private sharing, but seems like a risky move on the last two days of competition.",122203,2019-12-18 15:27:10,sandy1112
Lost close to 50 position while score improved by 0.004 in a week. It's really impressive to see the guys who solve the problem within 20 commits. Tired of overfitting to keep rankings anyway.,122203,2019-12-19 01:06:26,Ado Dream
Please do not submit high scoring predictions :),122203,2019-12-18 14:02:05,Oleg Knaub
"One idea might be that people that were previously not using any leak data at all, are now submitting their models using leak data. Thus jumping a lot of places at one time. However, you are right 9 positions in the top 20 looks a bit suspicious.",122203,2019-12-18 13:29:02,Timon Gurcke
well this makes sense! Still to start using it at the same time sounds strange … ,122203,2019-12-18 13:34:25,Pietro Marinelli
"It might because of the leak data usage, I was also thinking should I use leakage or not in the last 1-2 days.",122203,2019-12-19 09:58:57,Jie Wu
yes this is a good point … will see what happens in few days .. ;),122203,2019-12-19 10:03:33,Pietro Marinelli
"Yesterday, we were 9th with 0.938 score. Are you sure that it happened yesterday or it happened in several hours? If it is within 1-2 days, 8th to 17th is normal considering that it is the time which people start to blend.
On the other hand, there are some suspicious jumps but Kaggle admins usually don't consider private sharing as cheating unfortunately.",122203,2019-12-18 13:51:34,Ahmet Erdem
it's happened if few hours and now it is stable again … this is a 2 submission per day challenge it takes more time to lose positions … normally you keep your position for at least one day  … while yesterday was like two positions per hour … ,122203,2019-12-18 13:54:53,Pietro Marinelli
"
Ahmet Erdem wrote:
  On the other hand, there are some suspicious jumps but Kaggle admins usually don't consider private sharing as cheating unfortunately.

Yeah I saw many topics raising about private sharing solutions during competitions (like for instance this one and of course the famous X5 ) but it seems kaggle do nothing to punish it.
They also need to check solutions for more than only those on prize zone.  ",122203,2019-12-18 14:23:00,Serigne 
"for example today we lost 1 position … for now … but not 5,6 or 9 whatever it was yesterday ….",122203,2019-12-18 14:51:41,Pietro Marinelli
"There is another pattern that I observe on Kaggle: Monday effect. People usually focus on their models during whole weekend and run over night and submit on Monday. But this doesn't apply to this case since the day you talk about was Tuesday.
I guess there will be even more jumps tomorrow. But the question is if our current rankings are even similar to the future private LB rankings.",122203,2019-12-18 14:58:50,Ahmet Erdem
"yes, that makes sense … in general we have to consider that this is a 2 submission per day challenge … if you have 5 submission per day you can simply try stuff and be lucky … but with 2 submission per day is different … anyway will see what happens … ;)",122203,2019-12-18 15:03:16,Pietro Marinelli
"I must have been some change to the public leaderboard.. I observed this around a week ago. I suspect Kaggle moved some of the private LB dataset to the public LB.
It now says:
This leaderboard is calculated with approximately 78% of the test data.
The final results will be based on the other 22%, so the final standings may be different.
Not sure what was before but 22% remaining set seems low.",122203,2019-12-18 15:25:02,Jonathan Mallia
"Interesting, maybe they moved all shared external data (leak) into public test data.  If so then the shakeup can be huge.",122203,2019-12-18 15:57:23,CPMP
@cpmpml - that definitely sounds like a way to tackle partial leaks in the data (like in this competition).  If they did move all the leaked data to Public test data and for the teams that did gain positions because of this - would they be more susceptible to suffer from the shakeup (since their submissions have more overlap with the leaked data)? ,122203,2019-12-18 16:36:10,sandy1112
This is definitely not the case. It was 22% before and no submission is rescored.,122203,2019-12-18 16:55:35,Ahmet Erdem
They did say that they will remove all public leaks from private test data. This should show at some point.,122203,2019-12-18 18:59:24,CPMP
"It shows, with private test data at 11% now instead of 22% initially ;)",122203,2019-12-20 11:23:09,CPMP
"Some teams have achieved a high score with few submissions, in some cases this competitors have new Kaggle accounts.",122203,2019-12-19 16:53:31,Alan Clappis
This Comment was deleted.,122203,2019-12-20 07:22:05,No user
This Comment was deleted.,122203,2019-12-19 08:01:44,No user
"Hi, I found Rules  wrote
You may submit a maximum of 5 entries per day.
But Submission Predictions showed
You have 2 submissions remaining today even I have not submit any files.
Could you confirm it?",112838,2019-10-16 13:17:46,senkin13
Same issue I have.,112838,2019-10-16 14:24:19,Naruhiko Nakanishi
"Submission Limits have been updated: "" You may submit a maximum of 2 entries per day.""
ref: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112993#latest-650748",112838,2019-10-17 11:49:19,Naruhiko Nakanishi
"I have searched a lot of notebooks and threads but still a bit confused.
My question is about the timestamps on all of the datasets(train, test, weathertrain, weathertest)
are they local time or GMT time? or some of them are local time, others GMT time?",112838,2019-12-08 08:46:24,Navid Hakimi
"Hi there, 
Is meter reading per timestamp cumulative? I found that meter reading for a building id with respect to meter no. is not fluctuating per timestamp.",112838,2019-10-27 19:24:12,Dennis Cartin
"Thanks for creating this thread! I have some basic questions that I need to have answered to better understand the data and the goal of this competition.
Please correct me if I am wrong… It seems we are training our model on building and weather data from 2016, and it is our job to predict what the meter will read in 2017 and 2018. 
So.. I  can make predictions for 2017 and 18, and Kaggle and ASHRAE have the true meter readings hidden away? So after I make my predictions and submit them, Kaggle will compare my predictions to the true value and give me an accuracy score?
If that is correct, then how are these numbers to be compared to a building that has been overhauled and energy efficiencies have been made? 
Is the idea that all of these buildings will be overhauled sometime in the future to be made more energy efficient, and we will use the most accurate model to make predictions of what energy use would have been if changes were not made, and then make the comparison to find the amount of money the building saved?
The above questions have to do with the goal of the competition, now I have a question about the data itself…
For the meter types: (0: electricity, 1: chilled water, 2: steam, 3: hot water) are we talking about how much energy each type consumes for a specific building? 
I noticed there are way more entries for electricity than the other types.
electricitycount = 12,060,910
chilledwatercount =4,182,440
steamcount = 2,708,713
hotwatercount = 2,708,713
Is it safe to assume that some buildings have meters for each type of common electricity uses, and for those buildings the energy consumption would be divided between each use… 
And some buildings will have only one meter for electricity, so the meter reading would include all the electricity the building used, including electricity used for chilled water, steam and hot water?
I really appreciate anyone who takes the time to read this post and confirm or correct my assumptions!
THANKS",112842,2019-11-04 17:34:50,Chris Murphy
"Just consider the hot water meter as luxury one. Electricity meter is the basic one available in most home. But hot water meter is a add on. It is available in all home. We should find the meter reading forecast for these meters separately. 
Don't assume that if a building has one meter it does not include all meters. If a building has only electricity meter then no hot water meter is available in the particular building. 
Time series will have trend which will include the over hauled and depreciation of equipment energy consumption over a period of time",112842,2019-11-08 20:37:51,saravanansaminathan
"thanks for sharing, Already subscribed on Rachael's youtube channel. very cool and funny. ",112842,2019-10-15 20:29:01,Adrian Zinovei
"I managed to reduce the memory size and merge the datasets, but when i'm trying to clean them, it keeps giving me memory errors, where do I go from here??",112842,2019-12-12 23:23:13,Juan Vazquez
New to this… where should I start?,112842,2019-12-10 02:50:43,Alexis Ankrah
"Hi,
How would you develop a safety rating system for building based on alarm information, response to alarms, maintenance measures in place, inspections carried out etc.
Thanks,
RP",112842,2019-11-18 10:15:12,Roopa Prabhakar
"Great, Very useful resources.",112842,2019-11-15 17:19:31,Ali Hassan 
"Hi,
Respected Sir Howard,
Thank you very much for starting this thread. 
Indeed this is a great help for new comers like me. 
I am a chemical engineer who is interested in solving this problem. This is because it involves thermodynamics. 
I am presently studying all the comments and comprehend the Python Notebook of Sir Remis Haroon (ID: remisharoon) in which the solution of this problem has been uploaded. 
I will ask REAL questions, as soon as I don't get suitable answer even after searching on Google. 
Cheers!
Waqas. ",112842,2019-11-14 16:48:36,Muhammad Waqas 
"i am wondering how to merge weather table and building table, as there is no primary key or unique key in both tables",112842,2019-11-14 04:18:17,时间之外的往事
"@knightbyj1221 Every building is corresponding to a site_id in building_metadata.csv. Weather is also associated with site_id.
building_id -> site_id -> weahter",112842,2019-11-14 13:24:52,Buffalo Spdwy
This Comment was deleted.,112842,2019-11-15 20:52:39,No user
This Comment was deleted.,112842,2019-11-01 15:47:42,No user
Thanks @arnabbiswas1 ,112855,2019-10-15 23:36:55,Tarek Hamdi
"Thanks for sharing @ryches !
What are your thoughts on using causal time series analysis python package like tigramite?
https://jakobrunge.github.io/tigramite/",112856,2019-10-16 08:17:38,Daniel
"Hi, this is Sean Law (creator of the STUMPY package)! Thanks for the shout out. I can also imagine using the matrix profile and the matrix profile indices (and left and right matrix profile indices for 1D time series) as input features to an ML regression model. Additionally, don’t forget to leverage the time series chains capability to explore forecasting with matrix profiles or consider using semantic segmentation (also included in STUMPY) to inform automatic change points and that can be used in Prophet. We are always interested in seeing new applications so feel free to post an issue on our Github page!",112856,2019-10-19 14:20:11,slaw
"Hi Sean , I love you package and use it very frequently , i wanted to suggest a simple extension of your packages to support spark dataframes ",112856,2019-10-31 11:07:33,AdityaVikramSingh
"Hi, @avikrams! I am not a Spark user myself but feel free to request the feature by submitting an issue to our Github page and provide any specifics that might help. Perhaps, you might consider submitting a PR?",112856,2019-11-24 01:10:29,slaw
I'm looking for a team. I'm EE and data scientist so I think I can have good input for this problem.,112856,2019-10-16 17:02:30,moisessalazar17
"Thanks ! 
@ryches  have you  ever use the1owl ? 
!pip install the1owl",112856,2019-10-30 00:44:04,CaesarLupum
"Yes, prophet is a nice time series package, but as I know,  it can not include other infos like count features or weather infos(unless you treat weather as holidays). ",112856,2019-10-19 11:17:23,LongYin/杰少
"With Prophet, weather can be included here because we know the values for both the training and the testing data. Weather can be considered as an additional regressor as described in the Prophet documentation. Also here is a notebook showing weather being used to assist forecasting bicycle usage in New Zealand.",112856,2019-10-20 04:15:52,ABE_
Thanks! :),112856,2019-10-16 08:28:16,Carlos Sevilla
This Comment was deleted.,112856,2019-10-16 10:59:00,No user
This Comment was deleted.,112856,2019-10-16 06:38:21,No user
Great collection. Thank you for sharing!👌 ,113080,2019-10-30 19:12:12,Adrian Zinovei
you are welcome,113080,2019-11-04 21:55:58,Tarek Hamdi
Great ! thanks for sharing  @hamditarek  ! ,113080,2019-10-17 02:20:13,CaesarLupum
You are welcome @caesarlupum and good luck,113080,2019-10-17 02:27:27,Tarek Hamdi
Thanks @hamditarek ! Good luck ! ,113080,2019-10-30 19:31:54,CaesarLupum
Oh wow – good catch! Very much appreciated! 👍 ,113080,2019-10-18 21:53:31,chmaxx
You are welcome @chmaxx and good luck,113080,2019-10-18 22:08:29,Tarek Hamdi
"Very Informative & Helpful
Thanks @hamditarek ",113080,2019-10-17 05:54:09,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
You are welcome @veeralakrishna ,113080,2019-10-17 09:20:51,Tarek Hamdi
Very informative. Thanks for sharing buddy!,113080,2019-10-17 15:56:21,Manraj Singh
You are welcome @themonologue  and good luck,113080,2019-10-17 16:28:56,Tarek Hamdi
"Split into halves by time
train = train.sort_index(by='timestamp').reset_index(drop=True)
train, holdout = train.iloc[:int(len(train) * 0.5)], train.iloc[int(len(train) * 0.5):]

Stratify by building_id
X_train, X_test, y_train, y_test = train_test_split(train, y, test_size=0.25, random_state=47, stratify=train['building_id'])

Fit with CV control over X_test
Then predict on holdout and get the score.
preds = clf.predict(holdout)
np.sqrt(mean_squared_error(preds, y_holdout))
",113644,2019-10-21 17:26:50,Roman
Thanks for your reply! I will definitely try your method,113644,2019-10-21 18:47:07,Alex
@nroman How consistent are your holdout predictions with the LB score?,113644,2019-10-22 18:50:19,Alexey Pronin
Pretty consistent. Even though there is a difference between LB and CV in around 0.01 but improvement in CV always leads to improvement on LB.,113644,2019-10-23 07:12:45,Roman
@nroman Thanks for your sharing. I have a large gap between the cv rmse loss and my holdout loss after trying your method. I'm wondering whether it's normal or not?,113644,2019-10-26 13:23:17,syoya
"Overfitting is an issue in this competition. Mostly because of the regression type of the task and the way gradient boosting models perform them.
Try lowering number of boosting rounds (iterations) and see how that would affect your holdout set.",113644,2019-10-26 14:39:49,Roman
"""Overfitting is an issue in this competition""…
as allways here, you are very welcome !",113644,2019-10-26 20:13:26,mezoganet
@nroman  I am assuming that you are using this scheme to generate your local CV score and feature engineering. But for submission you must be building a model using entire 2016 data. Are you using any CV at that stage?,113644,2019-10-28 04:39:22,arnab
"@nroman 

improvement in CV always leads to improvement on LB.

what kind of changes do you make to see the corr between cv and lb? parameter tuning, feature engineering, or feature selection? ",113644,2019-10-28 11:52:41,YGSEO
Feature engineering and number of iterations.,113644,2019-10-28 14:15:19,Roman
"Thanks for sharing. When generating the submission file, do you retrain the model using the whole training set or just X_train? If the whole training set, what is the number of boosting rounds?",113644,2019-10-31 17:22:10,lll
Mind explaining what you mean by 'Fit with CV control over X_test'?,113644,2019-11-16 06:04:39,Lawrence Toh
"Thanks for ur sharing!
Are u still using this CV scheme?",113644,2019-11-16 09:53:58,jerrywu
"I'm using a variant of nroman's strategy below where I also validate backwards using the second half (nroman_cv_double). It seems pretty stable locally, but not super well correlated to LB currently. Nothing I have tried is well correlated with LB currently.
from lepmlutils import pdutils as pdu

def nroman_cv_double(train, lgb_params, bad_cols):
    trn_e, trn_l = pdu.split_at_proportion(train, 0.5)
    score1, val1 = nroman_cv(trn_e, trn_l, lgb_params, bad_cols)
    print(f""forwards score: {score1}, val: {val1}"")

    score2, val2 = nroman_cv(trn_l, trn_e, lgb_params, bad_cols)
    final = (score1 + score2) / 2

    print(f""backwards score: {score2}, val: {val2}"")
    print(f""final score: {final}, val: {(val1 + val2) / 2}"")
    return final  

def nroman_cv(train, test, lgb_params, bad_cols):
    Xtrn, Xval, ytrn, yval = train_test_split(train, train[""meter_reading""], test_size=0.25, random_state=47, stratify=train['building_id'])
    preds, val_score = predict(Xtrn, ytrn, Xval, yval, test, lgb_params, bad_cols)
    return mean_squared_error(np.log1p(preds), np.log1p(test[""meter_reading""])), val_score

def predict(Xtrn, ytrn, Xval, yval, test, prms, bad_cols):
    to_drop = bad_cols + [""meter_reading""]
    reg = lgb.LGBMRegressor(**prms)
    reg.fit(
        Xtrn.drop(to_drop, axis=1), 
        np.log1p(ytrn), 
        eval_metric=""rmse"",
        eval_set=(Xval.drop(to_drop, axis=1), np.log1p(yval)),
        verbose = 50,
    )

    val_score = best_scores(reg, ""rmse"")

    preds = np.expm1(reg.predict(test.drop(bad_cols, axis=1)))
    return preds, val_score

I suggest not actually copy-pasting the code, since it relies on lepmlutils a library I wrote, but maybe it will give someone some fun ideas. ",113644,2019-11-04 21:35:24,Louka Ewington-Pitsos
"Until few days back, I used to think there should be only one validation scheme selected for a particular competition/use case and that should simulate the test set (Public/Private LB). But, based on the discussions which happened during IEEE Fraud Detection competition, I understood, that may not strictly be the case.
As suggested by Chris (here), there could be two validation schemes:

First one to understand the effectiveness of the created features and hyperparameter tuning. 
The second one is for prediction on the test data (submission).

Moreover, even for #1, @cdeotte & @kyakovlev used multiple validation schemes (here). Even, I guess @senkin13 has suggested the same here:
Since it's a timeseries problem,so you would be better to split train and validation set by timestamp , sometimes kfold is also can be used for timeseries problem but it's better to check first,you can split train and validation set by timestamp, then train a kfold-model using train set to watch the validation set performance.
Enlightened with that knowledge, here is my thought for this competition:

Here the data is temporal in nature. Hence, for FE and hyperparameter tuning, we should have one or more time based holdout scheme(s). It should noted that the features built should be able to capture very long term trend. Considering, Train data - 2016, Public Test Data - Jan 2017 to June 2018, Private Test Data - July 2018 to Dec 2018, the gap between Train Data and Private LB is almost 1.5 years. So, the feature should be capable of predicting 1.5 years ahead. I don't think it's at all possible to simulate such a validation mechanism (train.csv is just for a year). Also, the models built at this stage will not be trained on the entire train data, hence can't be used for prediction/submission. @kyakovlev has his suggestion here. 
Once the FE and hypeparemeter selection is done, for the prediction, may be Stratified K-Fold based CV mechanism should be used (using the entire training data). As @nroman has suggested that could possibly be StratifiedKFold using building_id.
Should we use feedback from Public LB or not? Since the size of the public LB > Training Data > Private LB (as per the LB probing that has happened till this point of time), I think it should be.

Now, I have never won any medal in any competition. So, take my words with lots of salt. Also, if you think otherwise, please let me know.",113644,2019-10-28 15:51:32,arnab
"I might be missing something obvious here but to me it doesn't make sense to split by time. 
You will be unable to use yearly features in your model which can have a large impact such as:
*seasons
*holidays
*school vacation
*key business dates ( Q1/Q2 etc) , 4/15,
These and other yearly effects are more likely to have a bigger impact (variance) on meter reading in the short term of this dataset ( 3 years) then a small 3 year trend increase in meter reading. For this reason,  I don't think you should be splitting on the basis of time.
think the task here is not forecasting & prediction, but estimation of a hidden state or latent variable. ",113644,2019-10-23 00:11:17,Ptolemy
Pretty sure making time split will ruin your model. But it could be used to ensure your features generalize well.,113644,2019-10-28 10:04:42,WispZero
Why would splitting into folds based on time ruin your model? ,113644,2019-10-29 10:57:48,Louka Ewington-Pitsos
You will lose a lot of seasonal information,113644,2019-11-01 09:23:56,WispZero
"I think sites (0, 8) and (7, 11) are the same",113772,2019-10-22 10:42:48,Sergey Lebedev
"Wow!!  nice intuition. I'm not aware of that.
May be they are very close to each other.",113772,2019-10-22 11:20:13,CC Joshua 
"if you PCA the air temperatures grouped by site and timestamp you get sort of a map:

If you rotate that map, you should get somehow the map of USA? (after some non-linear distortion lol)
Left is cold and right is warm.
Up and down maybe the east and west? not sure…
Sites (0, 8) and (7, 11) are indeed the same (in the plot they're on top of each other) All the values match to the hour, abs diff == 0.0 weather_pca.png (10.74 KB)",113772,2019-10-22 22:54:13,Henrique Mendonça
"Never though of to use PCA to 'visualize' the location ! 👍 👍 
Very interesting approach !!! ",113772,2019-10-23 01:58:13,CC Joshua 
"I think 7 and 11 are in Canada, and 1,5,12 in Europe. Your PCA seems to capture those clusters.
Also I think 2,4,10 are more west than the rest, but your ""map"" doesn't capture that.
source",113772,2019-10-26 18:16:05,S D
@hmendonca Are you a wizard?,113772,2019-10-27 05:23:31,Gunes Evitan
"Sorry, with much delay, I finally posted the code for the PCA in this kernel: https://www.kaggle.com/hmendonca/clean-weather-data-eda
It's good that we now know the location of the buildings and can confirm our theories hehe
cheers",113772,2019-11-06 16:34:09,Henrique Mendonça
"(UPD) Another confirmation, site's (0-8) and (7-11) weather chunks are indeed the same, they have the same missing values in full 2016-2018 weather dataset
Full info you can view in this kernel
(however, PCA approach of @hmendonca is a much more elegant way to spot it 👍 )
",113772,2019-10-24 19:37:41,Fred Navruzov
"I just located most of sites according to external dataset from kaggle.
(Already post in ""External Data Disclosure Thread"")
Here's the notebook:
https://www.kaggle.com/patrick0302/locate-cities-according-weather-temperature
Hope it helps!",113772,2019-10-30 02:42:00,patrick0302
Definitely all places in the northern hemisphere,113772,2019-10-22 21:11:29,Sergey Lebedev
Definitely planet Earth. Only Earth people would use something other than Kelvin.,113772,2019-10-23 02:42:19,SteveKane
Definitely those are Mars temperatures. 👀😄 ,113772,2019-10-23 13:38:01,Ivan de los Santos
Cool idea,113772,2019-10-27 01:38:56,Adelmo Filho
"So what are the cities? or are you trying to crowd source it? ;)   
You should scrape a site with the averages per year, though. Or at least, average all 3 years in this dataset, otherwise you'll be comparing apples to oranges…",113772,2019-10-22 08:03:47,Henrique Mendonça
"Inside my kernel I have a 3 years plot, it repeated pattern is obvious, average 3 years out, I'm expecting similar pattern.
I will presume all station are in United State for now :D . 
Site_0 air temperature look similar to Tampa city
Perhaps can combine with other metrics to further narrow down.",113772,2019-10-22 09:08:40,CC Joshua 
"Cool
You can find the time zone by looking at what time the meter 0/electricity goes up (early morning)
As someone mentioned, it looks like the timestamp is in UTC…
Edit: Sorry, looking at the hours I can't see any difference on starting times, so everything has to be local time.",113772,2019-10-22 21:36:59,Henrique Mendonça
"how did you do the PCA? I kind of don't understand how PCA could be used to show location of sites, thanks^ ^ @hmendonca",113772,2019-10-27 02:22:56,neroli
"In nutshell, PCA will 'group' similar/relevant data closer to each other. 
If sites are close to each other, the temperature reading has be relatively similar. ",113772,2019-10-28 13:13:59,CC Joshua 
"@chenyamei you have to create a matrix of site_id by hourly temperatures, and then apply [sklearn] PCA on it to reduce to the 2 dimensions in the plot above. I'll try to post the code if I find some time (it's only a couple of lines though).",113772,2019-10-28 14:18:56,Henrique Mendonça
"In general everyone should ask himself what are his goals at Kaggle?  For me it's better to spend time to becoming smarter in EDA, ML but looking for the leaks and exploit them. ",116739,2019-11-11 15:12:00,Andrej Vetrov
"In my opinion, the competition data should be limited within Kaggle only with no reference to external dataset so that it is fair for every participant.",116739,2019-11-12 01:47:45,Mr Loke
I agree.The host should have restricted use of external data.This has now become a webscraping competition.✋ ,116739,2019-11-24 14:46:26,Shahules786
How can you detect the person uses it or not? Someone can easily slip leaks to their submissions like a blend and no one will notice.,116739,2019-11-24 14:49:07,Gunes Evitan
"This explains why some people jumped suddenly to the top of LB.
It has started and this is not gonna stop. LB will become useless.",116739,2019-11-11 11:13:29,Jie Lu
Is it not a good sign that using the leaked data improves the LB score - this suggests that at least some or most of the leaked data is not part of the private score.,116739,2019-11-13 17:03:01,PC Jimmmy
"In the Humpback whale competition, the LB was altered to remove a small leak:
https://www.kaggle.com/c/humpback-whale-identification/discussion/81341#latest-475664
Maybe the same can be done here? 
It's also interesting to see how different the first half of the train data is compared to the test data for the UCF site. This has a pretty big impact on what models for this site could look like.",116739,2019-11-12 13:42:32,datasaurus
"x0[,:=(rmx = rollmax(meter_reading, k = 50, partial = TRUE, fill = 0, align = 'right')),
         by= list(building_id, timestamp)]
why aggregate by timestamp? in this way you have as output meter_reading (cause every timestamp is different) and not a rollmax.",116761,2019-11-20 10:54:35,Davide Stenner
"For python,
data cleaning can be done by slightly modifying @ marshi's code.
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113774#latest-655011",116761,2019-11-20 04:44:49,horohoro
"Thank you!
Did you notice improvements on lb score after trimming? ",116761,2019-11-18 03:48:55,Davide
"Just wondering 

Why would trimming zeros after decimal may increase the performance of the model?
",116761,2019-11-13 00:22:58,Vishal 
Because they don't represent the test set meter_reading distribution. They exist only in training set.,116761,2019-11-14 08:15:30,Gunes Evitan
"Leading zeroes refers to the zeroes after the decimal? Shouldn't that be trailing zeroes instead? I don't read the R code, so not too sure what Konrad is doing.
How do you know the zeroes after decimal don't represent the test set meter_reading distribution?",116761,2019-11-14 17:04:01,Lawrence Toh
It was imprecise language on my part: i mean blocks of zero observations at the start of the series.,116761,2019-11-14 18:05:52,Konrad Banachewicz
"Hi Konrad, thanks for explaining. What's the reason that removing zero observations at the start (for siteid=0 presumably) lead to improved model?",116761,2019-11-17 04:15:28,Lawrence Toh
The notebook seems to be private.,116761,2019-11-11 09:39:30,Fredrik Jonsson
"Yeah, I noticed - sorry about that. For some reason the switch to public does not seem to work atm, perhaps because kernel is running? I am trying to fix it.",116761,2019-11-11 09:52:25,Konrad Banachewicz
Set sharing to public while editing kernel,116761,2019-11-11 10:02:20,Gunes Evitan
"Ok, worked when done that way - should be fine now. Thanks.",116761,2019-11-11 10:06:53,Konrad Banachewicz
why did you lose interest? to me it seems like there's still a lot to explore,116878,2019-11-12 03:58:03,Louka Ewington-Pitsos
,116878,2019-11-13 09:03:24,Louka Ewington-Pitsos
"Thanks for sharing, especially the downsampling method, will try to test.
Probably don't need to give up, come back in a few more weeks, see if there's anything new. From what I understand, all the external data people found need to be published here if they want to use them. ",116878,2019-11-12 05:38:04,Ben Fan
"As I used 2 Fold CV, one strange thing I found is that the model fitted in the first fold performs significantly worse than the model fitted in the second fold (first fold 1.12 in LB, second fold 1.24 in LB, total 1.10 in LB)
The second fold is closer to the Public LB Data (w.r.t. time). Could that be the reason?
The downsampling method is slightly modified from sampling without replacement. I upload a file containing the sample_index. If you want to use it, please use the np.load method to load the file as an array.
It would be helpful if you provide some more details about the down sampling methodology (not only for this competition, but similar problem set as well). ",116878,2019-11-12 04:16:22,arnab
"I think he meant the second fold validation score not the LB.  The second fold is trained then used to predict the first fold that is further away from the LB. I also have the same behavior on mine. I agree that it's probably caused by the outliers/missing data in the first 6 months. I only removed the 0s from site 1 from Jan to May 20th.  There's a lot of other buildings that had 0s in the beginning. Also the buildings that were in mid of constructions in 2016, mentioned in this thread. https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113254#latest-667876",116878,2019-11-12 05:33:55,Ben Fan
"Makes sense. Yes, the outliers and missing data would be a problem for sure.
Another thing could be related to features engineered. If the features are more suitable to predict into the future, those will perform poorer while validating the past.",116878,2019-11-12 05:47:50,arnab
"this weekend i will open all leakage data i have.
and there have two news to say.

bad news. maybe one site i have finded, it's in Private LB. it's realy realy is a bad news. so i want to double check this thing.
good news. i think it's a very important thing. must let everybody to know that,let the compation fair.
",118039,2019-11-20 00:05:40,Zhang Yunfei
"Tanks @pp2file, and I look forward to your update.",118039,2019-11-20 00:07:37,YaGana Sheriff-Hussaini
"How do you know it will / won't be scored in private LB?
From what the Kaggle admins responded here, it seems they can decide which rows not to score from test data at a later stage after all public data has been shared / released.
So I would assume any public data that is shared on the forums would eventually be removed from private LB scoring. Or at least it should, to be fair, clean and actually useful as a competition.",118039,2019-11-20 07:07:55,Vopani
"
I used a site data(this weekend I will be open kernel) to submit, but the public LB did not improve.
because, I have find some site data ,not yet be shared.
",118039,2019-11-20 08:58:21,Zhang Yunfei
"https://www.kaggle.com/pp2file/ashrae-site15-cornell
all sixes leakage has describe in there.",118039,2019-11-24 10:32:20,Zhang Yunfei
I retrieved the full data of UC Berkeley from its Engagement Dashboard. Anyone can fork the following kernel: https://www.kaggle.com/serengil/ucb-data-leakage-site-4,118039,2019-11-20 19:39:40,Sefik
I finded a  lot real data in BuildingOS . But I can't get the data.,118039,2019-11-19 10:10:12,Zhang Yunfei
"Hi @pp2file  Congratulations  , do you use these data to replace the values in the submissions directly or you are using them to fine tune the model ? , If you don't mind can i ask you about your CV strategy  ?",118039,2019-11-19 10:22:59,AdityaVikramSingh
"in the public score , i just used these data to replace. but i think the real useful skill is to finding dirty points in training data. I found this data is very very easily to overfitting. so i don't used shuffle in Kfold. don't used to many splits(just use 2 folds).",118039,2019-11-19 10:36:52,Zhang Yunfei
"Thanks for the response , I agree with the face that the data is too prone to overfitting ,  what do you mean by dirty points in the training data?",118039,2019-11-19 10:42:59,AdityaVikramSingh
"
just like this data.",118039,2019-11-19 10:46:16,Zhang Yunfei
"Thank you for sharing.
I’m afraid that all the test labels will be leaked before the deadline and this really interesting competition turns into completely meaningless.",118039,2019-11-19 10:30:01,Hiroyuki Namba
I don't think you have to worry about this problem. because it can not be happen. i have try to find a lot of data. but some site can not be finded. ,118039,2019-11-19 10:32:35,Zhang Yunfei
Thank you! I’m glad to hear that.,118039,2019-11-19 10:37:05,Hiroyuki Namba
We have reason to believe that enough of the sites are unavailable online that there will be a meaningful final leaderboard.,118039,2019-11-19 16:57:47,Sohier Dane
Yeah but we can't get any meaningful feedback no more from public LB and that's painful 😨 0.98 on the public LB now can finally get about the same score of 1.06 on the public LB (who doesn't use leaked labels) when switching to private.,118039,2019-11-19 17:18:32,Jie Lu
"@sohier why not to say openly which sites will not be used in the final score estimations? if I do not read the forum carefully or someone does not share a link with another site from the internet, you insist on people investing their time into wasteful effort?",118039,2019-11-19 18:41:04,snovik
I would even go one step further: completely remove the leaked data from both the public and private test sets and get a clean LB.  ,118039,2019-11-19 21:29:14,Panos
but now i think some things happened.,118039,2019-11-20 00:08:05,Zhang Yunfei
Another inconsistency is site 1 readings are aggregated on hourly sum and site 15 readings are aggregated on hourly mean,118753,2019-11-27 20:09:56,Gunes Evitan
The more you know.🤔 ,118753,2019-11-27 20:28:52,Tim Yee
"Unless I'm mistaken those are just two different ways of describing the same concept of kWh. If you run a 2 kW device for 0.5 hours, you will get 1 kWh as the result of both calculations.",118753,2019-12-02 17:43:22,Sohier Dane
That would be correct if only we had 1 hour of meter readings,118753,2019-12-02 18:48:46,Gunes Evitan
"Depends on the frequency of the meter measurement.  For example, at my institution, we push meter readings every 5 minutes.  To get the energy consumption, the calculation would be (KW(t)-KW(t-1)) x interval(minutes)/60 (minutes/hour).  The KWh would be the sum between any two intervals.  If the frequency is once per hour (which the competition standardized on), then KWh = (KW(t)-KW(t-1)) x 1 hour.  If, however, you're looking for demand (KW), then for a measurement frequency of one hour, it would be the mean of readings between t and t-1.  The measurements aren't actually in KWh, they're in KW.",118753,2019-12-02 20:24:12,Larry Schuster
"More reasons why this entire competition is a joke.  As an ASHRAE Life Member, I'm embarrassed.",118753,2019-11-24 13:37:43,Larry Schuster
"The data is highly valuable, but the inconsistency is literally frustrating. Now, most of the time wasted here is just about web scraping instead of working out a useful model. ",118753,2019-12-09 00:29:19,smerllo
"Per the data description: meter_reading - The target variable. Energy consumption in kWh (or equivalent).
We standardized the units before posting the data:

Asking 2,000 people to repeat the same preprocessing steps did not seem like it would be a fun addition to the competition
Not everyone reads the data description
",118753,2019-11-25 18:30:08,Sohier Dane
"Understood, however UCF's website is reporting in kBTU for electricity. Can't really argue with that since the values in 2016 from UCF's website and the training dataset match up. Unless of course the website is wrong. I'm just going to take the L here and accept that the website is wrong.",118753,2019-11-25 20:25:18,Tim Yee
"Where is the data description?  I looked in the data section and column information, and it doesn't indicate units.
EDIT:  Ah, drop down arrow.  Didn't notice it before.  I retract my comment.  Thanks for the response.",118753,2019-11-25 23:07:58,Larry Schuster
@teeyee314 I looked into this further and confirmed that the electric meter readings for site 0 were not properly converted from kBTU to kWh. I've made a general post here discussing the issue. Thank you for the catch! ,118753,2019-11-27 17:48:39,Sohier Dane
"Is it just the electricity meter readings that need to be converted or do the chilled ones also need to be done? In the table above, site 0 also has different units for these.",118753,2019-11-28 09:04:53,Steve Roberts
"Hey Yunfei, Are you going to make site15 raw data available? The Cornel website removed the link of their portal. Thanks!",118766,2019-12-05 01:24:39,Masoud
"wow, maybe just the website has some problem, or the website will be close forever.
I have post the raw data in the Kaggle.",118766,2019-12-05 03:16:51,Zhang Yunfei
"this is raw data for cornell .
https://www.kaggle.com/pp2file/cornell",118766,2019-12-05 03:18:21,Zhang Yunfei
"Thanks Yunfei! but I get 404 error. 
I would be thankful if you please make it available again and I will let you know as soon as I download it.",118766,2019-12-06 00:05:23,Masoud
"https://www.kaggle.com/pp2file/cornell
this is good. before i don't set the data public.",118766,2019-12-06 03:10:04,Zhang Yunfei
Thanks a lot Yunfei! Just downloaded.,118766,2019-12-06 13:22:19,Masoud
👍 ,118766,2019-12-06 16:11:35,Zhang Yunfei
"I am here to learn,so I just keep away from leaked data.😊 @rohanrao ",119015,2019-11-26 17:26:54,Shahules786
Thanks for sharing.,119015,2019-11-28 08:44:53,Prachi
"I appreciated your last words ""There's more to Kaggle than just leaks and leaderboard.""",119015,2019-11-26 14:30:16,Marília Prata
I tried using Prophet to do building-wise regression just like you but just looking at the graph of Prophet's prediction it was completely useless so I didn't even bother submitting. I am a complete newbie so this may be due to my poor abilities. Do you find the predictions you get are somewhat plausible? Are you planning to submit them?,119015,2019-11-26 09:08:07,Larry D.
"I am not going to give up. My only purpose is to get better understanding of time series modelling. Therefore i am still working on my model without glancing for external data.
Thanks for sharing your approach which is a completely new idea for me, and best regards",119015,2019-11-26 07:42:36,Marek Swieton
"I am trying to use (GP)genetic programming, and it is working but not improved generation by generation Since I am not sufficiently understanding this yet. still have to learn what's wrong in the code ;)",119015,2019-11-26 05:55:51,Hanjoon Choe
This Comment was deleted.,119015,2019-11-26 05:28:34,No user
,119015,2019-11-26 05:36:15,Vopani
Thanks for sharing !,120694,2019-12-16 00:22:22,CaesarLupum
Thank you,120694,2019-12-14 16:19:20,NWUT
Excellent! Thank you for sharing:D!,120694,2019-12-14 13:02:02,Beans
Good source to study. Thank you. ;-),120694,2019-12-12 19:39:34,Hanjoon Choe
Thanks for sharing,120694,2019-12-11 07:42:35,sjsjs
Thanks for sharing this information!,120694,2019-12-10 19:30:31,Rohit Singh
Thanks but common outlier detection/removal practices won't work in this case. You need some domain knowledge.,120694,2019-12-09 08:52:27,Gunes Evitan
Thank you for this information! ,120694,2019-12-10 06:22:04,Stanislav Lykov
Thanks for sharing,120694,2019-12-10 03:47:58,Salma Elshahawy
You're welcome,120694,2019-12-10 04:06:56,Naruhiko Nakanishi
"Exactly what I would like to know, thank you1",120694,2019-12-08 22:20:48,lonelycat
It's my pleasure,120694,2019-12-09 01:06:58,Naruhiko Nakanishi
thanks @grapestone5321 for sharing this ….. ,120694,2019-12-08 04:38:37,Saurav Anand
It's my pleasure,120694,2019-12-08 04:44:56,Naruhiko Nakanishi
Thanks for sharing❗️,120694,2019-12-16 04:27:46,Mashlyn
Half of the teams just vanished!,122347,2019-12-20 06:45:49,Firat Gonen
If there are massive private sharing existing… I don't understand if they are really satisfied with being medalist without their own solid methodology. It should be such a shame if I were. This does not help to them.,122347,2019-12-19 20:59:18,Hanjoon Choe
@hanjoonchoe I even don't know why they need that? empty accounts with medals for what?,122347,2019-12-19 22:07:58,Mukharbek Organokov
"For me, 2 possibilities : reset username and sell the account to someone who need. Use novice account to have more submissions.",122347,2019-12-20 08:53:48,Changyi
how many accounts are suspicious..according to you ballpark ?,122347,2019-12-19 19:01:23,Firat Gonen
"70-80 in 50 teams minimum (within TOP10%)
some teams have 3 guys with activity 1 month ago ))) and total <10 submissions for the entire team.",122347,2019-12-19 19:10:14,Mukharbek Organokov
thanks @muhakabartay ,122347,2019-12-19 19:15:46,Firat Gonen
"its too  hard
🤕 ",122347,2019-12-19 23:23:51,Zhang Yunfei
Almost 4000 teams decrease to 1800 and the number is still getting smaller.,122410,2019-12-20 01:49:40,fwj7
"Yeah, that is kinda strange, from around 3700 to 1877 teams, but it is still showing all the teams on leaderboard.",122410,2019-12-20 03:00:58,HarshitMehta
Are they removed? If so this is insane. I wonder what is special with this comp that attracts 1000s of fake accounts.,122410,2019-12-20 07:03:09,LaoLiu
Max 2 subs per day. People may create fake accounts to get more submissions. ,122410,2019-12-20 08:30:28,Xuan Cao
"I think it's just an artefact of rerunning all the submissions on cleared data, I hope it returns back to normal in a few hours/days",122410,2019-12-20 08:32:06,Fred Navruzov
"Yes only 2 subs a day and 2 decimal digits for precision.
It was a very strange competition, but 4000 teams and now 1800, it’s incredible ! 
So many cheaters ? Hum… If yes, there is something to do tracking them from the beginning of competition.
I know this is not very easy…",122410,2019-12-20 16:26:23,mezoganet
"It was my first competition so honestly I did not know what to expect. Seeing the number of teams almost cut by half is sad. I learned many things from the amazing kernels that were shared in this competiton. 
To all amazing kagglers, please keep sharing kernels and bring interesting insights after the end of this competiton. Thank you all !",122410,2019-12-20 07:51:05,Nono
Same here!! Congrats on your learning progress! Thanks everyone for the open notebooks have taught me so much. ,122410,2019-12-20 18:19:38,DavidHam
"Congrats, you got the most valuable aspect of Kaggle IMHO: learning something new! ",122410,2019-12-20 18:48:11,Xuan Cao
"Honestly it's sad. It had so much potential. The data was really cool, especially because one had to learn a lot about Outlier analysis, time series and how to deal with a lot of data. It was my first competition so I learned a lot nonetheless, but I see how it could've gone so much better…",122410,2019-12-20 02:16:10,Thiago Preischadt
It seems you've learn a lot from this competition. That's great. I didn't even follow what you have done. You have many potentials. keep going! ;-),122410,2019-12-20 02:21:46,Hanjoon Choe
"Thanks for the nice words dude :`), that's really nice to hear. I did learn tons! Mb we can team up in future competitions!",122410,2019-12-20 02:32:51,Thiago Preischadt
"No problem, Someday both of us could get strong backgrounds enough to get a synergy ;-)",122410,2019-12-20 02:41:49,Hanjoon Choe
"These issues were predictable, see for instance: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/116840",122410,2019-12-20 20:35:22,CPMP
"Yeah, I saw your post and decided not to join this competition. ",122410,2019-12-20 21:22:28,Xuan Cao
"I respectfully disagree. Here's a thread that you probably missed: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/119460
Despite the problems, there will be no damage in the private leaderboard and measures are being taken regarding suspicious or forbidden activities. ",122410,2019-12-20 03:45:18,Fernando Wittmann
I disagree as well. I actually had fun and learned a lot.,122410,2019-12-20 05:18:33,Gunes Evitan
It was the same for me 👍 ,122410,2019-12-20 08:32:16,Oussa
"Yes, removing leakage from the private leaderboard minimizes the damage, but it can not removing the impact of leakage. For example, people need to spend extra effort to collect the leakage. If you do not hold all the leakages, you may be at a bad position. You may argue that collecting data is one part of data science, which I do agree, but I don't think it should belong to a data science competition. 
As for removing suspicious of forbidden activities, you could expect some will be taken care of, but do not expect all of them. ",122410,2019-12-20 08:50:33,Xuan Cao
+1. People like to complain and probably there is a reason to complain in this comp but overall it was quite interesting.,122410,2019-12-20 08:56:43,Artem Dubinich
"I disagree as well.
Although a lot of leakage data in this game, there are still more than three thousand people participate in this competition.",122410,2019-12-20 12:43:37,Dean
"
For example, people need to spend extra effort to collect the leakage.

@naivelamb since you were not participating, probably you are not aware but some great Kagglers (@gunesevitan, @mpware, @poedator, @pdnartreb, @serengil and @pp2file) thankfully did the hard work of collecting the leakage. Also, the Kaggler @yamsam created a great kernel joining all the leakages. Thanks to them, the impact was minimal. ",122410,2019-12-20 15:37:16,Fernando Wittmann
"I agree for all the competitions with leakages, there are great kagglers helping to exploit the leakages and publish it. Shout out to them. However, the question still hangs there: are all the leakages floating on the Internet being published? 
I remembered that in the Google Analytics competition, when the Google Demo Account can be used to mining the target variables, lots of people left the competition and Kaggle had to reformulate it. In this competition, lots of people also left after releasing the target variables can be mined. 
I think the impact of leakage can be estimate by asking one question:

Will you join a competition knowing there are leakages?

If majority people answer 'No', its impact cannot be ignored. 
BTW, congrats on you 18th position on public leaderboard, wish you get a good one on the private leaderboard as well. ",122410,2019-12-20 19:10:36,Xuan Cao
"The worst competition ever may be this one : https://www.kaggle.com/c/draper-satellite-image-chronology/overview.
All the gold medals, a priori, got it by manually sorting the pictures without any ML or any calculation at all.",122410,2019-12-20 15:38:43,eagle4
Luckily I wasn't playing kaggle at that time. 😂 ,122410,2019-12-20 21:23:13,Xuan Cao
"I believe we have seen competitions where a high scoring kernel was posted at the end or people were accused of private sharing or fake accounts (which can be true too). People will do what they wanna do, kaggle can only put regulations and do compliance to get rid of above. 
This competition differs because of leak data, but kaggle is trying to get rid of that so that it won’t affect the final standings.
On lighter note, I still believe that Jigsaw Toxic classification competition holds the first spot 😛",122410,2019-12-20 05:41:50,HarshitMehta
"'Last minute high scoring kernel.'
Can I ask where is it?",122410,2019-12-20 08:15:15,Manraj Singh
"It was mentioned in some of the threads I went through. Unfortunately, it seems to be deleted. ",122410,2019-12-20 08:42:37,Xuan Cao
"@naivelamb I would request you to go through this discussion and kindly edit yours. :)
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122446
Thank you. ",122410,2019-12-20 09:32:03,Manraj Singh
"OK, edited. ",122410,2019-12-20 16:18:06,Xuan Cao
"I was participating in NFL and learning a lot. Then the ASHRAE started and it seemed very promising and interesting.
Unfortunately what you say is true, the massive leakes discouraged many to participate.
On this thread we have @scirpus leaving the  competition because of the web scrapping. 
I think it was a general feeling. ",122410,2019-12-20 08:44:04,python10pm
This Comment was deleted.,122410,2019-12-20 02:16:51,No user
"Quick update - we have a working draft containing ~40,000 rescores but need to check some edge cases to make sure we didn't miss valid submissions and to run additional tests to ensure that updating the scores kicks off all of the correct jobs downstream, like reinstating the ~1000+ teams that vanished from the LB. This will take additional time, but will not be done until after Christmas. We will post another update or the scores on the PST afternoon of the 26th. Thank you all for your patience!",122554,2019-12-24 00:22:26,Sohier Dane
@addisonhoward can we please get an update 3?:),122554,2019-12-23 18:53:16,Ahmet Erdem
"Ahmet: they said 48h business hours… => ~6 business days => Given Xmas + week-end, maybe New Year Eve or next year…",122554,2019-12-23 19:11:17,eagle4
I think they meant 2 business days by 48 hours. But I believe we have to wait until 2020 too.,122554,2019-12-23 19:14:34,Ahmet Erdem
"Plus the eve´s and time to be clear and forget the last  drunking night, this has to be taken in account !",122554,2019-12-23 22:58:14,mezoganet
"Gone like it is, it seems that I will know my final position for my birthday : January13Th next year…
Hope it will be kind of a gift ;-) ",122554,2019-12-23 19:56:09,mezoganet
This Comment was deleted.,122554,2019-12-22 00:10:14,No user
"Our team is looking for teammates. Some requirements as follows:
You must work hard on this competition, have new ideas, no multi kaggle accounts.
Currently, we are using LGBM and Kfold to get 0.96 with leak data. Our best single model without leak is 1.06",112843,2019-12-07 04:23:34,small tiger
"So I'am looking for some teammates to take on the remainder of this competition. Anyone in say the top half of the leaderboard and already a couple of competitions finished would be ok. 
I'am fine with either joining a team or starting one with other Kagglers. I have extensive experience already with Data Science and Machine Learning.
Anyone interrested post either a reply here or sent me a private message through 'contact user' on my profile.",112843,2019-11-27 19:43:51,Robin Smits
It is the first time to join competition with team. So I want to join your team!,112843,2019-11-28 12:31:50,makogarei
"Hi @makogarei. For me also the first time to team up. So lets team up.
I'll send you a private message through 'Contact User'. That way we don't have to put email adresses in the public discussion.",112843,2019-11-28 15:08:23,Robin Smits
Interested,112843,2019-11-07 17:13:14,Mehdi Janfaraz
Hello! Would you like to team up?,112843,2019-11-11 06:39:05,Francisco Escobar
"Hey @franciscoescobar would you like to team up 
I want to participate in this competition
but this will be my first team competition",112843,2019-11-11 07:19:20,sahib singh
Sure thing! What's the best way to contact you?,112843,2019-11-11 07:22:26,Francisco Escobar
email at ss9334931@gmail.com but do you know how to form a team? on kaggle,112843,2019-11-11 07:29:40,sahib singh
also interested: lorenzo.ampil@gmail.com,112843,2019-11-17 12:58:49,Lorenzo Ampil
"Hey Lorenzo, if you are interested, we can team up.",112843,2019-11-20 06:59:06,NitinKshatriya
"Hello Kagglers, I am Mukul sharma and I have started kaggling from last 3 months (although signed up a year before😅 ). Before this I was not into competitive data science or data science itself. I am currently at 7% of this competition and looking for partner. If you are interested you can drop me an email at mukulsharma.0462@gmail.com. 
Happy Kaggling. ",112843,2019-11-07 04:20:47,Mukul Sharma
@yourssharmaji Hello i am interested in teaming just  send me a team merge request. I am in a team currently but no one is active here ,112843,2019-11-07 04:34:51,Raj Shinde
@yourssharmaji we are looking for a team merger ,112843,2019-11-07 17:09:49,AdityaVikramSingh
@rajwardhanshinde  if you want to do a merger we can open to it ,112843,2019-11-07 17:10:15,AdityaVikramSingh
i am interested in… i also make it at 1.1 but i have no idea to make it better. maybe we can exchange idea to get better.,112843,2019-11-09 10:27:32,Yixinchen
"@yourssharmaji brother please can you help me knowing what is the purpose of the competition
I am new to this platform
I even asked the same question in forum but no one replied.",112843,2019-11-10 07:53:00,sahib singh
"Yes @sahib12 sure buddy. So here we are given the data (metere reading) of one year of different meters located at different buildings. Now we need to predict the meter readings of next two years. The purpose? Think of a scenerio where you installed some energy efficiency measures(devices or anything), and now you want to compare the resulting electricity v/s what would have been without any measures. This problem is to make a model which can predict the meter readings. I hope now you get the problem. If any further question please do let me know buddy✌",112843,2019-11-10 08:30:46,Mukul Sharma
"@yourssharmaji brother I don't understand what is given in sample submission file .
Because that is what we have to predict at the end .
And can you tell me (Hint)from here should I start.",112843,2019-11-10 08:36:57,sahib singh
Should I land to you some of public kernels? I mean after various trails we found the optimal CV method and that beats most of the other kernels with various hyper parameters and different sets of features. ,112843,2019-11-10 08:52:17,Mukul Sharma
"no brother just tell what is written in sample submission file.
Thank you brother for all your help",112843,2019-11-10 09:01:35,sahib singh
"Hi,
I have worked mostly on Image data so far. Have 3yrs experience in Computer vision. Have done most of my work in DL. This is my second competition on Tabular data. First serious competition.
I have submitted a baseline and yet to carry out feature engineering.
I did a complete course on ML
I am looking forward for team-mates.",112843,2019-11-06 14:05:04,Balaji Selvaraj 
"Hello Balaji. 
I have a similar profile as yours. 
I have individually finished many knowledge competitions on Kaggle.
Have back ground in Geography, Oil and Gas, Machine Learning and Computer Vision.
I have proficiency in Python, pandas, numpy, Matlab and Linear Algebra. 
I did a preliminary study/visualization of the data, would like to work on this project as a team.
Email Id - mayur.nawal@gmail.com
Happy Learning.
Preliminary work: 

Weather Variable - based on the following paper: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3478798/ 
Aggregating features: Feature-tools library for aggregating data from different data sets. 
Feature transformation - Tranforming timestamp into multiple feature like season, categorizing them into dusk to dawn, normalization of other features. 

**Would like to join the competition with your team **",112843,2019-11-08 02:44:53,mayur nawal
"Please accept the request.
Looking forward for working with you mayur.
Lets hope to add few more ",112843,2019-11-08 12:55:17,Balaji Selvaraj 
Anyone in Japan?,112843,2019-10-28 09:09:04,TN-Kaggle
"I'm in Japan, attending technical college.
I have experienced a few competitions.
I'm interested in working with you if you don't mind.",112843,2019-10-31 03:37:33,Hideo Kodama
"Hi @lunarmoon Interested! Please let us know.
@resistance0108 ",112843,2019-11-01 22:57:41,Pratik Sharma
"Hi @lunarmoon , I'm in Japan too.
Let's make Japanese team!",112843,2019-11-02 14:07:57,goreinu
"Hello, physically I'm not in Japan now, but time difference is just one hour. 
If you don't mind I'd like to join.",112843,2019-11-26 13:43:09,Kohei
"Sorry, recruitment is over. This competition has a lot of leaks and I look forward to seeing it in another competition.",112843,2019-11-27 09:36:57,TN-Kaggle
Anyone Want To Team Up ?,112843,2019-11-01 06:08:34,Raj Shinde
@rajwardhanshinde Interested! let me know.,112843,2019-11-01 06:19:30,Pratik Sharma
I am interested !,112843,2019-11-01 08:11:15,Naren
This Comment was deleted.,112843,2019-11-01 14:17:40,No user
"Hello, I am also interested!",112843,2019-11-02 06:02:09,Siyuan H
Interested,112843,2019-11-06 14:07:55,Balaji Selvaraj 
@rajwardhanshinde if interested we can do a team merger,112843,2019-11-07 17:07:51,AdityaVikramSingh
i really interested in… i am currently at 13%. i have no idea how to do better after trying many times. thanks!,112843,2019-11-09 10:30:44,Yixinchen
"@addisonhoward please can you help me knowing what is the purpose of the competition
I am new to this platform
I even asked the same question in forum but no one replied.",112843,2019-11-10 07:54:09,sahib singh
Interested If you are available.,112843,2019-11-18 07:44:51,NitinKshatriya
anyone who is interested to team up please contact me at anuragnegi008@gmail.com,112843,2019-10-29 05:41:33,AnuragNegi
I am interetsed,112843,2019-10-29 06:35:08,Data Driven
i have you team merge request,112843,2019-10-29 07:03:26,AnuragNegi
@anuragnegi Hey Can I Join your team ?,112843,2019-11-02 07:24:58,Raj Shinde
sure @rajwardhanshinde  i'll send you merge request now,112843,2019-11-02 07:31:55,AnuragNegi
@AnuragNegi Is Everyone working individually Or Is There any group or something?,112843,2019-11-02 07:39:30,Raj Shinde
@rajwardhanshinde we are in group i need you email to add you to our slack channel,112843,2019-11-02 07:52:02,AnuragNegi
rajshinde55553@gmail.com Heres my email @anuragnegi ,112843,2019-11-02 08:01:24,Raj Shinde
@anuragnegi Can I join your team as well? shirleysiyuanh@gmail.com is my email.,112843,2019-11-03 01:09:28,Siyuan H
Are you still looking for teammates? I am in the west coast of US. ,112843,2019-11-03 05:19:12,Jinpeng Gao
@anuragnegi I am interested too…,112843,2019-11-07 09:21:08,Barun Kumar
i am interested too.,112843,2019-11-09 10:32:45,Yixinchen
"Hi, I have just completed my Data science internship where I worked on time series forecasting model and different nonlinear optimization techniques, besides that currently, I am learning Computer Vision, I am very much familiar with Python, Ml algorithms and python libraries (Numpy, Pandas, Sckitlearn etc). Currently I am looking for a team to join or serious competitors to form a new team with me. I would like to start my Kaggle journey by participating in an ""Ashrae energy prediction""competition and I can assure you we will grow through sharing our ideas.
If you want to join/form a team please let me know or email me at subhajitf19@gmail.com, Thanks.",112843,2019-10-25 23:11:14,Subhajit Mondal
I'm up. I have sent you the merger request.,112843,2019-10-27 09:50:26,Akash Sharma
Fine.just shoot me a mail at subhajitf19@gmail.com for further conversation.,112843,2019-10-28 15:18:34,Subhajit Mondal
I am interested !! ,112843,2019-10-29 04:04:23,Data Driven
Hey can I join the team? I have 3 years experience in data science. I also work as a data scientist and have resources.,112843,2019-10-29 10:04:57,Kristóf Horváth
Yeah..just send me email on subhajitf19@gmail.com for further discussion,112843,2019-10-29 13:46:33,Subhajit Mondal
"Hi,
I have some background in math and statistics. I'm learning ML. I have been building and testing models in Python from the Scikit-learn module to make predictions on categorical and numerical targets. I have some experience working with large data sets such as cleaning data, EDA, and feature engineering with PCA to find best dependent variables.   New to kaggle competitions so I would like to work together with a team. I'm from Massachusetts. If anyone is interested in collaborating, my email is jie.dong.00@gmail.com
Thanks!",112843,2019-10-21 02:17:49,OG23
This Comment was deleted.,112843,2019-10-22 02:33:37,No user
"Hi, I have a Ph.D. in Industrial Engineering and I did my thesis about Life Cycle Assessment of buildings. In which the energy consumption has a great importance. I program in R and Python and I'm a beginner in machine learning.
I'm from Barcelona and my email is juanmah@gmail.com",112843,2019-10-17 20:40:01,Juanma Hernández
"Hola Juan Mah
Tengo 20 años desarrollando en diferentes lenguajes.
Para inteligencia artificial utilizo FastAi y PyTorch (Python) he logrado con esto realizar varias ejercicios de Kaggle e igualar los errores de los primeros lugares. 
Nunca he competido en Kaggle en una competencia abierta
Cuento con los equipos para realizar la computación en GPU
Me gustaría formar un equipo.",112843,2019-10-20 21:48:36,Pop
,112843,2019-11-07 08:16:07,Fred Navruzov
"Hi guys,
I'm belgian. French, Dutch speaking.
I'm currently working for a DSO. I have got some experiences in Python/Pandas/xgboost/neural networks, moreover in time-series predictions. ",112843,2019-10-16 07:33:07,Quentin Garnier
"Hi Quentin,
I'm ludovic, french speaker :)  and interested in working with you. let me know ;)",112843,2019-10-16 09:54:05,ludfanon
email me @ ludofanon@gmail.com,112843,2019-10-16 09:56:49,ludfanon
"Hi guys, another french speaker here. I want to join a team. arm852000@yahoo.fr",112843,2019-10-16 10:36:18,armbodj
"Hi, I'm also French, I've sent you a message :) Let me know 👌 ",112843,2019-10-16 10:48:35,Grégoire
All right ! I'd like to join the French speaking team as well ! Let me know =),112843,2019-10-18 19:13:23,Gijs Barmentlo
"Hi, I have a Ph.D. in theoretical physics and I am looking for someone to team up for this competition. I have experience with machine learning techniques with python and am eager to work on this project. ",112843,2019-10-16 03:20:24,jabt
"Hey @jabt1986 ,  I am computer science undergrad student,  would love to collaborate on this. ",112843,2019-10-16 10:30:56,Addy
"Hi @jabt1986, I have a PhD in theoretical physics too, currently a post-doc in London. I also have some experience with Python and machine learning. Will be happy to team up with you. Let me know if you are interested. ",112843,2019-10-18 06:02:33,Prarit
I would love to team up with you. I have a master in mathematical finance,112843,2019-10-21 06:26:56,lionel melhy
"Hi, M.Sc. theoretical physics and M.Sc. in data science - please let me know if there is still room for one more - abouhassan.md@gmail.com",112843,2019-11-06 17:23:07,Mohammed A. Hassan
"Hello Jabt and other members in the group.  
I have individually finished many knowledge competitions on Kaggle.
Have back ground in Geography, Oil and Gas, Machine Learning and Computer Vision.
I have proficiency in Python, pandas, numpy, Matlab and Linear Algebra. 
I did a preliminary study/visualization of the data, would like to work on this project as a team.
Email Id - mayur.nawal@gmail.com
Happy Learning.
Preliminary work: 

Weather Variable - based on the following paper: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3478798/ 
Aggregating features: Feature-tools library for aggregating data from different data sets. 
Feature transformation - Tranforming timestamp into multiple feature like season, categorizing them into dusk to dawn, normalization of other features. 

**Would like to join the competition with your team **",112843,2019-11-08 03:24:48,mayur nawal
Hi. I'm in GMT/UTC timezone. French speaking people welcome to set up a team. I know a bit of Python/Pandas/Matplotlib and am a DS learner.,112843,2019-10-16 00:15:40,Mehdi Sidi Boumedine
"Hey Mehdi, I'm also interested ;) arm852000@yahoo.fr",112843,2019-10-16 10:39:27,armbodj
Hi OK. Do you want we join Quentin's team or do you want a try a new one?,112843,2019-10-17 22:28:15,Mehdi Sidi Boumedine
Hello everyone! Anyone from Dallas/ Fort worth area wants to work on this project. It's fine if you live some where else we can work remotely too. I am very much interested in this project,112843,2019-10-15 23:16:50,jenildesai
,112843,2019-10-19 22:35:52,NarendraBadam
"Hi I am a student at the University of Chicago and am looking for a team to work on this competition. I have some experience working with R, Python and large data sets. ",112843,2019-10-15 22:56:26,Nupur Ghuliani
"Hi Nupur,
I am interested in this project, let me know if I can join you guys.",112843,2019-10-16 03:22:51,Avatad
"Hi Nupur, 
I am also interested, I have sent you a message ! :)",112843,2019-10-16 06:36:24,Grégoire
"Hi Nupur, 
I am also interested, let me know if I can join in. ",112843,2019-10-16 08:11:37,jabt
"Hi Nupur,
I am interested to join and work ",112843,2019-10-16 08:29:09,vijay
Interested in teaming up. ,112843,2019-10-18 09:09:39,Kamal 
"Hi Nupur, I will like to join your team",112843,2019-10-19 22:50:12,Abdulazeez Hamzat
"Hi Nupur, interested in teaming up. let me know!",112843,2019-10-22 08:02:51,Pratik Sharma
Hi everyone! Anyone here based in the bay area want to work together on this? either San Jose or San Francisco? I previously worked for HVAC/Energy industry and therefore would like to work on this project,112843,2019-10-15 22:45:14,Shafie Mukhre
"Hi @shafiemukhre , I am from the bay area. I have a similar experience as well and am interested to work on this project. Let me know if I can join you. ",112843,2019-10-16 01:40:01,Dhanaraja
Email me at k.dhanaraja@gmail.com,112843,2019-10-16 03:49:07,Dhanaraja
"Hi I am Apoorva from Colorado..  I have experience in python, data engineering etc. I am interested in working if there is space in the team",112843,2019-10-16 19:19:11,Apoorva
"It's kind of late into the game…  but I might be looking for a teammate.
I work for a utilities company (electric) as a long-term forecaster in a planning division, so my domain knowledge isn't too far off. I work with high-frequency (instantaneous) and 1-hour integrated load often, such as class load study data. 
I spent the past month lurking discussions and notebooks as we ramp up at work. I have the week off where I spent doing a marathon sprint. And I plan to commit the same amount of effort to the team.
What I'm seeking is someone who knows time-series CV in-and-out; this is really one of the biggest hurdles with forecasting, and something I'm not very strong in.
Mahalo.",112843,2019-12-12 01:40:37,Thomas Yokota
teammates needed. LB using single model without leak data is 1.06,112843,2019-12-09 02:05:04,bigbigbang
send my message.,112843,2019-12-12 02:09:36,Ado Dream
"Looking for a teammate. My rank is 68th currently.
I have a LightGBM model trained on Leak data with 1.06 score (single), and after blending + using leak data directly, my score is 0.96 (I think it is very close to 0.95).
My computer has 32GB RAM and RTX 2070 8GB vRAM, so I guess, we can use CatBoost + XGBoost also. 
But I don't have time for this competition anymore. So I won't be to help you further. You can team up with me, If you think our models will blend good or you can train a model using XGBoost + Catboost by modifying my LightGBM model with 1.06 score (trained using leak data also). ",112843,2019-12-07 05:47:06,Kamal Chhirang
"I'm interested. I'm using CatBoost and LGBM and kfold to predict. My best single model score is 1,06 without leak. But I joined this competition too late, which results I have no more time to build other models, so I want to team up and have a better blending or stacking score. Thanks.",112843,2019-12-09 02:12:26,bigbigbang
1.06 without using leak data to train also? I am interested. ,112843,2019-12-12 05:07:51,Kamal Chhirang
"Hi all,
I have an engineering and data analyst background with a special interest in energy technologies and markets. This is going to be my first kaggle competition after taking some courses. Anyone interested in teaming up?",112843,2019-12-03 19:54:42,Aslı A
"@rsmits @snakayama, may I join your team ?",112843,2019-11-30 11:36:59,Marc
Hi @marcpozzo thank you for asking. I noticed you have not yet made your first submission. Please try to use some of the kernels and other materials to at least give it a first shot :-),112843,2019-11-30 21:56:57,Robin Smits
"Hey everyone!
I have been trying to get serious with kaggle (although signed up way back years ago) and hopefully through this competition. I have had past experience with Python, Pandas , Scikit , Tensorflow and ML . 
I have participated in a competition few months ago working through various ML solutions( but couldn't keep up then) 
Looking forward to team-up and learn more.
Drop me an email at rajasvivinayak@gmail.com",112843,2019-11-26 20:22:08,Rajasvi 
"anyone who wants to team up?
contact me at: ayush.patidar4@gmail.com",112843,2019-11-26 16:46:14,ayush
"Hi, I want to team up. I have experience in Deep learning, Python . e-mail: artikwh@gmail.com",112843,2019-11-27 04:21:42,Arti Kushwaha
"Hello, I am a math senior students. I have know some about data analysis and machine learning. If you want to team up, please contact me: fzhang248@wisc.edu",112843,2019-11-21 22:11:22,zfs123
I am a beginner I am interested if anyone wants a team kindly contact me at varthiniraga110@gmail.com,112843,2019-11-19 07:32:57,Raga
Hello! Experienced data scientist here looking to form a team. Pretty familiar with the entire Python data stack. Also willing to contribute regularly to this competition—I have enough time to make a submission daily or every other day. Email is pspenano@outlook.com. Also on Twitter at @pspenano.,112843,2019-11-16 04:29:57,pspenano
We can form a team together. I will contact you on twitter,112843,2019-11-16 19:23:23,Yves Dusenge
"Hello! I've just joined the competition today if anyone is still looking for a teammate? This is my 3rd Kaggle Competition (finished the IEEE Fraud competition a few months ago.) Proficient in Pandas, with usable experience in ML modeling and introductory experience in DL. ",112843,2019-11-14 20:01:02,Luke Gray
I just joined too! If you are still looking for a team mate. Let me know. I have experience with ml. ,112843,2019-11-15 16:36:04,Yves Dusenge
Great! Would love to team up. My email is lugray94@gmail.com. I suppose that'd be best to contact through unless you prefer another platform.,112843,2019-11-17 19:56:08,Luke Gray
Hey! I would love to team up. I have prio experience in ML and DL. Drop me a mail if interested at ritikaagarwal646@gmail.com,112843,2019-11-21 03:36:49,Ritika Agarwal
Anyone?,112843,2019-11-12 17:35:35,NitinKshatriya
"Hello! I'm looking for a team. I come from china. My socre is 1.11 for single lgb model.  My email is 371517587@qq.com.
Please contact me.",112843,2019-11-11 15:39:30,Deng Yin
"I don't have much time in the next few weeks, so I'm looking for teammates. 
I spent too many submissions. If you joined in this competition recently, please consider contacting me.
LB without UCF＋UCL leakage：1.07
LB with UCF＋UCL leakage：1.03",112843,2019-11-11 10:25:49,Jonny Lee
Hey. Can you consider about me. My socre is 1.11 for single lgb model. I am interested in working if there is space in the team,112843,2019-11-11 15:41:15,Deng Yin
You have too much submissions as me.,112843,2019-11-12 03:05:17,Jonny Lee
"I'm interested, but it is my first competition :)",112843,2019-11-13 03:47:46,Alan Clappis
I'm interested.,112843,2019-11-16 18:56:54,small tiger
"Hey @wuliaokaola , have you merged team? If no, can we create a team?",112843,2019-11-18 15:52:02,Neelam
"Hi, Have you merged team? If no can we make a team?",112843,2019-11-18 15:54:39,Neelam
"Hi @wuliaokaola, I'm interested in joining your team. I just started working on this competition, so no submissions. Let me know if interested.",112843,2019-11-20 09:35:24,0DD1
"Judging by your forum posts you've incorporated all of the key cleaning elements and are blending models. Based on your score you probably have some clever features that we haven't discovered yet. We might be able to help with a few important time series features though.
We've been testing out different approaches (no use of leakage), but haven't submitted a fully developed model yet. Mostly working on time series specific feature engineering and non-stationary elements. We have 24 submissions right now and at 1.09 single model, but we haven't yet incorporated some of the basic cleaning steps. We need another 4-5 submissions to verify the critical time series specific feature engineering piece that we've been developing.  That would give us a handful of combined models to submit with your existing.
If you're still interested, give us until Tuesday December 3rd and we'll have our non-leak, minimal cleaning, single model on the leaderboard as proof.  Also, the final two weeks of the competition is when we will have the most time, so it might make sense from that perspective to combine.",112843,2019-11-30 19:03:35,JacobN
"hey gang, I'm looking for a team.
I'm not super experienced but I did well in my first competition, and I'm currently spending 40+ hours a week on this one. EDA is probably my strongest area, but I like to think I'm also decent at feature engineering/feature selection. 
Looking for a single team mate who's easy to work with. Ideal would probably be someone time poor but with more ML experience or a maths background, since I lack both of those. 
email is lewington-pitsos@gmail.com 
I'm in Australia",112843,2019-11-08 03:09:35,Louka Ewington-Pitsos
"Hello Louka 
I have individually finished many knowledge competitions on Kaggle.
Have back ground in Geography, Oil and Gas, Machine Learning and Computer Vision.
I have proficiency in Python, pandas, numpy, Matlab and Linear Algebra.
I did a preliminary study/visualization of the data, would like to work on this project as a team.
Email Id - mayur.nawal@gmail.com
Happy Learning.
Preliminary work:
Weather Variable - based on the following paper:
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3478798/
Aggregating features: Feature-tools library for aggregating data from different data sets.
Feature transformation - Tranforming timestamp into multiple feature like season, categorizing them into dusk to dawn, normalization of other features.
**Would like to join the competition in a team **",112843,2019-11-08 04:28:16,mayur nawal
Hey would you like to join our team ,112843,2019-11-13 07:31:33,sahib singh
Do you have slot in your team? I want to join,112843,2019-11-20 21:05:30,Neelam
"Anyone from the Houston,Texas area want to team up? I have some background in data science and familiarity with ML concepts. ",112843,2019-11-08 02:15:29,Subbu Kumar
"I have been in the energy sector for almost a decade and very involved with analyzing energy data and familiar with some of the background literature and guidelines in this space. Although I am not that proficient with Python or R, I know this domain very intimately, I can help if any of your teams are looking for some of that expertise.",112843,2019-11-07 21:33:44,Sam C
In top 3% in my last competition of IEEE Fraud detection. Anyone wanna team up? Mail me at mjsmanraj@gmail.com with a brief introduction :) ,112843,2019-11-03 16:10:54,Manraj Singh
Hi @themonologue Interested. We already have a team @resistance0108 @nanbei. Would you like to join?,112843,2019-11-04 02:07:36,Pratik Sharma
Anyone still want to team up?,112843,2019-11-03 05:22:32,Jinpeng Gao
Let's build a team. I am interested.,112843,2019-11-03 05:33:45,Anuar Assamidanov
Sure! So what's your email address? I will message you the slack channel. ,112843,2019-11-03 05:53:54,Jinpeng Gao
anuar.assamidanov@cgu.edu,112843,2019-11-03 06:00:44,Anuar Assamidanov
"great, email just sent",112843,2019-11-03 06:03:13,Jinpeng Gao
Anyone in UK/Ireland/ Western Europe time looking for someone on their team? Have a degree in applied maths and looking to learn more on data science ,112843,2019-11-02 11:23:28,Eamonn Organ
Anyone interested in teaming up? I would be interested in participating in competitions and to learn better as well. I have basic ML knowledge and currently learning NLP. ,112843,2019-11-01 16:09:53,Naren
Anyone still interested in teaming up? Undergrad student at University of Waterloo with ~2 years experience in ML. Currently interning as a data scientist consultant.,112843,2019-10-31 23:34:22,bowenyou
"I am interested dude. 
Email Id: rajatgupta301200@gmail.com.
Linkedin Profile:https://www.linkedin.com/in/rajat-gupta-353a6579/",112843,2019-11-08 05:45:38,Rajat Gupta
"Hey, I'm Waterloo grad and working as a full-time data scientist now. Would like to team up?",112843,2019-11-14 22:17:16,Fikasana
"hi  @safura Yes we would be more than happy to have you in our team
would you like to join us",112843,2019-11-15 00:04:27,sahib singh
"hi @sahib12. Sure, my email is suleymanovasafura@gmail.com, looking forward to work as a team",112843,2019-11-18 05:13:23,Fikasana
Anybody from Amsterdam area who's looking to team up? Currently I am doing a MSc in data science at University of Amsterdam.,112843,2019-10-27 14:30:41,jorijnsmit
Anyone in San Diego California ?,112843,2019-10-25 20:26:08,Prashant Kolkur
Am in LA. Do you have a team already?,112843,2019-10-29 21:03:40,imVignesh
I am a ​Ph.D. student in Economics at Claremont Graduate University. I want to start my Kaggle journey with this project.  I have experience in data engineering.  ​How about building a team together? My email: anuar.assamidanov@cgu.edu​,112843,2019-10-29 23:48:48,Anuar Assamidanov
Hey guys is anyone in to form a team? I have 3 years experience in DL. I graduated as an electrical engineer. Also have the required resources.,112843,2019-10-25 13:20:27,Kristóf Horváth
"Hey Krist, apoorve here, if you are interested to team up, reply back at apoo123rve@gmail.com ",112843,2019-10-25 22:37:00,apoorve kalot
"I am a technical lead working since last more than 10 years in web development.
Practicing machine learning since last 1 year and looking for a team to join. ",112843,2019-10-23 18:15:12,Moiz Kachwala
"Hi, my name is Saurabh, an IT Master graduate , currently working for a company as Jr. Data Scientist. I am looking for a team to join, if anyone is interested in collaboration my email id is  saurabhpanwar3@gmail.com  ",112843,2019-10-22 11:39:07,Saurabh panwar
"Hi guys, Have some background in statistics, EDA, supervised ML techniques and energy sector. Have been participating in competition on other websites too and using Python since last 2.5 years. If anyone is interested in working together, please reach out.",112843,2019-10-22 07:59:02,Pratik Sharma
Interested in Teaming up. Let me know,112843,2019-10-22 10:17:34,Saurabh panwar
can't find you in the list of users for teaming up. have you registered yourself for the competition?,112843,2019-10-22 11:04:10,Pratik Sharma
"Hi Pratik, I'm also interested in teaming up!
Your experience and domain knowledge seems wonderful.
I don't have strong ML background, but marked 8th by finding magic feature in latest competition so my insight may help you.",112843,2019-10-22 11:46:15,resistance0108
have sent a request for teaming up @resistance0108 ,112843,2019-10-22 12:01:05,Pratik Sharma
Thank you!,112843,2019-10-22 12:03:40,resistance0108
have sent the request @saurabhpanwar ,112843,2019-10-22 12:14:31,Pratik Sharma
Hey Pratik do you have more place in the team? I am interested to join you.,112843,2019-10-28 08:18:50,Kristóf Horváth
"Hi,
I am a post-graduate student majoring in data science, and a beginner in kaggle. I want to use my knowledge to solve practical problems. I have some experience in Python/Pandas， and familiar with some basic models. I am from Chinese Mainland, and now study in hongkong. If anyone is interesting in working together ,and don't mind communication remotely ,my email is nanbei629@gmail.com.
Thanks",112843,2019-10-22 07:54:24,Ada_wang
"Hi Ada, interested in teaming up! let me know",112843,2019-10-22 08:04:37,Pratik Sharma
"Hi,Pratik, I am interested in working together with your team",112843,2019-10-22 08:54:56,Ada_wang
have sent the request to you.,112843,2019-10-22 10:00:00,Pratik Sharma
"Hi guys, i am apoorve, English speaking from India, have experience with python/pandas/neural networks and new to kaggle and currently looking for teammate/s. for this competition. Contact me on apoo123rve@gmail.com for team-up :)",112843,2019-10-21 06:57:53,apoorve kalot
"Hi Apoorve, Pratik from India. let me know if you're interested in teaming up!",112843,2019-10-22 08:05:14,Pratik Sharma
"hello prateek, thanks for contacting me, please send your email Id or contact me on my email ( not able to send mail through kaggle, due to limited access ) i am interested for team-up :)",112843,2019-10-22 15:49:57,apoorve kalot
"Hi Prateek, Apporve, I am interested too. Please let me know.",112843,2019-11-07 06:04:18,Mahesh
"Hi,  This is Sailusha. I am a newbie to DS.  I would like to team-up, please let me know.",112843,2019-11-07 06:34:28,Sailusha
"Anyone from Taipei? I am experienced in Python, R and time series forecasting. ",112843,2019-10-19 15:38:43,Davide
"Hey, I am from India looking for teaming up.",112843,2019-11-18 07:39:06,NitinKshatriya
"Hi , I am Apoorva Bapat from Boulder, Colorado. I have experience with Python, PHP, R. I have worked with a startup to get the company their Angel funding, competed and won Go Code Colorado 2019 and I currently work with an energy and utility company. I am looking for a team to join! message me on andbapat@gmail.com if interested!",112843,2019-10-17 17:56:48,Apoorva
"Hello Apoorva, 
Call me Kola. I am from NC and currently  Data Scientist III. BS in Electrical Engineering and MS in Software Engineering. I am interested in partnering on this project. ",112843,2019-10-17 23:40:40,Niceclat
"Hi Apoorva, Interested in teaming up! let me know.",112843,2019-10-22 08:05:46,Pratik Sharma
This Comment was deleted.,112843,2019-11-01 14:15:00,No user
"Hi, I'm Tokyo-based Chinese, and I'm currently working in architecture industry.  I have some experience on Python ML/DL, and might have more knowledge on energy consumption than on data science. ",112843,2019-10-17 09:46:38,Iris
Interested in teaming up. ,112843,2019-10-18 09:08:27,Kamal 
"Hello I am a Portuguese Engineering Student doing my masters, currently studying in Germany, I have a good grasp in python and Machine learning models . I haven't done yet a competition involving time Series , I usually go to competitions involving texts interpretations and sentiment analysis etc.  But this competition seems very interesting to do and I am eager to find a team. ",112843,2019-10-16 15:51:40,Álvaro Silva
email me at agboolalucas2000@gmail.com,112843,2019-10-16 11:15:39,Mubarak
"Hi Addison Howard,  I will definitely be a plus to your team.",112843,2019-10-16 11:15:11,Mubarak
I am also interested in teaming up with any of you here. This will be my first Kaggle (although I was browsing previous discussions). Gmail me at gops75,112843,2019-10-16 09:13:40,Gopinath Venkatesan
"Hi Guys,
I'm in GST time zone. Having experience with Python and DS but a newbie on Kaggle. Let me know if someone wants to team up!
Cheers,
Shyam: shyambeersingh@gmail.com",112843,2019-10-16 09:02:52,Shyambeer Singh
"Hey anyone in the CEST timezone,  I have a strong background in Computer Science and have worked on a number of machine learning projects. If interested to form a team kindly reply. ",112843,2019-10-16 03:44:35,Migaliza
"Hey migaliza, I'd be interested to team up ! FYI i'm an engineer with limited ML experience but i know my way around, is that fine with you ?",112843,2019-10-16 09:55:32,Gijs Barmentlo
Hi I am computer science student and I have some knowledge and how to solve regression problem if you want to join me please contact me through pkvijay11042002@gmail.com,112843,2019-10-16 02:56:13,vijay
Hi I am PhD graduate in Control and Estimation Theory and have energy/HVAC knowledge from my undergrad. I am interested in this project. I am familiar with many machine learning techniques (completing deep learning specialization in Coursera),112843,2019-10-16 01:12:08,Controlist2025
"Hi there @hzengin2025, I am interested. Our paths may have  crossed in the past. ",112843,2019-10-16 01:55:01,Dhanaraja
Iam ready to team up. You shall mail to tcessk@gmail.com. iam good at feature engineering and ensemble based predictions. ,112843,2019-10-16 02:17:46,Gokagglers 
Hi @loveall @hzengin2025 . My email is k.dhanaraja@gmail.com,112843,2019-10-16 03:50:54,Dhanaraja
"Hi Dhanaraja, check your email :) ",112843,2019-10-16 06:30:52,Sakib Shahriar
"Hey, 
I'm an industrial engineer and and I have some experience with machine learning, I'd like to work on this project ! Feel free to email me at gijsbarmentlo@gmail.com",112843,2019-10-16 14:21:09,Gijs Barmentlo
"Hi Nupur,
I am in interested",112843,2019-10-15 23:02:31,lionel melhy
This Comment was deleted.,112843,2019-12-12 02:46:42,No user
This Comment was deleted.,112843,2019-12-09 05:54:04,No user
This Comment was deleted.,112843,2019-11-20 14:55:05,No user
This Comment was deleted.,112843,2019-10-28 21:59:31,No user
This Comment was deleted.,112843,2019-10-18 06:17:36,No user
This Comment was deleted.,112843,2019-10-17 14:35:42,No user
let me know if you're interested in joining,112843,2019-10-23 19:27:28,Pratik Sharma
This Comment was deleted.,112843,2019-11-01 14:14:23,No user
"Your validation split is leaking information - so longer epochs the leak lets the eval score get better, but the model is over fitting and would have early stopped with a not leaking validation set.  
I think the solution is to change the split - not reduce epochs.",113831,2019-10-22 22:01:22,PC Jimmmy
"You are right , cause i see the val_score always keeps decreasing no matter many epochs i take. That is indeed possible when there is some leak , only i still haven't figure out a gd solution to resolve it.  ",113831,2019-10-23 05:21:07,Khairul Islam
@khairulislam is your score 1.12 a single model score?,113831,2019-10-23 17:29:46,Manraj Singh
@themonologue  i took mean of the predictions from models trained during cross fold validation. ,113831,2019-10-23 17:45:14,Khairul Islam
@khairulislam I mean did you use only one model? Only LGBM?,113831,2019-10-23 18:06:13,Manraj Singh
@themonologue  Yes. Still haven't managed time to work of blending models.,113831,2019-10-23 18:20:48,Khairul Islam
"@khairulislam Okay,thanks. All the best.",113831,2019-10-23 18:58:26,Manraj Singh
"Thank for your comment, I was using too many estimators around 5000 getting an score of 1.30, I will try to train with less estimators, how was your eval score with 500 estimators?
Thanks for sharing Khairul.",113831,2019-10-23 13:19:35,Ivan de los Santos
it was around 0.88.  For 400-500 estimators and n-fold = 5,113831,2019-10-23 14:40:15,Khairul Islam
"I am not using KFold yet, but I managed to get 1.27 which is kind of a bit improvement, I will keep tweaking and probably will go down to 1.20, thanks a lot.
I mean in my final submission, validation score was around that too.",113831,2019-10-23 18:38:47,Ivan de los Santos
"Interesting! What was your val score after the changes? How did you select your validation set, traintestsplit?
I personally tried TimeseriesSplit and StratifiedKFold on building IDs keeping the remaining settings the same. 
Till now, I got better results with the stratification, so I guess it's really important to maintain the same buildings in training and validation sets.",113831,2019-10-22 22:47:34,Federico Raimondi
"Not sure that's its possible to keep leaks out.  I am trying to have different buildings in the train vs validation, but other buildings for the same site can leak weather info.
Right now my brain is fried - can't seem to get the right number of rows in train and test data sets with my merges.  All my work up to today has been done with bad merge so not sure I can believe anything I know.  But once I get the merges fixed will try some other splits.  
Seems like will need to make a judgement call - what worries me more - unbalanced buildings or memory leaking?
Using a time split my catboost model ran for 250,000 epochs and still wanted to keep going :)
When my model never wants to early stop one of the things that has often been the case was a leak.",113831,2019-10-22 22:58:57,PC Jimmmy
I used StratifiedKFold on building IDs. My validation scores were around 0.88.,113831,2019-10-23 05:18:24,Khairul Islam
"
Using a time split my catboost model ran for 250,000 epochs and still wanted to keep going :)
  I am facing the same issue ☹️ . Only i don't run more than 500 epochs, as it overfits afterwards.
",113831,2019-10-23 05:22:38,Khairul Islam
"Thanks for the reply @khairulislam. When you say 'made some feature categorical', do you mean newly created features or just passing some of the starting features as categorical to the model? ",113831,2019-10-23 15:49:09,Federico Raimondi
"@raimonds1993  Some of the starting features like primary_usage, meter etc. I did make 2/3 new features, but they weren't categorical.",113831,2019-10-23 16:14:29,Khairul Islam
The problem is that your validation set might have a very different distribution than the test set - so your models behave differently.,113831,2019-10-22 21:33:02,Konrad Banachewicz
Yes. As we know train data is from year 2016 and test data is from 2017 and 2018 . May be some time related features or weather change due to time causing huge difference . ,113831,2019-10-23 05:24:41,Khairul Islam
Overfitting is Allways a big issue here on Kaggle.,113831,2019-10-22 20:16:07,mezoganet
"hmmm i guess because of the split, the varying between the public and private lb sometimes scares me.",113831,2019-10-25 20:43:24,olaleye eniola
@khairulislam thank you for your post! Are you training on whole train dataset? I get a score of 1.46 with just 5% of the train dataset,113831,2019-10-23 23:10:40,Hakan
"Ah yes ,  I am training on whole train dataset. 

I get a score of 1.46
  That's interesting.  I haven't seen anything like that so far 😲 
",113831,2019-10-24 04:42:37,Khairul Islam
Nice work! Thank you.,113831,2019-10-23 22:24:18,Agastya Kommanamanchi
This Comment was deleted.,113831,2019-10-23 05:29:13,No user
Thanks for sharing.,114380,2019-11-01 09:31:08,LongYin/杰少
Thank you for mentioning my kernel! Our team will check out the past competitions to see if we can get some insights.,114380,2019-10-26 13:13:15,Carlo Lepelaars
thanks for sharing!,115499,2019-11-05 08:58:56,LongYin/杰少
thanks for sharing!,115499,2019-11-05 06:14:16,Pratik Sharma
Thanks. I don't know why people downvote stuff. ,115499,2019-11-03 11:21:01,Manraj Singh
Thanks for sharing!,115499,2019-11-07 05:21:19,Vikas Singh
"awesome, thanks",115499,2019-11-06 21:14:55,Felipe Mello
,116147,2019-11-07 10:17:46,Vopani
"Heewww ! It is so cold where you live…. Brrr Brrr Brr.
Wait for tomorrow. Sun will be there, for sure !",116147,2019-11-07 20:18:55,mezoganet
,116147,2019-11-07 10:47:13,Manoj Prabhakar
"Good atmosphere here ! 
Thanks, I will be back …",116147,2019-11-07 20:17:40,mezoganet
,116147,2019-11-07 19:35:03,kxx
"It's all over now… Baby Blue 🤜 
https://www.youtube.com/watch?v=af7ngGxEusE",116147,2019-11-07 20:25:15,mezoganet
I wish I could upvote you twice for that reference - one of my favourite BD tracks from his early era.,116147,2019-11-07 21:50:51,Konrad Banachewicz
"
Blending season is upon us 

…and it is actually making me really happy. When some get lured in the easy (but probably overfitted) path of LB climbing, others stick to their trusted models. It worked for me in IEEE Fraud Detection at least! :) 
",116147,2019-11-07 13:30:35,Georgios Sarantitis
"So what ? (Miles Davis) 
https://www.youtube.com/watch?v=zqNTltOGh5c
listen to that and keep cool 🤘 
Sure you will recover once.",116147,2019-11-07 20:16:51,mezoganet
That's *some *footage btw!,116147,2019-11-07 21:10:12,Georgios Sarantitis
"This is Miles Davis, don't know what you are meaning.
Just Miles.",116147,2019-11-07 21:15:41,mezoganet
Good score !!!,116147,2019-11-07 21:27:22,mezoganet
"Yes, people should pay more attention to keeping tight correlation between their own CrossValidated score on training/holdout set, and their personal LB submission.
What training set split and CV methodology do you suggest?
Do you think the ""half and half"" (caret::createTimeSlices) timeseries rolling split causes leakage? esp. when one single day is split between training and holdout set?",116147,2019-11-12 20:26:47,Stephen McInerney
Be more optimistic - that means different approaches could provide different results. And after you will add something unique to them (like features or processing) you will be able to increase your score at ease. Just don't spend any submission for blending right now - the blending guys did this for you.,116147,2019-11-07 12:00:07,WispZero
"Hem … Explain me what a blend can add to one competition, except but blending… Heeww (Homer Simpson saying).",116147,2019-11-07 20:59:40,mezoganet
Its the beginning of blending Season. It will be on till competition gets over. ,116147,2019-11-07 10:47:54,Manoj Prabhakar
,116147,2019-11-07 21:03:59,jleecook
"Hem, wot you mean ?",116147,2019-11-07 21:13:31,mezoganet
"Sorry mate! There is always a first one ;)
But it works as tutorial as well, for people that still don't know the power of the ensembles…",116147,2019-11-07 13:11:32,Henrique Mendonça
"But it works as tutorial as well
are you really sure ?",116147,2019-11-07 20:10:25,mezoganet
"It's been like a tsunami, but even if it was'nt for me it seems to shake up all around… And I can understand some frutration.",116147,2019-11-07 21:24:57,mezoganet
"@konradb 
You know what ? This competition is really oughta sight ! That's a weird competition : 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114893#latest-666521
What a feeling having  so much DownVotes…
Stand up man I will not win, but it's a really weird competition.
Is Blending the real point ? Don't know.  Is there something to grab from Data ? Don't know.
Whish you the best, and I am sure you wille be among the best score, sure !",116147,2019-11-07 21:12:46,mezoganet
"Thanks, that's very kind of you.",116147,2019-11-07 21:51:39,Konrad Banachewicz
"***My bittersweet experience of Blending:

https://www.kaggle.com/c/ashrae-energy-prediction/discussion/116264#latest-668099",116147,2019-11-09 01:48:44,Naruhiko Nakanishi
"@konradb 
Allways been a follower and a supporter, really !",116147,2019-11-07 20:54:28,mezoganet
,116147,2019-11-07 20:08:53,mezoganet
"I've got a feeling it won't be the last :-/
what you mean ?",116147,2019-11-07 20:02:48,mezoganet
"The last mega drop - I busy myself with sth else for a day or two, go back to Kaggle, booom 100 spots down. Happened a few times before (with pub kernels usually), but never so fast :-) ",116147,2019-11-07 21:52:33,Konrad Banachewicz
"While I agree that there is always difference in skills, resources available and so on, I'l absolutely against competitions, where hosts made mistakes and the way to win competition is to exploit leaks or scrape data.
The hosts won't get what they want (good solutions) and this isn't what kaggle is about. Competitions are about using data and finding patterns, in real world you won't find labels in the internet.",117016,2019-11-13 12:19:00,Andrew Lukyanenko
What I have learned during kaggling is that around 50% of the tricks being used here cannot be used in real world.,117016,2019-11-13 12:27:33,Roman
Many people are here for the other 50%,117016,2019-11-14 13:11:15,bluetrain
for example? ,117016,2019-11-14 18:30:06,EnvyMyFieldsMedal
upvote !,117016,2019-11-15 00:31:25,Hanjoon Choe
"If I knew that test or part of the test is available on the internet, I would never start this competition. I would rather do another competition. When I started, however, this leak did not exist. And now I have already invested time and work. Can you tell me that the other part is not ready for download anywhere? No. That's why I have a bad feeling about this competition. The leak seems to give big boost here. So should I focus on finding leaks to improve my score? I do not much like it.",117016,2019-11-13 07:56:05,Oleg Knaub
"At least, the way the leak was revealed still makes the competition quite fair currently: @gunesevitan @mpware made the leak public mostly as soon as they discovered it and prepared kernels (this and that) where all the web scrapping was curated for us. Thanks to them and to their honesty, we can all use the leak without much efforts. This would have been totally different if they had kept it secret untill the last days of the competition.
The main issue now concerns the leaks which are not public right now. Are there other sites with the target available ?",117016,2019-11-13 08:39:40,FabienDaniel
"I believe found leaks should be shared ASAP to make it fair for everyone. I would prefer to lose rather than winning with a dirty way. I came here to learn in first place after all…
and yes, there are other sites with target available.",117016,2019-11-13 08:49:09,Gunes Evitan
"
there are other sites with target available.

which sites ? 
@sohier are you already aware of all the sites than can be scraped ? Did the Kaggle team anticipate these leaks ?",117016,2019-11-13 08:54:06,FabienDaniel
"I think so, they are prepared for all data which can be found publicly being revealed.
And of course in the test set there is some data which is never published.
That's why after demanding clarification of this leak for 3 days and yet not a single organiser has stood out to say a word.
There is even one official guy upvoting this post which you can basically see the attitude of those organisers.",117016,2019-11-13 09:16:26,Jie Lu
"People are right to be annoyed - it is about the uncertainty that scraping creates. Who has scraped what? How much has team #1 found? Team #2? What if there's a group out there who found it all and is just about to submit? What are Kaggle going to do? When are they going to do it? When will they acknowledge the issue? How much will the competition change? Even if they leave out site 0 as you suggest, what if more public data is discovered? What if it's discovered in the last week?
These are not the kind of questions people came here to ponder.
External data rules are not the rules that apply here — external data is supposed to be something that could be gathered in the real world & used for prediction — not just scraped answers.
Scraped answers are worse than useless - they actively devalue the work the sponsor put in to collect data in the first place. (Why would anyone want to mark themselves out as the kind of person who spends their time scraping answers? Gold topics & kernels?)

Don't like - don't participate.

But competitions don't exist in isolation - compromised competitions create uncertainty about the platform. Winning by peeking at the ground truth is not winning, just cheating. The more this goes on, then the Kaggle gold standard isn't a simple gold standard any more. There are caveats. Specifics. People in general don't have time to remember those specifics - just that they are there. It pulls the whole site down.
It can be hard to let things go, and see others pass you on the leaderboard. There will be fear of missing out - but on what? A tainted win? You don't get to decide how you spent last month, but next month is up to you, and what does a sunk cost have to do with that?
You may want to check what happened last time someone went too far with external data.

any evidence that someone used the data to make a submission before this rule change will result in that individual's (or team's) removal from the competition once it has closed.

(I feel for Kaggle having to sort this out. It seems like it gets harder over time to run these competitions. The discussion & kernel incentives win out and are too strong to allow any hope that a whole crowd avoids temptation to go searching for a leaderboard shortcut…)",117016,2019-11-13 11:34:26,James Trotman
A competition with test labels that can be scrapped is definitely a lose-lose case for both the organizers and the participants. ,117016,2019-11-17 07:14:24,Xuan Cao
"
But don't be toxic, please 

I would argue this is the most toxic post in this competition. For sure more toxic than asking what is the effect of scraped data or whether the leak will be removed from private test set.

Will I use scrapped data? Sure.  

Then make sure you write a post in the External Data thread, I don't see any post of you or your teammates there ;)
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112841#latest-671680
Also, make sure you make all the external data available to all other participants, as per Competition Rules:   
C. External Data. You may use data other than the Competition Data (“External Data”) to develop and test your models and Submissions. However, you will (i) ensure the External Data is available to use by all participants of the competition for purposes of the competition at no cost to the other participants and (ii) post such access to the External Data for the participants to the official competition forum prior to the Entry Deadline.",117016,2019-11-13 07:36:49,bluetrain
"
There is some ""magic"" in data but I didn't find it -> competition is badly prepared and should be canceled.

Uncle CPMP would be triggered on this one :)
Kostia, you have posted a tread describing your way on kaggle. And, according to it, you never was in the situation when real leak has been found. Yes, it is frustrating to invest a lot of your time doing real data science just to see yourself dropping out of the gold-silver zone because people have found a leak.
But please don't try to excuse usage of scrapped true labels in the predictive competition.",117016,2019-11-13 05:57:22,Roman
"The point is: which skill do you want to develop in your free time? I don't want to spend 60% of my time to search for hidden leak or hidden dataset online and scraping it, but i want to increase my skill in ML and DNN so…",117016,2019-11-13 07:53:13,Davide Stenner
"I have not been following this competition so I don't know what is going on, but I would like to remind everyone that Kaggle just added a new Dataset tier to everyone' profile (info here). Perhaps there is opportunity in this competition to scrap data and make Datasets to share with other Kagglers. You can learn data scraping and start acquiring Kaggle Dataset medals.",117016,2019-11-13 15:10:28,Chris Deotte
"You make some good points above, but seriously: there ARE badly prepared competition - starting with at least two versions of Santander… Yes, people whine and moan (although I find the lgbm / nn example a bit contrived) - so what? Filtering out the noise in the forum, including people leeching for freebies, is part of the package…",117016,2019-11-12 22:54:23,Konrad Banachewicz
"And Microsoft. And ELO. And many others.
Kaggle also gain experience as we all do. I think we need to support each other (not only member-member but also community-host). 
I didn't see any structured proposal how Kaggle can deal with these leaks.

Remove Site0 - possible.
Private validation on site that never were published online - possible.
Better control data in future - possible.
…

But I saw many ""LET'S BURN IT!!"".
Why we should? I've invested almost month in it. Should people like me drop everything because one site_id gone public (and gave 0.03 on public lb boost)?",117016,2019-11-12 22:56:59,Konstantin Yakovlev
"Admirable goal, but let's be realistic here: on the one hand you have people cutting corners (as evident from the sock puppet purges), on the other - leeches in the forum. If we can establish a baseline of civility, that's already nice. 
Experience or not - leaks in Santander were just embarrasing :-)",117016,2019-11-12 23:01:01,Konrad Banachewicz
"
Santander were just embarrasing

Shit happens)) No matter how hard you try - sometimes it just happens.",117016,2019-11-12 23:06:06,Konstantin Yakovlev
"Problem is in expectations. There was less ""fair"" competittions then ""unfair"" for last two years. There is always some trick to use. Still you have to use ML skills to distill the trick into scores. Scrapping is not a trick - if you can find all answers to test set then it is not ML competition. Everyone feels frustrated because of that. Not so much motivating.
But you are feel free to continue. 
""Don't invent excuses for your dealings. Decided to scrap data - go, decided to win at all cost - ok, decided to cheat - also ok. But don't make a storm in a teacup after you will be treated as a web scrapper. Respect other's decisions. Don't say something is beautiful only because you are in.""",117016,2019-11-13 08:27:56,WispZero
"I'll go with my grain of salt, because I think this goes beyond this one competition. I haven't taken part in it either by the way, but still think this applies outside of this one competition.
First of all, I kind of agree with @jtrotman that sunken cost shouldn't really matter. Just because someone worked hard, doesn't mean they should be entitled to anything, IMHO. Hard work, if done well, helps increase your odds of reaching the top, but is not, in any way, a guarantee of success. Sometimes, someone can come up with an easy solution without much work and out perform everyone else: as you said, life isn't fair.
I do agree mostly on your point, that not everyone has the same tools in their hands, and that's how most things in life are. However, I also think we should thrive to make things fairer for everyone. Not everyone can have access to GPUs, that's just how it is (and I'm a student, on the low end of the budget compared to most people here I think, so I'm not in the best place when it comes to having access to good compute). Being smart about some things can help go a long way, and outsmarting raw performance is always satisfying, to me at least.
Now, I do think there are things where we can try to improve though. Just because some people seem to be unpleased with some aspects, wrongly apparently, doesn't mean there aren't things to fix in Kaggle competitions.
I think like a lot of things, it's not just black or white. so while I agree with the overall argument you're making, I still don't agree on everything, because I do think we should speak up when some things aren't fair and could be fixed. 
Good topic to bring up though, and I'm very pleased we can have a discussion about it here, it's interesting to see what others from all levels think, and helps also rethink the way I see things.",117016,2019-11-14 08:26:32,Maxime Lenormand
"Thanks for the @!
I should have added that I agree with the other points too - there is a lot of unfairness in Kaggle, disparity in hardware, free time. See Jeremy Howard's responses in this twitter thread for example:

Jeremy Howard:
  I'm similar - specifically I look for people that have achieved an unusually high level of capability despite limited opportunities or significant constraints. It's been the best hiring signal over many years and companies for me.

There have been write-ups by people finishing far out of the medals that impressed me anyway. Super lightweight solutions can be very useful, and many gold medal solutions can be entirely impractical. The real skill is knowing what is useful in the real world (but  hosts/sponsors do not usually comment on this).
The bit I disagree with is saying that scraping answers is just part of the game. It feels unfair to me to try to force people into going along with it when it is so obviously against the original intent of the competition. Even downloading ground truth labels other people have scraped feels very wrong; tedious extra work, zero insight, zero learning, purely to climb the leaderboard, like being the meat puppet pulling the levers in computer-on-computer action.
Also - it feels a bit harsh in retrospect when I said respect rational economic theory and just drop it! People don't really think that way. It is something I have to consciously tell myself if I no longer enjoy a particular competition - it can be a struggle to stop.",117016,2019-11-14 11:30:36,James Trotman
"Thanks for the read!
That Scrabble story hits home a lot, and is the reason why I usually end up in the bottom in competitions, but have probably learned a lot more than a lot of people.
It still is frustrating to see that you've put in a lot of hard work and a lot of the people above you have copied other answers. But if we were making competition with a leader board of who has spend the most time, I wouldn't want to participate, because that also is boring.
I think I feel pretty close to what you have been saying in this thread, so thanks for the thinking and the links, once again. ",117016,2019-11-14 13:00:51,Maxime Lenormand
"@kyakovlev I'm deeply sorry to hear you have no friends, no GPU, you don't know NN and you're lazy. 
Life is really unfair with you.",117016,2019-11-16 09:12:57,olivier
I believe he was being overly humble. His track record shows he's none of the above!,117016,2019-11-21 06:11:02,yukiya
"@wispzero 

There is always some trick to use. 
  win at all cost - ok, decided to cheat - also ok.

Win is always at all costs. Cheats - 100% NO - Competition rules are very clear here what we may use and how. 
Cheats in my vision:

Multiple accounts
Private sharings
Hide External data
Join team and not collaborate

Not cheats:

Resources - CPU/GPU/memory 
Data cleaning
Data enriching
Multidisciplinary Teams (As for molecular competition - area experts in team is not cheat)
Use public Kernels/Discussions
Modeling (Frameworks/architecture)
Paper/Book reading (people that don't read well in english are very limited)  
External consulting with field experts (that do not participate in kaggle)


@stecasasso 

this is the most toxic post in this competition 

Agree)) Seems that my english is not good enough to ""transmit my thoughts"". Anyway I said what I've said - it's my opinion. I'm just asking to read deeper than words and see the main idea.

@stenford23 

The point is: which skill do you want to develop in your free time? 

Good question and in my opinion in question you have also a part of the answer.
Define your goal:

Want learn new things - you don't need to be in medal zone to read and try new things. Medals are just for EGO (beat yours own score and not lb)
Want to inference with ds enthusiasts - teamup and share ideas (evolve, learn, teach).
Want create new model architecture - Code, share, discuss but in 99% you will not be able to beat sate-of-art as BERT for NLP models. You can contribute to global ML pool. Even small change and improvement is valuable.
Want win - ohhh this is other story and for me not the main reason to participate kaggle competitions.
It's very rare case when you have all together. Focus on your goal.


@oleg90777 

Can you tell me that the other part is not ready for download anywhere? 

A month ago I've mentioned that ""When I look at data I see ""not very reach"" features dataset. Seems that we can and have to use a lot of external data.""
Yes, test labels leakage is worst type of leak but why it's so different of scrapping weather data or historical consumptions for 2014/2015? Scrapping (enriching) data is just an instrument, same as data cleaning. 
I'm hoping that organisers did good analysis and will prevent such leaks in private set. 

@stecasasso 

Then make sure you write a post in the External Data thread 

Be sure all our external sources will be disclosed until ""external deadline"" date. Also not to forget that any link that was posted before by other team/member not needed to be posted again - link duplication is an optional step. 

@nroman 

Uncle CPMP would be triggered on this one 

I don't remember that @cpmpml asked to cancel competition. Anyway this post was made to ask people be wise and look on each situation from different views. I'm asking not say that ""greenholders"" are miserable if you don't like green colour. 

@nroman 

you never was in the situation when real leak has been found 

True. But read many stories like that here and can feel the pain. Also @cdeotte told me story of Microsoft competition labeling and I'm very sorry for such cases. Can we guarantee that it will not happen again? Sure not. We just need to leave it and keep going.

@jielu0728 

That's why after demanding clarification of this leak for 3 days and yet not a single organiser has stood out to say a word. 

That's the most alarming situation and I hope @claytonmiller @addisonhoward @chrisbalbach @sohier can break silence and clarify leak in the very short future.

For any constructive discussions (never mind if you agree with me or not) I'm inviting continue in private messages (linkedin link in my profile).",117016,2019-11-13 11:53:14,Konstantin Yakovlev
"Yes, you did and i read it. But with ""to use a lot of external data"" i understand something other like to just copy the test data and replace the predictions. 
I also hope that organisiers will prevent such leak in private.",117016,2019-11-13 12:05:46,Oleg Knaub
"Rules clarification from Kaggle: 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117357#latest-673345",117016,2019-11-14 21:44:33,Vishal 
"I think 78%:22% is reasonable. maybe the leaked data will not affect the private LB. But the public LB is useless for now. 
Having a rest and waiting for clarification form host is my choice.",117016,2019-11-14 12:40:41,Jonny Lee
This Comment was deleted.,117016,2019-11-13 00:34:25,No user
Good job !,117083,2019-11-14 03:56:38,uttam kumar
Great works!!,117083,2019-11-13 21:40:32,Hanjoon Choe
Hey guys. It's a Supernova … don't forget it ,122362,2019-12-19 18:37:09,Rasselio D.
lol,122362,2019-12-19 18:47:14,Vishal 
"It's not private sharing anymore, it's public sharing )))",122362,2019-12-19 19:34:12,Mukharbek Organokov
I'm really curious about the shake up,122362,2019-12-19 19:35:59,Firat Gonen
@frtgnn it's like a New Year lottery ))) I don't know what to expect and impressed by it,122362,2019-12-19 19:38:56,Mukharbek Organokov
true :),122362,2019-12-19 20:48:31,Firat Gonen
Wow. There are extremely intelligent folks. Getting a score of 0.953 in just one submission. ,122362,2019-12-19 17:30:20,Manoj Prabhakar
@manojprabhaakr on the last day ))),122362,2019-12-19 17:32:00,Mukharbek Organokov
"They are what recruiters referring as ""rockstar data scientists"" :D",122362,2019-12-19 17:34:00,Gunes Evitan
hahahaha,122362,2019-12-19 18:47:25,Vishal 
so jealous of their smartness,122362,2019-12-19 19:03:18,Firat Gonen
@frtgnn ahahahah,122362,2019-12-19 19:11:34,Mukharbek Organokov
Private sharing has never done so blatantly in the history Kaggle,122362,2019-12-19 17:21:47,Gunes Evitan
"@gunesevitan yeah, people have started less and less caring of that. ",122362,2019-12-19 17:24:16,Mukharbek Organokov
"relax guys, kaggle will take care of them!",122362,2019-12-19 19:02:49,Firat Gonen
But I know that Kaggle team will take care :) ,122362,2019-12-19 16:38:04,Mukharbek Organokov
Happy Holidays to all! Same on our Private Leaderboard! :D,123057,2019-12-25 11:26:56,devai01
Thank you! And happy holidays to you and all kagglers ! Wish you a merry xmas and happy new year :3 ,123057,2019-12-25 10:40:58,LinhLPV
Merry Christmas :p,123057,2019-12-25 10:58:36,Kangyu Chen
happy holidays to you too !..(upvoted :)),123057,2019-12-24 17:11:03,ravi tanwar
Thanks! I wish you a merry Christmas 😊 ,123057,2019-12-24 17:44:46,Oussa
Happy holidays to all!,123057,2019-12-25 10:00:56,Firat Gonen
"Helpful Stuff
Thanks @hamditarek ",112938,2019-10-16 17:06:33,𝓥𝓮𝓮𝓻𝓪𝓵𝓪 𝓗𝓪𝓻𝓲 𝓚𝓻𝓲𝓼𝓱𝓷𝓪
"You are welcome @veeralakrishna, you can check the code in my kernel.",112938,2019-10-16 17:09:52,Tarek Hamdi
"Sorry about that, the rules should have also specified 2 per day. I've updated them now.",112993,2019-10-16 15:41:34,Sohier Dane
Thank you for the quick reply!,112993,2019-10-16 15:44:21,hukuda222
Edit: Correct - it's 2! Sorry for the confusion. Corrected in our system and in rules.,112993,2019-10-16 17:45:20,Addison Howard
"2 is a real issue as far as I am concerned - I really could use 5 for the next month.
Assume that your poor little servers cannot handle the load of the large files size of the submissions.",112993,2019-10-18 04:51:03,PC Jimmmy
I guess you did some FE with the floor_count?,113013,2019-10-16 17:57:15,Manraj Singh
"Nope, weather",113013,2019-10-16 19:32:57,Konrad Banachewicz
"Trace precipitation. In the case of 1-hour and 6-hour duration precipitation
accumulations, trace precipitation is coded as a “-1” value.",113103,2019-10-18 01:23:11,Clayton
"Cheers @claytonmiller!
What about meter_reading = 0, can it be both missing metering data and actual 0 kWh?
I guess really, my question is what is meant by ""measurement error"" in the Data tab? It could mean measurement uncertainty in kWh, or it includes more fatal errors. E.g. certain meters reporting 0 kWh could be due to communication problems, where meter signaling is working but data load is lost. ",113103,2019-10-25 04:16:13,Martin Elfstadius
Thanks.,113103,2019-11-04 13:42:00,LongYin/杰少
"@claytonmiller Thanks for your response. May be you should start a dedicated thread for data related queries. That will be helpful for all the participants as the discussions will be there at one single place. 
My two cents. :-)",113103,2019-10-18 01:55:50,arnab
Awesome!,113136,2019-11-23 20:11:26,Carlo Lepelaars
Great post,113136,2019-10-24 19:05:06,Aykut Çayır
Thanks for sharing!,113136,2019-10-17 09:06:43,Mohammed Tayor
Thanks for sharing!,113136,2019-12-14 12:53:43,Beans
"Kostiantyn - I think that the peak temperature date for most of USA and Canada falls in July and August - while your correct on the peak position, the peak btu's delivered by the sun  are higher in July and August.  Don't know why, just know the numbers.",113431,2019-10-19 21:40:49,PC Jimmmy
"@pcjimmmy ,
Yes, you are correct about the temperature,
but in these months the length of a sunny day is less than in June.
Difference for July 0.5-1 hours,
For August 1-1,5 hours.
So in August every day people should start to use electricity earlier at evening than in June, for example.",113431,2019-10-19 22:09:54,Kostiantyn Isaienkov
"So are all the buildings at North hemisphere ? Cause if any of them isat South hemisphere , for that building it would be 21/22 December . Though i do think daytime length can be valuable here. ",113431,2019-10-19 13:47:58,Khairul Islam
"Hi, @khairulislam,
Probably data is for US.
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113286",113431,2019-10-19 13:53:12,Kostiantyn Isaienkov
Interesting! What was the dominant technique used in the 1993 competition? SVMs or were people already experimenting with ensembling of decision tree models? Or was it something else completely?,113703,2019-11-01 20:05:28,Carlo Lepelaars
"I'm not familiar with the details of the outcome, but I would expect that the resulting papers (posted to https://www.kaggle.com/c/great-energy-predictor-shootout-i/data) have more information.",113703,2019-11-01 20:14:48,Sohier Dane
"1393(building id) probably belongs to 15(site id), and you might see that this category has common missing(?) periods between feb/apr. It seems like someone artificially interpolated the gap by a line(including air temperature and dew temperature). Moreover, there are significant losses in the other features also I think. We have to deal with this.",114299,2019-10-25 17:52:29,Hanjoon Choe
"Well, yes. But will there be a gap in 2017? There is absolutely nothing - apart from LB probing - that can answer that.",114299,2019-10-25 18:02:08,Ants
"Yes, LB probing will be the best way for validation, but 2 subs a day really su…",114299,2019-10-25 18:07:25,Michael Jahrer
"You can only assume that the label is highly related to weather data, otherwise, it is quite hard! ",114299,2019-11-04 13:47:49,LongYin/杰少
"very good points, I also have done some experiments and now begin to think .. about good validation.
First I used last 20% as hold out set and got surprisingly a good single model lb 1.20, then I tuned something, validation got better (1.10 -> 1.06) but lb went up to 1.26. In these experiments I retrained on all data for test prediction.
Best kernels use ""stratify data by buildingid"", does this mean split train/valid by buildingid in each fold? I tried this and got horrible validation error of 1.59. This means that I predict unknown building_id's, which don't make sense to me.
Random subset results in very low validation error of 0.5 or so.
So far I don't find a proper validation schema that simulates the train/test split. To capture drift over a year I need at least the whole year in train.
If we would have 2 years of training data I can probably safe use first year for train, second for valid (as you said).",114299,2019-10-25 17:16:13,Michael Jahrer
Stratification with building id means the distribution of buildings in train and val is same. ,114299,2019-10-25 18:24:56,Suchith 
"I'm doing an EDA notebook to study the gaps in the train set. Currently, it is WIP. I want to get some conclusions in the next days.
The gaps WILL affect the model, because the lack of data. The lacking data SHOULD be interpolated to minimize the error of the model with the reality. Quantifying the affectation is a point to consider.
The buildings have, normally, a daily pattern (if the day is a working day, weekend or a bank holiday), a weekly pattern (every day has some nuances, for example, Fridays), and a monthly pattern (cold months has a different pattern than warm months). If you match the dayofthe_year from train to set it could be a good start point. If you align to match also the day of the week, you will have a better model.",114581,2019-10-28 00:36:47,Juanma Hernández
Thanks a lot. I now have some direction on what to do with the gaps.,114581,2019-10-28 12:34:44,Ayush Goel
"I think site 11,7 are ottawa in canada",114967,2019-10-31 04:18:31,Isamu Yamashita
Interesting. Why Ottowa and not Montreal?,114967,2019-10-31 14:48:23,Silverback
"Or Montreal🤔
I can get monthly average temperature of Ottawa in 2016-2017 from this site
and  Montreal's from this Kaggle Dataset 
",114967,2019-11-03 11:36:25,Isamu Yamashita
It's very cold in site 11. Minimum temperatures become minus 20 in February. More likely Canada as SD said https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114483,114967,2019-10-30 13:07:26,🐢 Jun Koda
"Excellent, thanks for the link — I really do mean it — I have barely looked yet :) The crowd-sourced data detective machine has been busy - I will get reading!",114967,2019-10-30 13:19:35,James Trotman
It's amazing if you realised is something Spanish and it's true 😆,114967,2019-10-30 14:01:42,🐢 Jun Koda
Lots of people do speak Spanish in Montreal!,114967,2019-10-30 13:00:30,S D
"Montreal looks a good fit! It is actually a coincidence then, not just my lame attempt at a joke 😛 ",114967,2019-10-30 13:19:55,James Trotman
"Maybe you can find a way to ""flag"" other sites!",114967,2019-10-30 13:26:38,S D
"Good tip! A Google Images search for:

Comes up with not only ""Electric Blue"" (wrong actually blue=steam in the kernel) but ""Brooklyn"" too:

I'd better get cracking - this is going to take weeks 😂 ",114967,2019-10-30 13:45:48,James Trotman
Thanks for sharing. Beautiful,114967,2019-10-31 03:27:33,LongYin/杰少
That's some great visualization. Thanks for sharing. ,114967,2019-10-30 16:35:07,Manraj Singh
Good catch @themonologue. There were around 2.5 Lakh out of 41 million records which had less than 0 as meter reading. Thanks. ,115547,2019-11-04 04:33:13,Manoj Prabhakar
@manojprabhaakr Pleasure,115547,2019-11-04 16:39:16,Manraj Singh
"That makes sense, if to think about meter not being able to generate negative values. But I have checked couple times, and it didn't show any improvement. Probably it is just less than 0.01.",115547,2019-11-03 18:16:16,Mykola Maliarenko
@grebublin  I guess. My score went up from 1.12 to 1.11. Maybe mine was something around early 1.12. ,115547,2019-11-03 18:19:08,Manraj Singh
"Hi,
How did you solve this in the end ? ",115940,2019-11-19 05:59:01,Alice Gabriel
"Very nice - easy to look at - great job.
However, it does make me want to quit this competition and move on to a set of data that's less full of holes :)",119175,2019-11-27 03:01:24,PC Jimmmy
Thanks PC! Yes I agree the data is not ideal.,119175,2019-11-27 03:20:32,pspenano
"Thank you, but reading discussions helps sometimes ;)
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115248",119175,2019-11-27 12:18:50,Roman
"
Look at these
And why the mean meter_reading is always going down for prediction ?????",122263,2019-12-19 17:01:04,Pratik Poudel
"There's one more notice. Meter 3, building_id 1241 is filled not with zeros. It has left part the same as building_id 1253, but the right part of 1241 is filled with very low numbers. Seems like they're taken from the other building_id.",122263,2019-12-19 04:45:02,Artyom Vorobyov
really nice finding! but it is too late for me to use.,122263,2019-12-18 23:07:14,eagle4
I don't understand why so many teams are removed，can anyone tell me what happen to them,122415,2019-12-20 02:23:15,Wu_Yiqun
@decarmelo  I think people made several accounts to test more submissions per day. That's crazy. Never seen that. ,122415,2019-12-20 02:31:01,Mukharbek Organokov
That is so crazy，can't imagine 1800 of 3600 are smurf🤕 ,122415,2019-12-20 02:44:13,Wu_Yiqun
Don't fret! The leaderboard will look a little odd during our closing processes. See this post for more: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122296#699019,122415,2019-12-20 03:59:11,Addison Howard
"Almost half got removed but, strangely there is no/not much change in Public Leaderboard positions below 200 ranks.",122415,2019-12-20 02:43:34,Kranthi Kumar
"To be honest, I have checked in the last few minutes before competition ends and I didn't see any new kernel before the competition ends, including Manraj Singh kernel. 
Otherwise I would have complain too.
I'm not taking any sides here with anyone, just want to speak out the truth here.",122446,2019-12-20 12:39:25,Mr Loke
@klloke thanks for letting to know 👍,122446,2019-12-20 12:42:14,Mukharbek Organokov
@klloke Thank you!,122446,2019-12-20 15:28:46,Manraj Singh
"I just wanted to make new post with apologies but if you have already posted, I will take advantage here.
My personal apologies to you Manraj. @themonologue
I was told by someone that you have done it before and like the kernel was public 20 min before the end, thus I made the post to clarify that. As now, that person I think not sure but he made this misconception. 
And thus I would like to apologize for the situation. 
Sorry again. We will be more rigorous next time. 
And beat wishes with the competition. ",122446,2019-12-20 08:50:27,Mukharbek Organokov
"@muhakabartay Thank you for your reply. I would and I guess all the kagglers here would really like to know that ""someone"", cause this is a big deal if ""someone"" says I have done it before too as this is just my second competition I'm actually working, I don't remembering sharing any kernels before. 
Also you are now saying that ""someone"" told you that the kernel was posted 20min ago, but in your discussion you mentioned few hours ago.
Please clarify the ""someone"" here. Cause you were deliberately posting in others discussions too about ""public sharing instead of private sharing"" 
Thanks in advance. ",122446,2019-12-20 09:26:54,Manraj Singh
"@themonologue
I said not someone told me 20 min ago but that kernel was made public 20 min before the end. It's completely different phrases.
Remarking the public/private. That was in another topic, not concern you and I don't need to give any comments about it since it didn't refer to anyone special, but would say that it was just a joke seeing how many new empty accounts were created and submission are made, hopefully, Kaggle deleted that. ",122446,2019-12-20 09:32:09,Mukharbek Organokov
"@muhakabartay  I'm telling you the same, ""someone"" told you that the kernel was published 20min before competition ended but you wrote in your discussion that I (you mentioned my kernel) posted it few hours before the competition.
Anyway, I would like to end this since you have apologized, though I'm not convinced.
Good luck to you too for the competition. ",122446,2019-12-20 09:35:46,Manraj Singh
"@themonologue ""few hours"" I said because seen 6h In kernel and summarizing with 0.3h I was told.
As you see, really it's better to wait 3-6h after the competition end and post smth later on, this will keep people not be confusing because really such confusion  things happen. ",122446,2019-12-20 09:39:39,Mukharbek Organokov
"@muhakabartay Sure. But atleast not posting publicly and demeaning just because of confusing. Will keep in mind next time, agreed to the point that people might get confused who actually didn't check at the last moment. The only thing I wanted to clarify is I didn't post it before the competition ended. ",122446,2019-12-20 09:43:21,Manraj Singh
"@themonologue yeah, you are right. I support that. But as you see, not all the time I am so patient))",122446,2019-12-20 09:46:27,Mukharbek Organokov
"Hello Manraj, let me shed some light here. 
At first i want to add to Mukharbek apologies on what has happened was certainly not required, (he got carried away after what happened in last few hrs/ days that resulted for his de-mean language used in this context), but I saw the kernel posted roughly around 6:45 -6:50 pm (7 pm the competition ends my time). But as you mentioned that you posted the kernel after competition, I totally take your statements as true statements
We were actively discussing our progress during this time. We had already lost around 25 positions in that last 1 hr. And roughly after 15 minutes (around 7:10) our positions even dropped further 5 places down.
Since both of these events are really close, this was really an outburst of reactions which wont have happened otherwise. Kaggle can be sometimes really stressing. Mukharbek is a great person to work with, and as a team leader , I (on his behalf for his outburst) would like to sincerely apologize to you. ",122446,2019-12-20 10:45:06,Ashish Gupta
"Thanks for your reply Ashish, @roydatascience 
I agree there has been some confusion, I very well remember posting my kernel 5mins after the competition ended (5:30 UTC). Appreciate your reply, also I'm on the same boat of frustration of losing positions.   ",122446,2019-12-20 10:50:25,Manraj Singh
"I can really understand your stand point, now that The leaderboard is calculated with approximately 89% of the test data. The final results will be based on the other 11%, so the final standings may be different
Good luck to you and your team",122446,2019-12-20 10:58:08,Ashish Gupta
"Chill guys!
",122446,2019-12-20 10:36:24,Kiran Kunapuli
Yeah! We were just clarifying stuff! xD ,122446,2019-12-20 10:43:49,Manraj Singh
"You guys are getting scored?
",122562,2019-12-22 10:40:00,Kamal Chhirang
this one's gold :) ,122562,2019-12-23 06:27:37,Firat Gonen
🙏 .,122562,2019-12-23 02:50:46,PurpleCode_073
"just 1 , the very first (the worst) one",122562,2019-12-22 21:06:33,Firat Gonen
,122562,2019-12-22 21:26:03,Oussa
Noob question: why is each and every submission scored since we are only selecting two submissions to be scored?,122562,2019-12-22 09:54:12,Georgios Sarantitis
So that you can regret not selecting the good submissions. Jk,122562,2019-12-22 10:40:47,Kamal Chhirang
And I will do! I forgot to select the right ones 🤒 ,122562,2019-12-22 11:50:35,Oussa
…right..thanks!,122562,2019-12-22 13:51:45,Georgios Sarantitis
"Hey guys,
Please check: https://www.kaggle.com/c/9994/publicleaderboarddata.zip
You can get the latest update for public score calculation in a record file (.csv)
Unfortunately, according to the record rows over total submissions (847 / 12409 = 6.8%), it seems the calculation goes really slow😐 
Be patient!",122562,2019-12-21 03:33:49,patrick0302
I am worried that the finalization will be completed within two days.,122562,2019-12-21 02:21:47,Yoshi
"3 of 41  … yes, it's my fault it's all taking so long. ",122562,2019-12-21 02:07:12,Bruce Young
it will take about 13 days…,122562,2019-12-21 03:21:38,Yoshi
"The backup system passed 847 scored a while ago, and hasn't been running for very long yet. At the moment it looks like we're back on track. 
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122296#699887",122562,2019-12-21 04:08:33,Sohier Dane
"All my submissions have not been scored yet.It's really hard to wait for the final result.
By the way.Isn't the result on the private LB only base on the two submissions we choose?Why I can see some entries greater than two?",122562,2019-12-21 03:04:59,Wu_Yiqun
where can I find out my submissions have beed finished rescore?,122562,2019-12-21 02:51:57,Dean
"'Public Score' clumns in 'My Submissions'.
finished score : Numeric value
NOT finished : The circle is spinning…",122562,2019-12-21 03:06:26,Yust
"wow!
Thanks for your explanation.
but my submissions are 0/64 xd.",122562,2019-12-21 03:39:49,Dean
Mine is all pending. Looks like it will take longer than I thought. Hopefully before Christmas we can know the results.,122562,2019-12-21 05:53:15,Mr Loke
"4 out of 97 submissions. If it goes like this, I am expecting the results would complete by mid January for all our submissions…",122562,2019-12-21 03:43:37,Manoj Prabhakar
In total we have 42 submissions and so far 42 spinning circles…  🙁🙁🙁 ,122562,2019-12-24 06:04:40,Stanislav Lykov
This Comment was deleted.,122562,2019-12-24 05:22:09,No user
This Comment was deleted.,122562,2019-12-22 14:06:57,No user
"I agree with you but where is it mentioned that the rescoring is done to select best two submissions?
I think the rescoring of all submissions is done only for the new private LB for display and analysis and the best two submissions will be based on the original public LB itself.",122584,2019-12-21 07:37:07,Vopani
Hi Vopani - I do hope like you said it's scored based on the original public LB. But after reading some forum posts I feel like some reminder to the host and other participants may help.,122584,2019-12-21 07:51:14,Shiu-Tang Li
"This is about modeling a counterfactual, not the future, so weather data really is available for these sorts of problems.",112881,2019-10-16 15:31:16,Sohier Dane
"Thanks, that helps. I guess it makes sense (although the title is maybe misleading :) ) . 
So a practical scenario example would be:
1) building gets created  in 2016
3) Calibrate model with kW & weather data from 2016-2018 
4) Upgrade building in 2019 and measure its kW 
5) Run Model from 3) with weather data from 2019
6) Perform A/B testing of kW usages between steps 4 and 5 ",112881,2019-10-16 22:32:35,Ptolemy
"Just a guess on my part but I think the buildings only in North America and likely USA only.  Go to the DOE web site for USA and it would seem that the data is result of survey's conducted in US.
So I will be using USA holidays myself.",113032,2019-10-18 04:47:20,PC Jimmmy
"Thanks, @pcjimmmy. I'll check that site out, but the reason I was confused is because the ""Data Description"" section for this competition says, ""The dataset includes three years of hourly meter readings from over one thousand buildings at several different sites around the world."" (bolding is mine)",113032,2019-10-20 21:28:38,MDA Turse
"Yep I saw what they said - plot out the temperatures on one year scale.  The shape of the curve is very good match to data I have worked with several times over the past decade for USA / Canada.  Must admit however, that I only have looked at the summary plot and not done the plot by the 15 sites.  Will add that to my todo list.   ",113032,2019-10-20 22:17:37,PC Jimmmy
"I don't think anyone except the organizers knows the answer to this. Which means, sans them telling us, one submission trained w/o outliers / using your imputation scheme, and one sub biased with the messy data. Then pray for the best.",113181,2019-10-17 23:22:46,authman
"In my view, it should be one of the basic things to clarify. But in the case they don't, I guess we'll go your way somehow…",113181,2019-10-18 07:22:30,Georgios Sarantitis
"This is not specific to this competition. Here are the materials for Kaggle days ""Competitive GBDT Specification and Optimization Workshop "":

Materials
Youtube

I found this thread from Santander Competition very helpful as well 

How do we optimize GBM hyperparameters?
",113387,2019-10-19 12:08:52,arnab
Thanks you,113387,2019-10-21 18:01:39,Tarek Hamdi
Debugging? What is that 👀 ,113536,2019-10-20 11:52:28,Ivan de los Santos
,113536,2019-10-20 09:13:06,Khairul Islam
Thanks. Those are good reads.,113642,2019-11-20 14:20:15,Buffalo Spdwy
@barnwellguy You are welcome,113642,2019-11-20 15:14:34,Tarek Hamdi
"To streamline this though process it is useful to know the 3 categories in which missing data can be classified into:
Missing Completely at Random (MCAR)
Missing at Random (MAR)
Missing Not at Random (MNAR)
deeper into each one of them and look for potential treatments that can help us circumvent the problems that missing data creates. 
Some ideas:
Time series imputation
     - Non-time-series specific method

mean imputation
median - imputation
mode - imputation
calcucate the appropriate measure and replace NAs with the values. Appropriate for stationary time series, for example, white noise data
Random sample imputation
replace missing values with observations randomly selected from the remaining (either of it or just some section of it)

Time-Series specific method

Last observation carried forward (LOCF)
Next observation carried backward (NOCB)

This article can be helpfull: https://www.linkedin.com/pulse/data-cleaning-guide-dealing-na-values-karan-rajwanshi/",113795,2019-10-22 22:14:39,CaesarLupum
"Hi there could be a problem with the DAQ system where all values are dropping to zero.  ""he peak must be consumed electricity values at missing value time"" if this statement is correct what about other peaks that i can see in the signal. we can cut the blank section and use ramp/half sine/ linear overlap/ sine overlap  function to join time series data.",113795,2019-10-22 12:03:20,AP Jahagirdar
"Thank you for plite comment. 
I reconsidered and
Meter_values are lost their time series property 
This data has time series property, but predict targets are not time series prediction but just regression using weather data. If we interporate with some values, the values must have include weather information. For predicting next year meter values, it is better way to use last year data, but it is not true that next year meter values is the same with last year.  So I think that drop missing values is better way. ",113795,2019-10-24 12:21:15,Takumi Ban
"Look at the person at the top , its been 16 days and he has only made 9 submissions to reach this point.  It is not about how many submissions you make, it is about how many of them are effective. As Mr Yakovlev said, i agree with him. With more submission you are more likely to overfit on public LB, which would be  risky for you. ",115113,2019-10-31 15:04:57,Khairul Islam
Perfect! This makes sense. Thanks,115113,2019-10-31 15:36:12,Kaushal Shah
"Humm… Sorry Fabien Daniel lost his 1st place rigth now…
Hope he will be back soon at TOP, and I'am sure he can.",115113,2019-10-31 21:16:58,mezoganet
"I think it's a very good decision from Kaggle side:

Less LB probing
No more 5x teams with 300+ submissions in total
Better CV planning
More interesting solutions
Better overall planning
",115113,2019-10-31 13:47:22,Konstantin Yakovlev
"On the learning side, sufficient is a vague term: should 1 submission daily be considered sufficient? Should 50 submissions daily be considered sufficient? How can this be determined? 
From the competition side, we can be sure though: since we all get equal chances (no matter 2 or 5 or 100) the game is fair. ",115113,2019-10-31 14:09:05,Georgios Sarantitis
"Good thing is that we have several ongoing competition and each of us can find what serves the goal better. 
Think of it as practice challenge - lesson here is to manage better resources you have and find best CV option to survive with 2 submissions per day. ",115113,2019-10-31 14:14:10,Konstantin Yakovlev
"@Konstantin Yakovlev
You are  really brillant for sure and you can surf on 100 competitions at the same time. But not a lot of people can do that .
You are very welcome anyway, and I thank you so much for your help in this competition… Really !",115113,2019-10-31 21:22:57,mezoganet
@kyakovlev Completely agree with you.  Well explained. ,115113,2019-11-02 14:15:45,Manraj Singh
"Once again I just agree !
Two submissions, 2 decimal places, too much garbage in Data …
I can't do more than applause!",115113,2019-10-31 20:26:07,mezoganet
"I can understand why you don't like it, but there is no obligatory participation. If you don't like then just move to other one.
Each competition created for some reason that was set up by organisers. Good or bad goal, good or bad results - they will make analyses after competition ends and evolve. We are here to support them in good faith. ",115113,2019-10-31 20:32:17,Konstantin Yakovlev
"I don't know what you mean … ""there is no obligatory participation. ""
I am here to compete , like everybody here, even if I don't like this competition so much.
Don't blame us please !",115113,2019-10-31 21:13:54,mezoganet
"""no obligatory participation"" - nobody force you to join this competition. If you don't like - don't enter, just find competition that you like. 

In my opinion you are showing unnecessary aggression with your ""not fair""/""Don't like"" comments.  ",115113,2019-10-31 21:18:14,Konstantin Yakovlev
"""no obligatory participation""
This I already known, for a long time !",115113,2019-10-31 21:24:56,mezoganet
"Yew, who is aggressive ? * ""not fair""/""Don't like"" comments.*
Please let me be who I am.
And do not mix agressivity (I aint agressive) and upsetting (frustrated yes I am), for a better understanding of everyone of us.
Do not spend time on this thread please, it would be more quiet then here…
I am elder than you are, so it is my role to calm it down… that's what I (try to) do.
You are still welcome, when words are words of friendship. 
I am so sad tonite… Not her for fighting, my really friend. ",115113,2019-10-31 22:06:42,mezoganet
"Are you aware that timestamps for weather data and meter data are probably in different time zones (GMT vs local)? It's been discussed a lot and people have guessed out the local timezones of all the sites.
Edit: I'm not sure why you are bringing up GMT+5. Maybe your timezone conversion logic is backwards? 2000 GMT is 1500 local time zone.",115852,2019-11-06 15:00:16,S D
"Oddly, the timezone which is 5 hours behind GMT is called ''Etc/GMT+5"", allegedly to match POSIX convention. See description from Wikipedia:
The special area of ""Etc"" is used for some administrative zones, particularly for ""Etc/UTC"" which represents Coordinated Universal Time. In order to conform with the POSIX style, those zone names beginning with ""Etc/GMT"" have their sign reversed from the standard ISO 8601 convention. In the ""Etc"" area, zones west of GMT have a positive sign and those east have a negative sign in their name (e.g ""Etc/GMT-14"" is 14 hours ahead of GMT).",115852,2019-11-06 16:52:55,Poe Dator
"I see, thank you.",115852,2019-11-06 17:21:19,S D
what i am trying to convey is time data for site 14 buildings in train.csv is in GMT rather than in local time,115852,2019-11-11 07:09:07,Prashant Chaudhari
"I see your point. Usually, energy consumption is small in 0 - 5 am, but it's small in 5 - 10 am on site 14. Site 8 and 10 in the figure are also different in that sense.",115852,2019-11-11 08:41:21,🐢 Jun Koda
Site 10 is normal if I average log1p(meter_reading). Site 8 is extremely flat (hr independent).,115852,2019-11-11 09:23:14,🐢 Jun Koda
"Based on building data I identified Site 14 as Charlottesville, VA (https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115698). So it is definitely in US/Eastern time zone. 
Do you see such meter readings picture in all buildings in Site 14?",115852,2019-11-06 07:28:06,Poe Dator
"In some buildings seems that there is a direct relation between electricity and chilled water:

In my notebook the correlation between the different energy aspects is analysed.",116748,2019-11-12 21:33:18,Juanma Hernández
"This is cool stuff. Your work brings in lots of insights. Thanks for the same.
 I guess the competition organisers will have to say a word on two on this thought process. As the cause and effect can only be confirmed by them. 
If chilled water and steam correlates with electricity and we treat the other two as attributes and only electricity as dependent those rows on which electricity is to be metered should influence it.
However considering Larry's thought process all three may rise together due to external factors outside the building which may not be one influencing the other. 
Another thought is if they are not influencing each other then if one is rising the other should be falling. which is not happening.
Cause and effect is very important",116748,2019-11-13 03:49:30,manu
"Exactly.  Let's look at two possibilities; the buildings are stand-alone or they're part of a district energy system that supplies them chilled water, steam/hot water, and electricity from some centralized plant (many campuses, cities, and large industrial users are supplied this way).  If the building is stand-alone, the most likely equipment scenario is cooling with an electric chiller; heating with a steam or hot water boiler, most likely natural gas-fired; and electricity supplied from the local utility.   In the heating season, electricity should be base-loaded (for lighting, receptacle loads, etc.) and steam/hot water will be from the boiler (which could be electric. High coincident electricity use could indicate that).  If the building is fed from a district energy system, then the chilled water energy will be metered and would be approximately the same as a similar stand-alone building, but the electric use will not show up since it's consumed at the district energy plant.  Steam, if supplied from electric central plant boilers would be similar. Same building, entirely different energy profiles.  That's why I think that any attempt to generalize energy use on so many undifferentiated buildings is a total waste of time unless more useful significant features are added.  The leaked stuff doesn't add that kind of information, BTW.",116748,2019-11-13 04:24:39,Larry Schuster
"You're making an assumption that the source energy is what's metered.  Steam and hot water, no matter what fuel is used, will be metered as the product, i.e. hot water/steam million BTU/hr or KWh.  You can't make any assumptions about what fuel source is used.  Steam or hot water can be produced by waste heat, electricity, natural gas, fuel oil (diesel), wood waste, etc, but the dataset doesn't reveal that.  In the worst case, an electric hot water boiler could be used, and the energy would be counted twice, once as electricity and once as hot water.",116748,2019-11-11 18:18:05,Larry Schuster
"You're going wrong somewhere.  Although the four energy PRODUCTS are not mutually independent, you can't assume they're all produced from the same energy SOURCES.  They can be produced from natural gas, fuel oil, coal, etc.  Your observation that they interact is correct, however.",116748,2019-11-11 12:40:58,Larry Schuster
Thats possible. But not sure anyone will allow producing hot waster through fuel oil or coal.  and metering fuel oil and  coal seems far fetched. Although Gas is a possibility. Just a hypothetical thought. Not sure it will work in this problem as the test data consists of prediction of all 4 meters,116748,2019-11-11 16:50:33,manu
"@abednadir, thanks for sharing. 
I took a look at your script and I could not find ""timestamp"" defined anywhere. Before the following lines.

train$d_week = weekdays(as.Date(train$timestamp))
test$d_week = weekdays(as.Date(test$timestamp))
",117773,2019-11-18 19:36:54,YaGana Sheriff-Hussaini
"@abednadir  We can team up but we will need to stop making submissions , let me know your thoughts ",117773,2019-11-18 14:48:29,AdityaVikramSingh
"I don't know R, would you care to mention briefly which features you are using and whether this is simple Linear Regression or a constrained version?",117773,2019-11-18 12:43:02,Larry D.
"WOOOOOW!!
Thanks for sharing! The quintessence of a linear regression👍 ",117773,2019-11-18 00:57:57,Kangyu Chen
"Hi, @abednadir , 
how many submissions do you have?",117773,2019-11-17 19:41:13,Kostiantyn Isaienkov
51 😳 ,117773,2019-11-17 19:44:51,SteveKane
"I've manage to score 1.07 (public LB) with per-building models. More precisely, and this might be part of your problem, I used per-building and per-meter (2380 models instead of 1449). I used GBDTs, and did use different hyperparameters than I would use for a single global model.",120810,2019-12-12 06:17:24,Robert Stockton
"Wow, congrats! That will bring some hope for me to keep trying. I also tried both per bldg and per meter models with no success. The problem seems to be overfitting.",120810,2019-12-12 17:09:30,Fernando Wittmann
"If you are doing a linear model with a lot of features and no regularization, chances are you are overfitting a lot and may have very large coefficients which will result in crazy predictions on the test set. You should try plotting your predictions to get a feel of what is going on. They should at least bear some resemblance to the data from 2016.",120810,2019-12-11 08:57:09,Larry D.
Are you using the same model as you would use when training on all the data? Then the model may have some serious overfitting because you are training on less data. Adding more regularization will probably improve the score.,120810,2019-12-09 17:39:14,Carlo Lepelaars
"the problem is that, a single building does not have enough of observations to gerenalize well! For example, there are buildings which don't have some of the observations for some intervals, however, these interval exist in the test set.",120810,2019-12-09 08:55:40,BearInMind
"Maybe you are overfitting. Try simpler models,  I managed 1.15 with one linear model per building, using Fourier terms to model daily/weekly/yearly seasonality. ",120810,2019-12-10 01:59:09,Davide
"Thanks for the hint @davide0burba ! If you don't mind answering, how did you extract Fourier terms from the training set? To my knowledge, that's only possible in signals (which is the target in your case). ",120810,2019-12-11 21:38:32,Fernando Wittmann
"No problem answering :)
Maybe I was a bit misleading by saying Fourier terms, it is more correct to say Fourier components. The corresponding coefficients are fitted through the linear model.
Since a partial Fourier sum can approximate an arbitrary periodic signal, I introduced the components as features to model daily/weekly/yearly seasonality. This is made extremely easy by using the fbprophet package.
My code looks something like that:
import pandas as pd
from fbprophet import Prophet

def build_fourier_components(dates,daily_order,weekly_order,yearly_order):

    l = []
    if daily_order > 0:
        daily_fourier  = pd.DataFrame(Prophet.fourier_series(dates,1,daily_order))
        daily_fourier.columns  = [""daily_fourier_""+ str(i) for i in daily_fourier.columns]
        l.append(daily_fourier)

    if weekly_order > 0:
        weekly_fourier = pd.DataFrame(Prophet.fourier_series(dates,7,weekly_order))
        weekly_fourier.columns = [""weekly_fourier_""+ str(i) for i in weekly_fourier.columns]
        l.append(weekly_fourier)

    if yearly_order > 0:
        yearly_fourier = pd.DataFrame(Prophet.fourier_series(dates,365,yearly_order))
        yearly_fourier.columns = [""yearly_fourier_""+ str(i) for i in yearly_fourier.columns]
        l.append(yearly_fourier)

    fourier_df = pd.concat(l,axis = 1)
    return fourier_df


fourier_features = build_fourier_components(train.timestamp,3,3,3)

This technique seems to work well for most of the time series..

But sometimes it probably overfit :')
",120810,2019-12-12 03:11:27,Davide
You're doing something wrong. For me it works good.,120810,2019-12-09 09:18:33,Oleg Knaub
Thanks for the comment @oleg90777 ! Did you use NNs?,120810,2019-12-11 21:40:05,Fernando Wittmann
At the moment only lgb,120810,2019-12-12 07:27:47,Oleg Knaub
Does sklearn allow train several models per type? And automatically apply them during prediction? Or I should manually create and evaluate each model?,120810,2019-12-18 11:04:06,Nazar
"Thanks for the comments guys! Good to know that there are some competitors having success on that road. My last failed attempt (quite frustrating because it took me some time) was to first create a generic NN trained on all buildings (LB of 1.12). Later, I used transfer learning to fine-tune the very same model for each building. But for some reason, the results are getting worse. Even when using higher dropout rates, model checkpoint and freezing the top layers.",120810,2019-12-11 21:32:30,Fernando Wittmann
I am getting scores as high as 6.77 with lgbm trained on single building with cv of 3 to 12.On whole i am getting local score of .75 and lb score of 1.43,120810,2019-12-09 03:12:07,Pratik Poudel
This Comment was deleted.,120810,2019-12-14 20:35:15,No user
"One of my model without including leak data in training has lb 1.062, when I trained using leak data, lb improved to 1.056. It can be possible because I trained using leak data and then make prediction for the same. 
One way i could have tested it by setting meter readings in all the leak data in test set To zero and then compare the results of model trained with or without leak data, unfortunately don’t have submissions now.",121261,2019-12-19 06:49:09,HarshitMehta
The real question is does it improve other sites without leak?,121261,2019-12-19 06:59:22,Gunes Evitan
"We’ll know that tonight ;-)
I honestly hope for you and Fred that you will stay Gold after the « Shake up ».",121261,2019-12-19 07:20:22,mezoganet
"I tested it, it improves.",121261,2019-12-19 11:48:52,Kamal Chhirang
It is not directly useful but you can use leaked data to increase the training model size. ,121261,2019-12-12 06:06:09,Kamal Chhirang
"I see, but that may cause overfitting I suppose?",121261,2019-12-12 06:43:02,Thiago Preischadt
it didnt make my LB score better ,121261,2019-12-12 12:18:33,Mohamad Falah
有泄露的数据吗 ,121261,2019-12-17 08:48:30,zhengchenglong
Deleted…,121261,2019-12-19 07:18:48,mezoganet
That eases my mind,121261,2019-12-12 16:14:41,Rain-S
"the same question, is there a risk of cancelling my score if I use these leaky data?",121261,2019-12-12 13:59:12,Rain-S
I don't think so honestly,121261,2019-12-12 15:51:25,Thiago Preischadt
"No, they just won't score the rows with leaked data.",121261,2019-12-12 16:19:52,Kamal Chhirang
And how « they » should know your submission comes from leaked data ?,121261,2019-12-12 17:56:50,mezoganet
"I mean, they will remove the rows whose meter reading is leaked publically before calculating the private leaderboard score.",121261,2019-12-12 19:23:43,Kamal Chhirang
"This was not my comment : how will « they » find your submission comes from leaked data ? Imagine your guesses were right, imagine you’ve been creative, imagine you found the right algorithm… Who can prove that you cheated ? 
And moreover, if you used leaked data, Kaggle is responsible for that, and this is not the first time they do such a mistake, not the the first time and not the last. It happened so many times in the past.",121261,2019-12-12 19:30:24,mezoganet
"You can use leaked data for training the model + submitting it. No problem. Kaggle will just remove the rows whose meter reading was leaked publically (like site-0,site-1,site-2,site-3,site-4,site15 I think). 
If you find a leaked data on the internet, but do not publish it on Kaggle. No one will know, and you will probably get a prize. ",121261,2019-12-13 05:44:53,Kamal Chhirang
"And you will eventually get disqualified ;)

If you find a leaked data on the internet, but do not publish it on Kaggle. No one will know, and you will probably get a prize.

That's just stupid, sorry.",121261,2019-12-17 16:15:51,danzel
"It is not a kernel only competition, so if someone finds one, and do not publish it, there is a very good chance, he/she can get away with it. Right?",121261,2019-12-17 16:45:37,Kamal Chhirang
"Good luck to everyone 
And expect write-up tomorrow :)",122329,2019-12-19 15:46:06,Dean
Good luck to you @ebudia .. ,122329,2019-12-19 15:12:51,Manoj Prabhakar
"No matter what's the result, finally it's ending …
Good luck to you !",122329,2019-12-19 10:56:03,Jie Lu
I put together a summary. Just waiting to see what happens.,122695,2019-12-22 09:39:55,Tim Yee
I have written my solution. Hope that I can get a good result to post my write-up.,122695,2019-12-22 15:57:14,Dean
"For the purposes of setting up a competition, when your response variable is non-negative and highly skewed, you should use a Gini metric.  There is a reason why insurance competitions typically use a Gini:  it's the only practical metric that will not reward your model for giving you obviously stupid predictions in such a situation.",113270,2019-10-18 15:35:58,Dmitriy Guller
Thanks! I will find my way,113270,2019-10-18 15:53:28,Neo Zhao
"MAE in this case is good and still think most of the people have already correct answers.
Just cannot make it smaller because of incomplete data.
up",113270,2019-10-18 15:10:46,Adrian Zinovei
"you are right, thanks!",113270,2019-10-18 15:52:37,Neo Zhao
"@laplaceplanet  For this competition, I would recommend using a RMSLE loss function as it is the evaluation metric.
RMSE or MAE or MSE. Which is better?
I think that this can be narrowed down to a comparison between RMSE and MAE.
RMSE penalises larger errors compared to MAE. 
Ultimately, the choice of metric is highly dependant on the problem you are apply the model to.",113270,2019-10-18 08:09:30,Daniel
Thanks,113270,2019-10-18 15:52:12,Neo Zhao
@cyberia This is very insightful. Thanks for sharing good job! :),113856,2019-10-22 18:42:15,Manraj Singh
"Nice!
how's your dendrogram generated?",113856,2019-10-23 09:09:01,Kim Wilson
I am using pandas_profiler to generate this entire report including dendogram.,113856,2019-11-02 20:11:57,Aman.cyberia
"Hi, Did you use this strategy in the end ? Did it help you ? ",114458,2019-11-22 03:31:42,Alice Gabriel
I managed to get 1.11 with linear models. 1.14 without penalized regression. It only takes like 3 minutes too,115279,2019-11-02 04:50:28,SteveKane
I was thinking about starting a discussion on how far people have gotten with lr models so I could compare mine. I guess I've got work cutout.,115279,2019-11-02 07:07:56,Tim Yee
"Steve, dude, that's sick! what a cool idea.",115279,2019-11-07 23:29:52,Louka Ewington-Pitsos
LGBM per site scores very similar to LGBM on whole data (my best model scores 1.10 with both approaches). So it may not be particularly better but can add value to ensembles. I've shared a kernel that scores 1.15: https://www.kaggle.com/rohanrao/ashrae-divide-and-conquer,115279,2019-11-09 07:26:27,Vopani
That's because building_id is the best feature in terms of both split and gain in both models. ,115279,2019-11-09 07:40:05,Gunes Evitan
"Going to the root of the problem, you are trying to guess a meter time series.
There are 2380 meters measuring energy. There are 1449 buildings, and 15 sites. And 16 types of buildings and 4 types of meters.
You can consider the following options:

only one model. The type of meter, the type of building, the building ID are features. A random forest, for example, will take them in consideration, and have mini-models in the branches of the trees.
4 models. One for each meter type. The energy measured for each meter type has its own behaviour. Hot water, for example is very season dependent.
15 models. One for each site. This doesn't have a lot of sense for me. Is the energy consumption different in US than Canada?
16 models. One for each building type. Each building type has a characteristic energy consumption.
1449 models. This doesn't have a lot of sense for me. The energy consumption between meters have nothing in common.
2380 models. One for each meter. As you are guessing the consumption for the next two years of the very same buildings, it's like 2380 different datasets. Every building and every meter have its characteristic behaviour.

People, which option is what you're doing? ",115279,2019-11-03 04:11:21,Juanma Hernández
I am trying one model. I thinking about the last approach. However  building 2380 models looks like it will take astronomical time to train and predict. If anyone have tried this approach please feedback your execution time to take insights about this way.,115279,2019-11-07 10:00:49,Aymen Khelifi
"One model has all the data. 2380 models only have 1/2380 of the data each.
Very likely, 2380 models with few data (8784 samples each) will take a bit more training and predicting time than a single model (20 million samples). But not 2380 times.",115279,2019-11-08 03:43:39,Juanma Hernández
don't even ask how many I made,115279,2019-11-02 04:53:15,SteveKane
"I tried lgb per site, 1.58 overall - so needless to say, some work will be necessary :-) ",115279,2019-11-01 17:22:19,Konrad Banachewicz
"yeah , i tried that too with a bit of parameter tuning it resulted in 1.35 ",115279,2019-11-01 18:05:08,AdityaVikramSingh
"I tried 4 * 16 models for meter and corresponding primary use. LB -2.09.
I really thought that would work. ",115279,2019-11-03 16:13:01,Manraj Singh
"I tried lgb with 4-fold per site_id , public lb -2.50 
notebook - https://www.kaggle.com/kimtaegwan/what-s-your-cv-method?scriptVersionId=22377656
Maybe I didn't do it well.",115279,2019-11-02 05:16:30,Taegwan Kim
"Same with me, public lb -2.63.",115279,2019-11-02 21:26:32,Mykola Maliarenko
How about 1449 models for each building :D,115279,2019-11-01 17:00:22,Gunes Evitan
"not a bad idea ;) whats next? 1449 x 4 per building meter? Btw, I actually have 1449 linear regression models… 1.42 LB last time I checked. prob lower by now.",115279,2019-11-01 17:09:09,Tim Yee
"But it won't be a viable solution , what if in future a new building is constructed ?  I understand what you can saying and probably is being tried by everyone 
Moreover i want to understand how can we can tune the hyperparameters for such large number of models?",115279,2019-11-01 17:16:54,AdityaVikramSingh
"There's no tuning for lin reg. Just to be clear, I've also tried 1449 x 4 lin reg models as well. But I think you should try and find what works and what doesn't instead of asking is ideas are good or not. The only way to know is to try it. ",115279,2019-11-01 17:33:32,Tim Yee
"i am inclined towards using lgb or similar tree based model , is there any specific logic for using linear regression models? , while eda i didn't observed high correlation  , maybe i missed something.",115279,2019-11-01 18:08:43,AdityaVikramSingh
No all buildings have 4 meters. There are 2380 building-meter timeseries at all.,115279,2019-11-03 03:49:24,Juanma Hernández
"Yes the order of features matter . You can read about it in the Here
In gist it says : 

This is by design. feature order will affect the accuracy.
  The reason is, when choose a feature to split tree node, if two features have the same split gain, the feature with smaller index(id) will be chosen.

I would suggest you to use SHAP for understanding the feature importance",115454,2019-11-03 05:47:39,AdityaVikramSingh
@avikrams nice share. Thanks.,115454,2019-11-03 11:23:22,Manraj Singh
Then how can we decide order ? Thanks,115454,2019-11-03 12:12:48,ADITYA KUMAR
Did you use the same random_state for both?,115454,2019-11-03 03:37:04,Juanma Hernández
I also suggest to try the same random_state and let us know if the problem/behavior persists or not.,115454,2019-11-03 16:04:38,arnab
Thanks a lot to everyone who replied. In this particular case I do not think it was a problem of numeric + categorical or vice versa. I think I was making mistakes in NaN imputations. After cleaning up the code now I am getting the same results in either order!,115454,2019-11-03 21:19:47,Viswajith
I guess you are using feature subsampling ?,115454,2019-11-03 02:23:18,Jie Lu
Yes. 'subsample': 0.25,115454,2019-11-03 05:41:28,Viswajith
and how is feature subsampling related to feature order?,115454,2019-11-03 05:44:10,Viswajith
"In fact I believe that feature subsampling will still take the same indices, but the problem is that you change your feature order from numeric + categorical to categorical + numeric, and by the same indices the model just gets different subsets of features, that's why you encountered a different result I think",115454,2019-11-03 06:48:53,Jie Lu
"Exactly. This is a common strategy as mentioned by @cdeotte here (in the context of IEEE Fraud Detection) : 
Many people don't realize that your validation (model tuning) and inference (predicting test) procedures can be different.

For example, you can split train dataset by time into early 75% and late 25%. Tune your features and hyperparameters with this validation scheme. Also use early stopping to determine a good number of iterations. Then predict test dataset with 4-Kfold using the same features, hyperparameters, and number of iterations with early stopping disabled.

This works well because your features, hyperparameters, and early stopping were determined with 0.75*len(train) rows of data. And when you predict with 4-KFold, each model will also use 0.75*len(train) rows of data. Also your features, hyperparameters, and early stopping were determined with a scenario that somewhat mimics the relationship between train and test.
",115963,2019-11-12 06:37:39,arnab
shouldn't k-fold take care of this issue?,115963,2019-11-06 19:47:26,Poe Dator
"well here my knowledge starts to run dry, but generally I'm pretty sure a single model trained on 100% of the data will behave differently to the average of 3 models, each trained on a different 66% of the data, and different could easily mean better.
I think someone made a kernel demonstrating something similar…",115963,2019-11-06 21:29:46,Louka Ewington-Pitsos
"Consider algorithms such as random forests and GBMs which use partial data (subsample) at each iteration. Subsampling is similar to your case of ""the average of 3 models, each trained on a different 66% of the data"".  Subsampling makes different models, but I would consider that between subsampling and not subsampling, both would have taken into account the full set of data.",115963,2019-11-06 22:17:16,HyperSeedOptimized
"
train a new model M on 100% of the  data

If you do this, how can you assure that the new model is not overfitting the training data?",115963,2019-11-06 09:43:20,Timon Gurcke
"Right, well you cant. 
But the assumption is that if you trained a model on 80% of the training data (or whatever CV you used) and that didn't overfit  then doing exactly the same thing again just with the whole training dataset also won't overfit. In fact (and in my experience), generally the more data you train your model on the less likely you are to overfit. 
Also, CV can never tell you if you are over-fitting your training set because CV only scores you based on your training set.  If you get a good CV score, it means you aren't over-fitting some subset of the training set. If the training set is super different from the overall distribution of the data, then you can have an excellent CV score and great CV strategy but still horribly overfit the training set. 
In kaggle the only way you can know if you are over-fitting the training set is by making submissions. In real life you have to look for new observations. ",115963,2019-11-06 11:08:05,Louka Ewington-Pitsos
Did you find a way to insure the overall distribution between train and val is similar ? This is very hard with only 1y of data.,115963,2019-11-12 21:18:25,Antoine
"Moreover, with data.table one can write fwrite(sub, ""sub.csv.gz"")",116244,2019-11-07 22:25:16,kxx
"kxx is kind of untouchable, master I would say :-)
Nothing to criticize, never.",116244,2019-11-09 21:39:45,mezoganet
"If I interpret the goals of the competition correctly, the building ids should be in both the public and private datasets. We are asked to produce a model that predicts how much a specific building will consume in the future based on its consumption patterns in the past. Not how much an unknown building will consume based on the past consumption patterns of other buildings. I would therefore be surprised if there is a trick in the train/public test/prive test split.
It also makes sense from the physical interpretation point of view to use building_id in a model, there are most probably building specific characteristics -apart from the ones we have access to- which can be proxied by building id.",117023,2019-11-13 16:27:35,Panos
"Wow amazing answer, thanks a lot!",117023,2019-11-13 18:00:22,Thiago Preischadt
"
It also makes sense from the physical interpretation point of view to use building_id in a model, there are most probably building specific characteristics -apart from the ones we have access to- which can be proxied by building id.

It does not make sense, however, from a generalizability point of view. It would be more useful IMO to create more robust models that are independent of building-specific data since it's plausible that not all buildings that require this model to forecast a counterfactual have historical data available for them.",117023,2019-11-16 18:17:14,calvdee
"You are right, it doesn´t help for the generalisation. We would need more details on the buildings (materials, insulation, type/ size  of windows and a long list of other factors) to be able to do so. But the competition has a clear focus on predicting a specific building´s consumption to serve as a counterfactual for the impact of mofifications in the buidling or its users´ behaviour.",117023,2019-11-16 19:25:23,Panos
"
Is there any guarantee that for instance, building_ids in the private set will be the same as those in the given training/test set?

Answer - Yes 
It's a completely valid question , since we have been provided with a static train / test data , which is not going to change over time .",117023,2019-11-13 07:25:14,AdityaVikramSingh
"Alright so if I understood correctly, the public and private sets are necessarily taken from the same place? If that's the case, isn't it still possible that the way the public set was chosen could only provide information about a set of id's and leave the others to the private set?",117023,2019-11-13 12:35:43,Thiago Preischadt
"What are your thoughts, then, on what is going to be different in the private leaderboard set?",117023,2019-11-16 18:18:45,calvdee
"If let's say, private set's id's were completely different from the public set's id's. But I think I get it now, you guys helped me clear out my thoughts, thanks!",117023,2019-11-17 20:39:18,Thiago Preischadt
"Sorry - this question was meant for Aditya. @avikrams, could you comment?",117023,2019-11-18 20:36:03,calvdee
"
Alright so if I understood correctly, the public and private sets are necessarily taken from the same place?

Yes they are taken from the same source 

isn't it still possible that the way the public set was chosen could only provide information about a set of id's and leave the others to the private set?

It is possible , but in this case you have complete access to the private set (i.e test.csv)  so you can check whether all the building present in the Test set are a part of the train set or not , fortunately for this problem we have exactly same buildings present in both test / train sets, therefore it won't be a problem if you use them directly. 
However if you don't want to take it as a feature then you can focus on forming surrogate features which can describe a building , which in this case is - square-feet , year of build ,floors , usage type , etc . 
In real life I think it's better to have a data preparation script in place , which will ensure that catergorical variables have same levels , and any previously unseen levels are moved to a default level -  'others'",117023,2019-11-19 05:48:45,AdityaVikramSingh
"It’s quite obvious that site id an building Id are very important features, treated as categorical.",117023,2019-11-16 19:17:39,mezoganet
"https://www.kaggle.com/c/ashrae-energy-prediction/discussion/117357#latest-673490

Sohier Dane wrote:
Since several people have asked:

Using publicly available meter readings is within the rules, assuming the general restrictions are also followed such as posting a link to the data in the external data thread by the entry deadline (December 12th).
We will ensure that any meter readings that can be publicly downloaded are not included in the private set.

",117624,2019-11-18 08:37:58,Oskin Nikita
"One of the possible problems solving your problem, try using:
submission.to_csv('submission.csv' , index=False)",117630,2019-11-17 00:07:02,Yaroslav Isaienkov
Yeah thanks a lot @ihelon. I forgot to put in csv index=False. he he he. Thanks a ton ,117630,2019-11-17 06:33:52,Raju Kumar Mishra
Could be the reason why the original author has removed this kernel.,118188,2019-12-05 13:52:19,Mr Loke
"@klloke, you may be right. It has to be broken into more than one script to run it sucessfully. That is what I did after Sohier answered me earlier. Now I am having a similar issue with a script that does not use much space so I asked the Kaggle team to check again. Since the current Kaggle kernel IDE have so many issues, I thought this may help them in debugging.",118188,2019-12-05 14:42:21,YaGana Sheriff-Hussaini
"I do remember Jupyter was having problem where it can't deallocate resources properly in Kaggle last time, but I'm surprised that you mention the script coding is having issue too. Could you try to insert the Except clause to print out the exception in multiple section of your script to see if it consistently died at the same place?",118188,2019-12-05 15:24:40,Mr Loke
"@klloke, sorry it is not a script but a kernel. I misspoke :-)",118188,2019-12-05 15:35:12,YaGana Sheriff-Hussaini
"Try .""New Notebook"" and then select ""Script"" to see if it helps.",118188,2019-12-05 15:42:22,Mr Loke
"It just finished running successfully when I tried it just now. Thanks, I will run it as a script next time so that I do not have to run it more than once.",118188,2019-12-05 15:52:12,YaGana Sheriff-Hussaini
You're welcome. And try re-runnng it a few times again to ensure no more problem.,118188,2019-12-05 16:39:46,Mr Loke
"UPDATE
I rerun the kernel without any changes and now there is no error. The problem must be Kaggle kernel issue and not related to my script when I ran version-2. Anyway, it is at least fully visible for people to see now.",118188,2019-11-22 14:05:24,YaGana Sheriff-Hussaini
"@sohier, can you please check if possible why my kernel ended with a red? I do not want to rerun and waste 2hrs of my GPU and end up with the same thing. Read my post above for details of my experience. Thanks.",118188,2019-11-20 16:08:25,YaGana Sheriff-Hussaini
"@sohier, @addisonhoward, can you please comment on my issue above? I re-ran this kernel without modification and it completed successfully. I have a private version that I ran 3X and still ends up with a red which means I cannot use its output in an ensemble. I have also wasted 6hrs of my GPU quota rerunning the private kernel. This feels frustrating with the fixed hours of GPU Kaggle is allowing at the moment.",118188,2019-11-27 14:40:05,YaGana Sheriff-Hussaini
Looks like it was running out of disk space. I've also passed along the feedback to our engineers that more detailed error reports would be helpful.,118188,2019-11-27 16:37:03,Sohier Dane
"Thanks @sohier, this is helpful and I know what to do now.",118188,2019-11-27 17:16:26,YaGana Sheriff-Hussaini
"@sohier, @addisonhoward, do you have any update on this issue? I just experienced the same issue on a rerun of CPU script that does not have musch to ran out os space. If it helps my script have only 15 minutes runtime.",118188,2019-12-05 11:58:27,YaGana Sheriff-Hussaini
"I have the same question. However, googling for AMLT definition and example gave me a rough idea.
Any library which can:

Automate Data Preparation
  Automate Feature Engineering
  Automatic Model selection
  Automatic Problem detection (Leakage detection, Misconfiguration detection)
  User interfaces and visualizations for automated machine learning

A few examples for such libraries could be  Auto-WEKA, auto-sklearn, H2O Driverless ai etc..
However, this is my assumption and would be great if some kaggle expert validates this. I'm new here :)
Ref: 

https://en.wikipedia.org/wiki/Automated_machine_learning
https://www.kdnuggets.com/2019/01/automated-machine-learning-python.html
",119742,2019-12-01 07:11:04,yashendu
I didn't discover this coefficient until this post mention it. :(,122366,2019-12-20 12:47:45,Dean
"0.9 : I discovered this effect thanks to a coding error :-D 
From the leak data you can see that the average energy consumption went down over the years, although not for all meter types/buildings individually",122366,2019-12-20 05:41:21,Rahul
I found this trick when averaging models. The multiply coefficient can between 0.88 to 0.90.,122366,2019-12-20 03:23:23,Yuan Liu
0.9,122366,2019-12-20 01:02:17,Manraj Singh
"Yes , Why is the mean meter_reading so low ?",122366,2019-12-19 16:56:59,Pratik Poudel
"This is all due to the presence of noise in the data. Therefore, we have optimal predicted values between the average of ""noisy"" training meter readings and zero.",122366,2019-12-19 17:17:51,Nika Zhidkonozhkina
"My private score of last submission is 1.2413097349 which can get a gold. So sad.
",122399,2019-12-28 02:40:14,daishu
@daishu what's the magic behind this submission? The public LB score 1.30 is very high.,122399,2019-12-28 03:10:46,Waylon Wu
Same. Can i select which submission to use for final score after the competition end?,122399,2019-12-20 00:51:40,Shaochun Ning
the same to me. Kaggle waste me one chance!,122399,2019-12-20 00:15:08,Kai Shu
Same here. I submitted mine about 20 minutes before closing of competition.,122399,2019-12-20 00:04:45,YaGana Sheriff-Hussaini
rip,122399,2019-12-20 00:01:25,Aleck Wu
the same…,122399,2019-12-20 00:00:24,Jack
This Comment was deleted.,122399,2019-12-20 12:57:02,No user
Miss the old one,122450,2019-12-20 10:14:39,Stanislav Blinov
Yeah :/ ,122450,2019-12-20 09:11:42,Psi
I'm was cleaning my screen until this.,122450,2019-12-20 09:28:29,Manraj Singh
and here I was thinking to get my lenses checked again 🤓 😂 ,122450,2019-12-20 09:13:30,sandy1112
The kernel is here,122989,2019-12-25 04:30:13,Kangyu Chen
👍 ,122989,2019-12-24 12:00:07,Wu_Yiqun
"
But I have seen some notebooks with weekday and year as features and to be honest

Those may help. 

There are educational institutes/offices which may only operate during the weekdays (more energy consumption) and closed over the weekends (lesser energy consumption). Shopping malls may be more crowded over the weekends (Heavy energy consumption) and less crowded during the weekdays. Thus is_weekend is an important parameter. Similarly, holidays and days of yearly sales could be important features as well.
The energy consumption of a building could be increasing year over year. But, since our training data belongs to just one year, not sure how effective it will be. May be we should build a feature with number of months since 2015 Jan (or something like that to capture the long term trend in the data)

Finally, one thing which I have learned about ML from Jeremy Haward (from his fast.ai course) is : 
Question : ""Will blah help as a feature?""
Answer: ""Try blah & blah and see…""
Hope this helps.",112930,2019-10-16 10:03:45,arnab
"Thanks, that clears things out.",112930,2019-10-16 11:18:54,Aleksander Jakubowski
Maybe this is a dumb question but are these timestamp in the same time zone?,112930,2019-10-17 22:28:04,TerryDuan
"There are no dumb questions as will to know something is intelligent itself.
As for timestamp.. I don't think timezone would matter anyway as midday in for example France carries the same characteristics as midday somewhere else. I think location matters more than timezone.
Besides I don't think they run experiment on sites that far from each other, and also I think algorithms would be able to handle it as each site have unique site id.
Correct me if I'm wrong.",112930,2019-10-19 10:03:00,Aleksander Jakubowski
"Yes it does. buildings metadata stores building type, so you can extract ""working_hours"" factor with lower energy demand",112930,2019-10-16 20:11:11,michal.p
"Thanks arnab, for the explanations you provided. In fact, all of the above explanations are valid, since Energy is a resource that we all use, some more, some less, varying according to the days within the year. Thanks.",112930,2019-10-16 14:52:18,José Bruno
Thanks!,112930,2019-10-16 15:06:38,arnab
"Here you go:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113103#651794",114184,2019-10-24 23:01:51,Francesco Daghero
"Thanks,
Didn't see it earlier.",114184,2019-10-24 23:09:20,Kostiantyn Isaienkov
"When you use the drop down to put your submissions in Public Score order my understanding from lots of discussions over the past two years is that Kaggle sorts using full number of decimals including those not shown.
As noted in the Data section - for this challenge they are using only 4 decimals and suggest we trim our file to the same to reduce the size of the submission file.
S",114501,2019-10-27 01:33:36,PC Jimmmy
What is blending? ,114896,2019-11-07 02:38:05,Martin Elfstadius
https://www.kaggle.com/hmendonca/4-ashrae-blended,114896,2019-11-07 06:30:22,Gideon Vos
We have a winner! https://www.kaggle.com/hmendonca/4-ashrae-blended/output,114896,2019-11-06 23:38:25,Gideon Vos
"As far as i know, people are already doing blending. We have seen NN, catboost, xgboost, lightgbm models doing pretty good themselves. So it is expected to blend the best models among them.",114896,2019-10-30 09:19:28,Khairul Islam
Participated in large number of competitions in last years.  My casual observation is that blending starts when a space of 10 days occurs without someone sharing a new low score kernel.,114896,2019-10-30 02:00:27,PC Jimmmy
I reckon it has already started. ;),114896,2019-10-30 09:19:10,Fredrik Jonsson
"I think it will be quite trouble some to treat it as timeseries as you will have to forecast for 2 years in future and LSTMs are very suitable , it would take a lot of time doing that . Let me know if you can figure out a way to predict multistep forecast at a faster rate",117304,2019-11-14 20:27:56,AdityaVikramSingh
"If you want a public notebook to contain private information (that viewers cannot see), you can use Kaggle Secrets

",117648,2019-11-17 15:37:50,Chris Deotte
wow i never knew i can do something like this would apply it then makes it very easier thanks @cdeotte ,117648,2019-11-17 20:10:08,olaleye eniola
"In the secret value, you can include a URL to another script. Then your public notebook can download and run the script and nobody will know what code you executed. ",117648,2019-11-17 20:12:11,Chris Deotte
"this is serious how did you get to know all this i guess you must have invested critical time on kaggle thank you i cant wait to start doing well in competitions also.
@cdeotte ",117648,2019-11-17 20:19:33,olaleye eniola
"Wow, this is useful.
By the way @cdeotte read your email ;)",117648,2019-11-18 09:19:31,Roman
very very useful @nroman ,117648,2019-11-18 10:24:08,olaleye eniola
"Hi @unilageni If you make a notebook public..you make the history of that Notebook public so all the versions of it if I remember correctly.
What you can do is fork your own Notebook and keep that specific fork private.",117648,2019-11-17 07:36:04,Robin Smits
"oh okay thanks one more question please.
can i have let say 5 versions then delete the one i dont want to make private out of the versions or it automatically deletes the whole version.
@rsmits ",117648,2019-11-17 08:24:36,olaleye eniola
"Hi @unilageni you can only delete a whole notebook…not a specific version of it.
If - for whatever reason - you want to make part of a current private notebook into public then it is either forking and making that one public. It won't show your private original kernel.
Or you can download a specific private kernel as code and then upload it into a new kernel which you can make public.
I hope this answer helps you. Good luck!",117648,2019-11-17 10:00:49,Robin Smits
hmm great thank you very much i get it now @rsmits ,117648,2019-11-17 20:07:51,olaleye eniola
i tried looking for a way to fork it didnt see any thing like that,117648,2019-11-18 18:57:52,olaleye eniola
@rsmits ,117648,2019-11-18 18:58:22,olaleye eniola
"""Copy and Edit Kernel"" is Fork",117648,2019-11-18 19:04:22,Chris Deotte
got it thanks @cdeotte ,117648,2019-11-18 19:06:10,olaleye eniola
"Friday night at midnight UTC is when one week ends and a new quota week begins.  So for me, on the East coast of the USA, that's currently 7pm Friday night.",117856,2019-11-18 11:10:59,Bruce Young
Do you know if GPU augmentation can be done if you are teaming up?,117856,2019-11-18 11:14:06,Linyan
No sorry. I haven't worked in a team yet.,117856,2019-11-18 11:33:38,Bruce Young
"anyway,thank you! : )",117856,2019-11-18 12:10:16,Linyan
"What does GPU augmentation mean? Are you asking if you can use your teammates' GPU quota? The answer is yes. Once you team up, all teammate have access to your Kaggle notebooks. Therefore even if your GPU quota has run out, a teammate can open your notebook and click ""commit"". Then the GPU usage gets deducted from their account and all teammates see results.",117856,2019-11-21 00:58:27,Chris Deotte
Wow，That's it,117856,2019-11-21 04:40:03,Linyan
So this is the final boss :D,118392,2019-11-21 18:31:43,Gunes Evitan
LeakBoost is the best choice for this competition.,118878,2019-11-28 11:02:47,Mohsen Yazdinejad
"Because this is a web scraping competition,  data leakage is the king and kills all the algorithms",118878,2019-11-25 23:00:45,Jie Wu
"We all know that LightGBM is almost 10 times faster than XGBoost. This means that you can just try one model with XGBoost whereas you can try 10 different models with LightGBM. Feature engineering steps require so many trial and errors. That's why, most of kagglers spend time on LightGBM in the early steps of the competition.
On the other hand, it is a fact that XGBoost provides more accurate models. You would do your final attempt with XGBoost when you finish the feature engineering steps.",118878,2019-11-25 07:17:43,Sefik
"But in most recent competitions from last 1 year, most of the people use LightGBM for final predictions in kaggle. Why XGBoost is better as a fact?",118878,2019-11-25 11:52:50,Keshardev
Because how it calculates the bin. They are more accurate then lightgbm one (which uses hist),118878,2019-11-26 07:04:46,Davide Stenner
"XGB is here:
https://www.kaggle.com/sheriytm/simple-xgboost-regressor-model
and catboost https://www.kaggle.com/kyakovlev/ashrae-catboost",118878,2019-11-25 10:46:31,YaGana Sheriff-Hussaini
"but not good scores :/ 
Is it because, they are not performing good? I tried CatBoost and got 1.21 score, while I got 1.08 with LightGBM. Maybe I did not tune Catboost properly.",118878,2019-11-25 11:54:17,Keshardev
"I see the same pattern @keshardev 
I think that is related to features as meter buildingid and siteid.
I think that's because how catboost calculate the numerical representation of categorical. 
In particular i think the problem arise because test set has 1 observations for every timestamp (it has not gaps) so the calculation are different between train and test.
Also in the discussion ""Final thoughts…"" The author says he observed a better score by dropping buildingid and siteid.",118878,2019-11-26 07:03:27,Davide Stenner
"LightGBM does a great job of directly handling categorical values (as long as you correctly declare them), and thus can achieve more power with fewer estimators than XGB. Since it's faster even with the same number of estimators, it's really no contest for this contest.",118878,2019-11-26 00:33:53,Robert Stockton
XGB is too slow,118878,2019-11-25 06:33:18,ahao
I believe NuGene from Sonaosft can accomplish this no problem. ,118878,2019-12-10 23:10:45,Aaron Marinucci
"I tried both XGB and CatBoost,with XGB I  get 1.13 lb score,but it is hard to optimize it，because xgb is too slow.And catboost seems to require memory consumption too much,my catboost died each time I try it",118878,2019-12-06 04:32:54,Wu_Yiqun
"I try to use Catboost, but it doesn't allow me to use too many features( leading to the kernels died), and the best score I got is 1.17. And I fail to run Xgboost until now. Maybe I write the wrong code as it's the first kaggle competition for me. ",118878,2019-11-27 11:31:52,Vivi
"Using catboost with NO category features - on my dual GPU machine it out runs lightgbm hands down.
After lots of attempts I have given up getting lightgbm built with GPU on both Windows and Ubuntu, so only running it with CPU.
Using catboost WITH category features - lightgbm on CPU is the winner by landslide.  Catboost looks busy as heck - both GPU's at full speed with 96% of memory in use - but its at least 20 times slower on basic feature set for this competition vs lightgbm.  ",118878,2019-11-27 03:55:27,PC Jimmmy
"LightGBM CPU version is blazing fast compared to XGBoost. Even for GPU versions of both algorithms, in my tests, I have seen slightly faster runtimes on LightGBM. I haven't tested Catboost for speed though.",118878,2019-11-26 17:12:48,sandy1112
"Could you please elaborate on LGBM GPU speed? I tried it on the 5mln rows sample of ASHRAE data and so virtually no difference with the CPU version speed.  Moreover, unlike Pytorch or TF2 GPU versions, LGBM only loaded my GPU (nvidia 1660ti) by 6-7% (!!!)",118878,2019-11-27 13:38:56,Igor Dudchenko
Is the accuracy of GPU and CPU version same?,118878,2019-11-28 07:23:13,Keshardev
I see the same as ahao.  XGBoost is slower than LightGBM in my experience.  I don't know about CatBoost speed,118878,2019-11-25 11:40:01,Eric Sonnen
CatBoost is faster or same speed.,118878,2019-11-25 11:53:23,Keshardev
"I have tried but it didn't work,seems that size of training data don't influence my model too much，or I didn’t find the best cv with leak date in train set",119726,2019-12-05 03:12:06,Wu_Yiqun
"Yup. It gave me a nice boost on the public LB, not sure how it will play out with the private set",119726,2019-12-02 07:16:14,Rahul
"I had same performance without using it but i had a better cv score estimation (same as lb).
So in the end I'm not using it.",119726,2019-12-02 06:02:33,Davide Stenner
0.01 boost for me,119726,2019-12-02 05:55:28,spongebob
At last…..,121563,2019-12-14 15:17:23,Mr Loke
It would have been better from the beginning of this competition ;-),121563,2019-12-14 08:54:25,mezoganet
I agree  ;-),121563,2019-12-14 09:06:46,Naruhiko Nakanishi
Finally and exactly!,121563,2019-12-15 23:23:41,flyingpiggy
you are either shuffling or using a feature that is overfitting. My first guess is you are shuffling.,122008,2019-12-17 03:37:12,Tim Yee
"As I recall I got a 2.11 and a 3.32 when I had my very best rmse values, slightly better than your numbers.  
I was deleting all the zero and NAN rows from the training set along with the initial months of the site_0 values - my created model was a good match to the training but my elimination of those rows made the training different from the test.  ",122008,2019-12-17 17:12:01,PC Jimmmy
Data cleaning/outlier removal is particularly tricky for just this reason. It's quite hard to judge whether you are training a better model or just eliminating challenging but legitimate entries from your validation set. I usually try to err on the side of caution and adapt my CV to drop potential outliers from training but not from eval. Either that or hold my nose and resort to leaderboard probing.,122008,2019-12-18 07:09:56,Robert Stockton
"I'd agree that you need to be careful about how you do your CV split. Since this competition is time-sequence, it's pretty much vital that you do some sort of time-based split -- and finding just the right split can be pretty hard.
In addition, your brief snippet leads me to suspect that you are evaluating based on early-stopping scores. I try to avoid doing this because the early-stopping process is pretty much designed to overfit on its validation data, in the interest of not overfitting on the training data. Sadly, I suspect that the only safe way to evaluate early stopping is to put CVs inside your CVs or to do a three-way split instead of a two-way split.",122008,2019-12-17 08:22:03,Robert Stockton
"Tim/Rob, thx for your prompt replies. Indeed I set shuffling to True, and maybe that was the cause. As Rob was saying, this is time series data and its vital to do some sort of time-based split. I will set Shuffling = False, much easier :-). ",122008,2019-12-17 16:02:28,ram sastry
This Comment was deleted.,122008,2019-12-17 19:48:25,No user
"I guess so. Since the leak data are from 5 sites, they cannot represent the other sites. Relying too much on the leak data might cause larger discrepancies in the predictions of other sites.",122407,2019-12-20 03:14:49,Yuan Liu
I think it will lead the overfit for the leak data .Maybe it is the most worrying for most kaggles,122407,2019-12-20 01:24:26,九营十八组仙子狗尾巴花
It looks like the number of teams is decreasing(over 3600 teams -> 1877 teams !).,122411,2019-12-20 01:41:07,Yoshi
1500 teams got removed..,122411,2019-12-20 02:07:55,Manoj Prabhakar
"The problem with Public leaderboard is temporary and seems to have been fixed.
If the private leaderboard's ranking was leaked, I regret not having recorded it…",122411,2019-12-20 02:02:04,Yokoichi
Now I am not going to get any medal xd that's so sad ,122411,2019-12-20 02:52:04,Taco
"
You are given hourly data of a building, and must predict the hourly data of the building in the future
Meters within the same building may be correlated
Weather affects how much energy a building may use (cold day -> lots of heating. Hot day -> lots of air conditioning & cooling)
Certain buildings may have traditions where they use a lot more energy on that day. For example, people only come to church on Sunday, so you will see churches have lots of energy use on Sundays and not so much on other days. For example, students only go to school Monday-Friday, so the buildings are barren on the weekends.
The meter readings may be increasing year over year if the building is acquiring more people; it may be decreasing year over year if the building is losing people.
",112871,2019-10-15 23:54:14,CoreyLevinson
"There are few more things, which I want to see if they are true based on data:

older buildings are build with different technology then new one, so the consumption schema will be different. (I would expect that for building from around 1900 cooling is not so big issue as for new buildings. On the other hand heating buildings with high ceiling (often the case for older buildings) may be difficult. 
Wind will be more significant for older buildings, where you are able to open windows. For new, high building it will not matter so much, as they usually have fixed windows. 
Buildings built in the places where weather often is extreme are better prepared for different temperature. (You can see it in real life if you compare buildings from South Europe with these from North Europe. The second one are well prepared for low temperature, and you often don't even need to switch the heating when it's 10 degrees outside. In South you usually have the same 10 degrees inside). 
",112871,2019-10-17 00:47:16,Izabella
"And everybody do so - develop models where there is feature 'building id' - in fact, for the individual buildings. This is a mistake of the authors of the competition. If they wanted to get a versatile model, they had to ask for predictions for the 'square' of ​​the houses or 'year' they were built. And so the predictions are needed for the each buildings.",113362,2019-10-28 18:53:51,Vitalii Mokin
"Seems for this competition it is , as we already have a fixed list of building id in building_metadata.  Also as found in this  kernel, its importance is pretty high.  ",113362,2019-10-19 14:15:17,Khairul Islam
"Since there are over 1000 buildings in the data set a per building model might make sense except for the compute cost - if you get a very big feature set your google cloud platform bill might get pretty big.  So I think its a feature with lot of importance.  Also the issue of train data for a building seldom seems to hit the full year hours that occur in the test.
Harder question for me is if a need exists for a per meter model - I am thinking that it does but using meter type only as feature in early days.",113362,2019-10-18 22:45:00,PC Jimmmy
"Rather than per-building model, what about per-primary_use model? 😏 
Also in my model, building_id is the most significant.",113362,2019-10-20 11:55:51,Ivan de los Santos
"One thing you have to keep in mind is that the classical time series models exploit patterns in the data. We do not have enough data to understand the intrinsic patterns of each building. Moreover forecasting methods are usually best for short term forecasts because they use a bootstrap process of using previous predictions to make future predictions. The errors will accumulate and the method will necessarily be unreliable after a certain amount of time. General regression models will thus usually outperform standard time series models in this competition. You could however mix both up, by using a classical time series models for the short term (e.g January and February 2017) and a regression model for the long term.",113602,2019-10-20 19:44:57,Max Halford
"Thanks @maxhalford, please can you tell us based on your experience and the available data, which is maybe the best single model that we can use in this contest. I am thinking about XGBoost and LightGBM, is there any other model that we can took it in consideration.
Thanks",113602,2019-10-21 04:02:48,Tarek Hamdi
"There is no such thing as a ""best model"" :). You should be spending time thinking about the features, not the model.",113602,2019-10-21 06:11:49,Max Halford
Thanks a lot sir for your inputs Can you please give your 2 cents on CV? I know Stratified KFold won't work. So should I keep 1 month data (Last month) as a validation set for cv? Or any other approach?,113602,2019-10-21 07:57:04,Mukul Sharma
For the moment I've only tried standard k-fold CV with k set to 3. I'll be putting some effort into designing a better CV strategy this week.,113602,2019-10-21 08:11:37,Max Halford
"Yes, actually in my view a standard KFold won't work as sit will leak the future data points and thus its score won't be reliable enough. ",113602,2019-10-21 10:14:40,Mukul Sharma
"To streamline this though process it is useful to know the 3 categories in which missing data can be classified into:
Missing Completely at Random (MCAR)
Missing at Random (MAR)
Missing Not at Random (MNAR)
deeper into each one of them and look for potential treatments that can help us circumvent the problems that missing data creates. 
Some ideas:
Time series imputation
     - Non-time-series specific method

mean imputation
median - imputation
mode - imputation
calcucate the appropriate measure and replace NAs with the values. Appropriate for stationary time series, for example, white noise data
Random sample imputation
replace missing values with observations randomly selected from the remaining (either of it or just some section of it)

Time-Series specific method

Last observation carried forward (LOCF)
Next observation carried backward (NOCB)

This article can be helpfull: https://www.linkedin.com/pulse/data-cleaning-guide-dealing-na-values-karan-rajwanshi/",113602,2019-10-22 22:14:10,CaesarLupum
The best way to fill the meteorological would be filling it with the previous hour/day value rather than mean.  ,113672,2019-10-21 12:36:19,Manraj Singh
"To streamline this though process it is useful to know the 3 categories in which missing data can be classified into:
Missing Completely at Random (MCAR)
Missing at Random (MAR)
Missing Not at Random (MNAR)
deeper into each one of them and look for potential treatments that can help us circumvent the problems that missing data creates. 
Some ideas:
Time series imputation
     - Non-time-series specific method

mean imputation
median - imputation
mode - imputation
calcucate the appropriate measure and replace NAs with the values. Appropriate for stationary time series, for example, white noise data
Random sample imputation
replace missing values with observations randomly selected from the remaining (either of it or just some section of it)

Time-Series specific method

Last observation carried forward (LOCF)
Next observation carried backward (NOCB)

This article can be helpfull: https://www.linkedin.com/pulse/data-cleaning-guide-dealing-na-values-karan-rajwanshi/",113672,2019-10-22 22:13:20,CaesarLupum
"the test data reflect the actual live examples so anyway we need to put in place something to handle Null values.
Try to use something like df.interpolate() for now and maybe there will be better suggestions in the future.",113672,2019-10-21 18:13:56,Adrian Zinovei
"I have not yet made my first submission, but I would say, it depends on the size of the ""hole"" (duration of missingness) in the data. If the it's just one or two hours of missing data for a building, even a simple, ffill() will work. If the ""hole"" is little larger, may be interpolation could be a better approach. But, if the size of the 'hole' is too large, either we have to ignore the data or pick up data from another ""similar"" building. I am not sure if that is at all possible or not. Will expect to hear back from others. ",113672,2019-10-22 05:15:22,arnab
"@themonologue I did as you suggested, but there are no data for some columns in some siteid. 
In this case, I filled with mean of others siteid's.",113672,2019-10-21 17:38:22,Vitor Alcântara Batista
"Try setting timestamp as the index of yor dataframe and use df.interpolate(). Think method=linear would be most accurate, but if the model is to be used in practice, then I guess one would have to use ""ffill"".  This has to be done for each site_id",113672,2019-10-21 16:37:07,maka
This Comment was deleted.,113672,2019-10-22 07:05:59,No user
This Comment was deleted.,113672,2019-10-21 16:36:36,No user
Re-inventing the wheel and occupying bytes in Kaggle's servers…,113679,2019-10-21 13:25:09,Georgios Sarantitis
??,113679,2019-10-21 13:27:07,Manraj Singh
Hahaha this competition is not efficient at all,113679,2019-10-21 13:39:05,Gunes Evitan
I confirm that removing month helps improving my LB 🤘 ,113679,2019-10-21 22:09:27,Jie Lu
@jielu0728  :) An upvote will be appreciated!!,113679,2019-10-22 18:17:36,Manraj Singh
Thanks a lot.But i am curious that 'month' is an important feature from the ouput of lgb features_importance.Why?,113679,2019-10-25 10:42:59,Dxx
"test dataset has 2-year data,  train set has 1.",113756,2019-10-22 02:34:24,LeiZhou
Got it. Thanks!,113756,2019-10-22 03:52:00,Hemanya Tyagi
"
I guess the reason why there are some zeros in target variables is just that these building had not set the sensor at the time. I think it is because these zeros occurred at the beginning of observations. I have to check whether counter-examples exist or not.
",113926,2019-10-23 05:17:23,JumpeiT
"For building id = 7 and meter = 1 there are sudden zeros in between the year and also reading started in 3rd month of the year. 
",113926,2019-10-23 14:30:33,Suchith 
"Thanks for the information, I think there might be great things to do here like filling gaps or erasing them for example 👀 😏 ",113926,2019-10-23 19:01:43,Ivan de los Santos
@suchith0312  Great EDA. Can you share you code for this?,113926,2019-10-23 21:05:17,Manraj Singh
"x = train.loc[(train['building_id'] == 7) &amp; (train['meter'] == 0)]
plt.figure(figsize=(15,5))
plt.xticks(rotation='90')
plt.plot(x['timestamp'],np.log1p(x['meter_reading']),label='meter = 0')
x = train.loc[(train['building_id'] == 7) &amp; (train['meter'] == 1)]
plt.plot(x['timestamp'],np.log1p(x['meter_reading']),label='meter = 1')
plt.title(""building id = 7"")
plt.legend()
plt.show()
",113926,2019-10-24 07:41:08,Suchith 
"Coming back to the subject, now that we have more information as regards the sites:
Sites, buildings identified by internet search
In Canada and the USA, the dates of DST were:
2016: 13 March- 6 November
2017: 12 March - 5 November
2018: 11 March- 4 November
In the UK and Ireland:
2016: 27 March - 30 October
2017: 26 March- 29 October
2018: 25 March-28 October
Most tree-based models should capture the impact of DST for April-October through the combination of site_id and month. This leaves us with a large part of March and a few days in November in Canada/US & a few days in March and October for UK/Ireland that may need to be flagged.",114520,2019-11-16 13:07:54,Panos
Canada does apply DST,114520,2019-11-16 14:23:16,S D
"Thanks for this, I read it wrong in the website I consulted. Canada has the same DST dates as the US. (edited original comment)",114520,2019-11-16 15:03:25,Panos
"My impression it that we're dealing with data from the US, Canada, and UK mainly. AFAIK they all use daylight savings, but the UK switches at a different calendar date.
Daylight saving time 2019 in United Kingdom began at 1:00 a.m. on
Sunday, March 31
and ended at 2:00 a.m. on
Sunday, October 27
All times are in United Kingdom Time.
Daylight saving time 2019 in United States began at 2:00 a.m. on
Sunday, March 10
and ends at 2:00 a.m. on
Sunday, November 3
All times are in Eastern Time.",114520,2019-10-28 16:07:13,S D
@sdoria I'd appreciate your feedback. I found site 0 is ucf: https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114762#latest-660382,114520,2019-10-29 04:10:45,RC
@klloke We can use temperature unit of Celsius vs Fahrenheit to validate this theory.,114520,2019-10-28 03:32:25,RC
But how do you know which building located in USA?,114520,2019-10-28 00:20:24,Mr Loke
"Check out this post to understand how daylight saving works. 
In the U.S., clocks change at 2:00 a.m. local time. In spring, clocks spring forward from 1:59 a.m. to 3:00 a.m.; in fall, clocks fall back from 1:59 a.m. to 1:00 a.m.
http://www.webexhibits.org/daylightsaving/b.html",114520,2019-10-27 17:07:27,Kangbo Lu
Some predictions will not be scored.,114538,2019-11-04 13:44:36,LongYin/杰少
There was a comment by the sponsor that if any gaps existed in test they would not be scored.  I think that meant that gaps existed but the test set was built for the full hours in 2018 for every building.  ,114538,2019-10-27 06:44:33,PC Jimmmy
"That makes sense.
I found it in data description:

Gaps in the test set are not revealed or scored.

Thanks.",114538,2019-10-27 08:14:02,Juanma Hernández
You might have noticed that in my notebook https://www.kaggle.com/datadugong/locate-better-cities-by-weather-temp-fill-nans some of the data goes to 1/1/2019. I started doing this when I realised the same. All new data I collect will have this 👍 ,116028,2019-11-07 23:09:57,Silverback
thanks! this should fix it.,116028,2019-11-08 09:07:29,Poe Dator
"
What I am missing?

Web scrapping obviously ;)",116989,2019-11-12 18:44:14,CPMP
"Thanks a lot, will look into it.",116989,2019-11-12 19:22:13,Mukul Sharma
"The US sites use Celsius, but those temperatures have (generally) been converted from Fahrenheit.",116989,2019-11-12 19:33:31,S D
The boost we get from leaks is 0.03. You have to be creative with your models.,116989,2019-11-12 18:15:11,Gunes Evitan
"Thanks for reply, I have been following you in this competition. Do you find change in temperature units over site ids or change in units of area measurements ? As said by some dicussion threads? Also  I think thid competition is more of finding appropriate external, please correct me if I am wrong. Thank you!",116989,2019-11-12 19:25:08,Mukul Sharma
"Hi! Take a look at this great kernel! 
At section 5 there is an explanation how to reduce memory usage.
https://www.kaggle.com/caesarlupum/ashrae-start-here-a-gentle-introduction",117284,2019-11-15 00:14:09,Stanislav Lykov
Thank you! I'll take a look!,117284,2019-11-15 00:25:42,Miguel Wychovaniec
"Like @starl1ght mentioned, almost everyone uses a version of this function (reducememusage). Also, always make sure to delete all variables you are done with and call ""gc.collect()"" to force the kernel to free up space. ",117284,2019-11-18 08:49:27,Rasselio D.
thanks @starl1ght for this suggestion,117284,2019-11-15 13:29:21,BryanB
Does anybody know what has happened to Cornel (site 15) website? [http://portal.emcs.cornell.edu/],118363,2019-12-04 16:24:55,Masoud
"Oo…
most likely the admin though he got hacked given the sudden scraping activity…",118363,2019-12-04 19:54:23,eagle4
"Data Leakage Exercise
refer to:
https://www.kaggle.com/grapestone5321/ashrae-sample-submission-and-data-leakage-exercise",118363,2019-12-02 10:38:00,Naruhiko Nakanishi
"Data Leakage in Machine Learning
refer to:
https://machinelearningmastery.com/data-leakage-machine-learning/",118363,2019-12-02 10:34:16,Naruhiko Nakanishi
"Naruhiko ,  Can you please explain it ( in simple sentence) in the context of this competition.",118363,2019-12-02 07:49:31,yogi
"The goal of predictive modeling is to develop a model that makes accurate predictions on new data, unseen during training.
This is a hard problem, because we cannot evaluate the model on something we don’t have.
Therefore, we must estimate the performance of the model on unseen data by training it on only some of the data we have and evaluating it on the rest of the data.",118363,2019-12-02 10:32:34,Naruhiko Nakanishi
i tried with site 1 and 0 but lb score decreased by 0.01 (no idea why ),118717,2019-11-25 14:35:37,ADITYA KUMAR
"So it was worse? This might have to be asked by ourselves. How does it guarantee that data from the leaks are significantly helpful to predict the other sites? just nothing but augmenting some proportion of train dataset, so potentially helpful by the law of large numbers or nothing different from before leaks????
We know one of the worst scenarios. Someone hide publicly available leaks and blend some of it at the end of competition. Who will be going to do and explain their secret when they get gold or above? looks fun. silver/bronze might be… I cannot see how LB will be changed, it is so contaminated, and I cannot aim it not too low position.",118717,2019-11-25 17:47:06,Hanjoon Choe
"Hi @hanjoonchoe I was able to increase my score by 0.01 by using leak data as additional training data. To eliminate the effect of the train data on the test data I compared 2 submission where the leak data was used to update the test portion for that site.
1 submission I had not used the leak data to train…the other submission I did use. It gave a nice performance boost.
I've only used 1 site with leak data sofar but will be adding more in a similar way.
It could be coincidence or the way I put it together…for me at least it helped.",118717,2019-11-25 21:02:56,Robin Smits
Thank you for notice. ;-),118717,2019-11-25 21:16:37,Hanjoon Choe
"I made an attempt at this using site0 data to try to improve score for sites 1-15, but I saw mixed results. I think you're asking the right question though.",118717,2019-11-23 21:31:53,Tim Yee
"What is the mixed results? So, There is nobody(but you) willing to say about this topic. XD I don't know if someone has significant advantages from the leaks(except replacing) when I see LB histories seemingly.",118717,2019-11-25 13:47:56,Hanjoon Choe
Check the first pinned topic.  It's there.,119619,2019-11-30 23:01:57,Larry Schuster
This Comment was deleted.,119619,2019-11-30 15:26:10,No user
See top pinned post from competition organizer,119660,2019-12-01 19:31:08,Larry Schuster
I know about it.But what I really want to ask is that does anyone mtiply 0.2931 to correct it .and how about the result.,119660,2019-12-02 04:26:27,Linyan
@huanglinyan  improve lb score ( but less then 0.01),119660,2019-12-02 10:05:17,ADITYA KUMAR
"@negi009 ,if it less than 0.01 ,how do you know about it .because 0.95 leapboard can't check it out through the rank",119660,2019-12-02 10:43:51,Linyan
"you can sort submission file in My submission for 3rd decimal digit accuracy  
sort in MY  submission by  public score  # I GOT  with site  0 multiply  0.2931  (0.96) without (0.96) but # after sort lb improve  shown in my submission (with site  0 multiply by   0.2931 )",119660,2019-12-02 11:46:07,ADITYA KUMAR
"Wow,thank you ! I haven't think of this - sort by My submission. I haven't find it before",119660,2019-12-02 14:51:23,Linyan
"I have to admit that I realized that thanks to your post 😂 
I was quite used to be the opposite ratio… Anyway, I think they might have changed it in order to reduce shake ups. It's just a guess :)",113976,2019-10-23 16:12:08,Federico Raimondi
I think it's because of infrastructure and the size of the data. The hits/sec is might be high that's why they reduced it to 22% for this competition only. ,113976,2019-10-24 08:26:51,Shashi Prakash Tripathi
"When there are a bunch of folks at the same value you can figure out another decimal by the position your in compared to the others.    When I looked you were near the bottom of the 1.25 pile, so give yourself a 1.251  ",114372,2019-10-26 01:02:19,PC Jimmmy
It depends on the precision (decimal digits) Kaggle use to round your score…,114372,2019-10-26 10:57:44,mezoganet
This dataset is prone to overfitting. Changing parameters makes sense only when you have stable validation,114745,2019-11-03 20:55:02,snovik
"I think we can find storm day from air pressure values. Storm will be big impact for energy consumption of the city.
http://www.rbs0.com/spm.htm
Check this site. Threre were some storms in US in 2016-2018.  ",115102,2019-11-01 09:01:01,Isamu Yamashita
"Great point, Isamu!",115102,2019-11-01 23:15:09,AI Penguin
Thanks for idea Isamu,115102,2019-11-03 01:51:02,Praveen 
It's still in my idea list yet😄 ,115102,2019-11-03 12:03:21,Isamu Yamashita
"There are few ways to make it smaller:

lower boosting rounds (higher lr)
less categorical features
less depth
",115310,2019-11-03 10:56:35,Konstantin Yakovlev
"Or more gpus, haha",115310,2019-11-04 13:37:49,LongYin/杰少
@longyin2 not an option for kaggle kernels :/,115310,2019-11-04 18:21:58,Gunes Evitan
"Yes, not an option for kaggle kernels",115310,2019-11-05 01:43:15,LongYin/杰少
"Do you have many of categorical features? The combinations that CatBoost creates greatly increases the GPU model size.
Hope this helps!
More information:
https://github.com/catboost/catboost/issues/680",115310,2019-11-01 20:03:06,Carlo Lepelaars
"Thanks, this explains a lot but I have only 6 categoricals. I don't think it is too many.",115310,2019-11-01 20:08:12,Gunes Evitan
"Hmm, strange. I've also had problems with CatBoost GPU gobbling up a lot of space, but I'm not sure why it needs that much.",115310,2019-11-01 20:11:27,Carlo Lepelaars
catboost is coded to run maximum GPU size - I think the default is to use 96% of available.  Not sure if this is related to save size of the model - ??    You can change the default to a lower percentage and see if it helps.,115310,2019-11-02 03:00:27,PC Jimmmy
I iteratively take predictions from each model and then delete the model and its training data before blending the predictions.,115310,2019-11-05 04:40:14,Siddharth Yadav
"Tried lots of different parameters, but still couldn't find a way to implement a 5 fold cross validation with CatBoost GPU models without running out of memory.  Any ideas?",115310,2019-11-03 08:38:08,Gunes Evitan
"Increase size of swap file - I am using 200GB on my Ubuntu machine and 400GB on my Windows machines - needs to be on SSD to not slow things down.
Increasing on Windows 10 very easy to do - on Ubuntu I am on my 5th attempt to change my 3rd PC - think something got broken in 19.10 - going to replace with 18.04 and see if I can get it working.  Fails when I try swapon.",115310,2019-11-03 16:22:42,PC Jimmmy
OK - 6th attempt in progress.  Ubuntu 18.04 hangs as I try to build the swap file.  Not sure how I got lucky on the first machines that it worked.  Time to re-install and try again.  ,115310,2019-11-04 00:49:47,PC Jimmmy
"
Out of the 1449 buildings only 1413 have electricity meters installed. 498 buildings have chilled water connections, 324 have stem connection and  145 have hot water connections.
Again  875 buildings have only one type of meter, 230 have two types, 331 have three types and only 13  buildings have all the types.
If the   image  is not clear check this this kernel. for details.",115572,2019-11-04 08:23:33,Rajesh Vikraman
Looks like a good feature candidate 🤔 ,115572,2019-11-04 09:29:59,Jie Lu
"""Not every building has all meter types.""
You don't need to predict the missing meters",115572,2019-11-03 19:54:14,bluetrain
"Thanks, I realize that the answer is already in test dataset, we have the same missing data for the same buildings. Only the following buildings have all meter available: 1232, 1241, 1249, 1258, 1259, 1293, 1294, 1295, 1296, 1297, 1298, 1301, 1331. For others we can have partial data (so we can predict) or no data at all (we don't predict). For instance:
test_pd.query(""building_id == 504 &amp; meter == 1"")
return zero lines.",115572,2019-11-04 08:02:54,MPWARE
For your information I've already tried to predict without building_id. That requires much longer time to converge (early stop). And yet no better result than my model with building_id.,115572,2019-11-04 09:12:08,Jie Lu
"
We could try to predict by ignoring building_id. Is that what organizer want from us?

This is most likely what organizers are trying to get from us, but you know Kaggle doesn't work that way. The most overfitting model to the test set wins the price.  You should use everything they gave you.",115572,2019-11-04 08:00:26,Gunes Evitan
Nice visualization @mpware. Would love to see your code.,115572,2019-11-03 19:42:42,Manraj Singh
"Here is how we can see missing data (in test dataset):
# Missing data per building_id in test data
METER_DICT = {0: 'electricity', 1: 'chilledwater', 2: 'steam', 3: 'hotwater'}
test_pd[""meter_reading""] = 1.0 # Dummy prediction
test_meter_pd = test_pd.groupby([""building_id"", ""meter""]).agg({""meter_reading"": 'count'}).reset_index()
test_meter_pd[""meter""] = test_meter_pd[""meter""].map(METER_DICT)
test_meter_pd = test_meter_pd.pivot(index='building_id', columns='meter', values='meter_reading').fillna(0)
fig, ax = plt.subplots(figsize=(26, 4))
d = sns.heatmap(data=test_meter_pd.T, ax=ax)
d = plt.title(""Count of non-null meter_reading observations"")
d = plt.yticks(rotation=0)
",115572,2019-11-04 08:07:00,MPWARE
"It has been happened to me yesterday too,.",117592,2019-11-16 15:00:51,AdityaVikramSingh
"Perhaps you are much more than overfittting ;-)
Or quite at the Karma ? 
Don’t know honestly… not seen from my side that at the moment.",117592,2019-11-16 19:11:40,mezoganet
"Not sure what do karma and overfitting have to deal with that, but the last sentence is actually informative - so thanks for that :-) ",117592,2019-11-16 19:47:31,Konrad Banachewicz
"After two years on Kaggle I still find it very surprising that the parent company is Google but connection issues are pretty frequent on the site.   I don't use it that often but also found the Google Cloud Platform to have frequent connection issues for me.  I have 1GB internet, but don't think that matters or helps.
Closing my browser and clearing the cache often works - but the thing that works best is time - just wait.
Two days ago I did the wait thing.",117592,2019-11-17 17:06:51,PC Jimmmy
"It seems you saved submission as super user. Try to $ chmod 777 submission.csv, for instance. I faced the same problem when I ran scripts with super user rights.",117592,2019-11-17 09:36:22,Sergey Bryansky
I'm getting the same now.,117592,2019-11-16 18:38:48,Robert Stockton
"
Data Leakage is the presence of unexpected additional information in the training data, allowing a model or machine learning algorithm to make unrealistically good predictions.
Leakage is a pervasive challenge in applied machine learning, causing models to over-represent their generalization error and often rendering them useless in the real world. It can be caused by human or mechanical error, and can be intentional or unintentional in both cases.

You can find out more info in the competition documentation here:
https://www.kaggle.com/docs/competitions#what-is-leakage",118685,2019-11-23 17:46:30,Andrea Garritano
"In terms of competitions, it means that the target values of the test set have been found, or some very strong indicator like the mean of the meter reading per house, per hour or something like that. It is not data that is given by the organizers. In fact, they try to stop participants from finding this data. :)   To improve your score, you should check out other high scoring kernels and see what they have done and you haven't. This way, over time you will learn more concepts which can be applied in other competitions as well.",118685,2019-11-24 07:46:50,Ayush Goel
"Fairway:
Just try to plot some histograms and scatter plots and see what each column means. Then submit a baseline, even if it is just the train data. Now think of features like interactions between weather, or even try to get some more data online like humidity or something. You could also see other notebooks and discussions to get some more ideas. Often I just end up multiplying various features and feeding it into the model as part of feature engineering. Towards the end of the competition, try out multiple models and ensemble them any way you find best.  You should probably be able to find the right set of params in notebooks anyway.
Unfair way, but it is already out there:
People have posted notebooks on leaked data, so you could follow those and get some. The discussions are also generally updated if someone finds another leak. So all you need to do is pretty much stay updated!",120376,2019-12-05 17:15:47,Ayush Goel
"Instead of the daily average, you can use interpolation for the weather data: https://www.kaggle.com/hmendonca/clean-weather-data-eda
Also, I believe some nulls like in precipdepth1hr actually mean 0",120552,2019-12-09 14:00:45,Henrique Mendonça
"Thank you @hmendonca .
I have checked you kernel, what a beautiful and useful kernel.",120552,2019-12-09 15:07:03,python10pm
"Happened to me too. Rectified it by clearing my browser cache. 
Alternatively, if you are using Chrome, you can just open an incognito tab Ctrl + Shift + N and log in there, it worked for me. ",121544,2019-12-14 04:09:47,ChewZY
"@chewzy thanks, clearing my Chrome browser cache also solved the problem for me",121544,2019-12-14 14:30:13,Numeral
Ok it is back to normal now.,121544,2019-12-14 06:45:49,Mr Loke
me too.  White out at kaggle kernel. please fix it soon 🙏 🙏 🙏 ,121544,2019-12-14 01:36:33,Isamu Yamashita
Same with me,121544,2019-12-14 01:26:40,Numeral
This Comment was deleted.,121544,2019-12-14 01:22:00,No user
This Comment was deleted.,121544,2019-12-14 01:21:04,No user
"I want to reduce your minus votes, but I cannot upvote your post. why?",121863,2019-12-16 21:27:16,Hanjoon Choe
I did it 👍 ,121863,2019-12-16 21:55:52,Oussa
"You can check the merge team deadline on Rules.
In this competition, the deadline was 12 Dec",121863,2019-12-16 15:05:53,Alan Clappis
the deadline to make teams is over!,121863,2019-12-16 08:20:23,Mohammad Farzanullah
Why would the medal zone be based on 1877 teams! It would be very irrational,122476,2019-12-23 10:03:57,Navid Hakimi
At least the organizers should declare any such drastic decision.,122476,2019-12-23 10:59:50,Pankaj
"
According to Addison Howard:
Don't fear! We didn't remove 2,000 of you as cheaters 😉  , but we've encountered a few snags in the meantime. 
",122476,2019-12-23 16:22:26,Masoud
the count on the banner is misleading. If you click and expand the Public LB fully you will see that majority of teams are still there. Many teams with public scores (at random positions though) are appearing there. ,122476,2019-12-20 12:42:25,sandy1112
ah thanks :),122476,2019-12-20 16:17:34,Pankaj
But medals zone is now based on 1877 teams  if you see the  medal color bar next to each user in the board (e.g. top 10% is now 187 and any team above 187 ranking is out of medal zone now).,122476,2019-12-20 19:26:19,Masoud
"Any reason why it is being done this way?
This competition has been very weird",122476,2019-12-20 19:35:48,Pankaj
Scroll to bottom and you'll find that the entire public LB is being recalculated,122484,2019-12-20 13:39:07,Jie Lu
"nothing funky here. The organizers did mention that LB will look odd while they are rescoring and checking for cheaters. Refer to this discussion:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122296#699019",122484,2019-12-20 13:30:51,sandy1112
"UPDATE
See the latest from the Kaggle team
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122554",122488,2019-12-21 22:02:14,YaGana Sheriff-Hussaini
I notice some of the top 50 contestants name disappeared in LB too. ,122488,2019-12-20 14:00:15,Mr Loke
"@klloke, if you scroll down all the way to 3677, you will see all the rescored teams so far as you go down from there. Like I said most teams may have only one submission rescored at the moment but this will soon change.",122488,2019-12-20 14:13:18,YaGana Sheriff-Hussaini
Now I notice it. Thanks :),122488,2019-12-20 14:15:54,Mr Loke
You are welcome @klloke.,122488,2019-12-20 16:13:36,YaGana Sheriff-Hussaini
"
Inversion:
  We always split time-based data by time for Train/Test, and Public/Private. If we didn't do that, the Public leaderboard score would provide feedback (leakage) for future time points.
",112896,2019-10-16 04:03:53,authman
"Usually, this information isn't given by the competition host and you should use some submissions to probe the leaderboard and try to find out!",112896,2019-10-16 03:56:00,Mohammed Tayor
"That's true…
But I think they might randomly select 78% data as public LB since my validation score and LB score are very close
Edit:
I'm wrong. It's split by time ",112896,2019-10-16 03:59:04,Alex
Even if it is randomly split just probe the leaderboard :),112896,2019-10-16 04:02:05,Mohammed Tayor
"Hey Zachary, great to see you here. Thanks for the notebook.",113045,2019-10-17 11:22:56,Rafid Abyaad
"Hi, thanks for the code - in this line:
to = TabularPandas(train, procs, cat_vars, cont_vars, y_names=dep_var, splits=splits, is_y_cat=False)
you need to add is_y_cat=False otherwise it will categorize y_names. 
At the beginning I've also done:
train['meter_reading'] = np.log1p(train['meter_reading'])
And while it trains now:

I've added Cuda here procs = [Normalize, Categorify, FillMissing, Cuda] but still it doesn't use Cuda.
When doing .databunch()  it casts targets to Longs, so precision is lost (this is relevant as Im doing log1p above)
",113045,2019-10-17 09:15:29,Andres Torrubia
"Thank you Andres! That would explain everything :) 
I’ll look into those two issues you mentioned today and see what I can come up with ",113045,2019-10-17 14:47:52,Zach
Cuda is not a tabular proc so it wouldn't apply here. I'm working to get RAPIDs working in Kaggle as that would be how to use Cuda. Fastai has it integrated just need to figure out how to install the library in a kernel,113045,2019-10-18 00:23:08,Zach
"I've updated the notebook according to Andres' suggestions, it's working here",113045,2019-10-18 00:20:15,Zach
thanks sam,113763,2019-10-22 14:53:32,Ashik M
"If you consider mean (sum divided by a constant) which is an estimator  of expected value , it's true that:
Defining F(x) = log (x)
E(f(x)) != f(E(x))   --> this hold only for linear function
That's because of the Law of the unconscious statistician (Lotus). https://en.wikipedia.org/wiki/Lawoftheunconsciousstatistician
Thanks to the law of large numbers you can estimate it by using the mean.
So if you want to create some kind of aggregation as a sum or mean on a function i suggest you on applying the function first and aggregate after.",116602,2019-11-10 09:33:58,Davide Stenner
"Hmm, interesting point. Just seems a bit odd that with the charts above using sum(logs) shows that electricity usage is the highest, whereas using the log(sum) shows that steam uses the most. 
thanks for the help",116602,2019-11-10 10:17:10,BhavekJ
My approach for batches of zeroes is to filter them out - seems to help a bit.,117077,2019-11-13 10:10:47,Konrad Banachewicz
I haven't tested everything yet but its good to know that there are others thinking about similar approaches.,117077,2019-11-13 10:29:33,krinya
"I will go with option three.need to replace it with estimated value .For Estimation one can ignore zero values.
Estimation method can be average of last six months ignoring zero 
Or any other",117077,2019-11-18 17:23:54,Smita
"Thanks for the answers.
I solved it by the following steps:

I opened only needed dataframes and deleted them immediately after completing to use them. For example test.csv is a huge one and I don't need it until the end of training.
I used reducing-dataframe-memory-size-by-65 techniques. But be careful: it's filling Null values. So make sure to fill them according to** your** logic.
Even after improvements, I wasn't able to run it on Kaggle, since it fails on out of memory without proper logs. I had to run it on my laptop.
",117102,2019-11-14 10:10:23,Benzion
"First, i suggest to make etl pipeline and save train, test in pkl or hdf in another kernel.
After that , load and fit and if you run out of memory again , you need to drop one or more column (ram used by xgboost depends highly on the number of column and row). Max number of columns to keep is 26/29 (i find it by some trial).
Take in mind that xgboost leaks some memory during the fit so if you make cv or more model you need to optimize more.",117102,2019-12-03 07:09:36,Davide Stenner
"Hello! I've faced the same problem, but I could only choose to run on my laptop because I lack an GPU on my computer. Do you use CPU to run the program?
By the way, do you solve the problem now?",117102,2019-12-06 07:37:24,lawrence
If your running kernel on local PC you can change virtual memory size (Windows) or change swap file size (Linux).,117102,2019-11-13 15:32:13,PC Jimmmy
"No, I run Kaggle kernel.",117102,2019-11-13 15:46:32,Benzion
Ok - I can't be of much help since I seldom use Kaggle kernels - but several shared kernels have described methods to reduce memory size so if you have not used any of those methods to reduce the bytes size for each of your variables they would be worth incorporating into your kernel.  Probably any of the best scoring or high voted kernels will have a def for memory usage reduction.,117102,2019-11-13 16:24:19,PC Jimmmy
"I was also facing the same issue. 
You can delete the variables which are not in use so that it can be deleted from RAM.

Like after you split your training data into train and validation you can delete you full training dataset to save your RAM memory. 
del training
",117102,2019-12-03 05:39:37,Prateek Mishra
Are you using GPU? I found using non GPU will have higher RAM available compared with GPU enabled. ,117102,2019-11-13 13:39:23,Mr Loke
I just tried CPU. Still fails. ,117102,2019-11-13 15:38:23,Benzion
Please try to reduce the number of features and reduce the RAM usage of data types before calling XGB fit/train method.,117102,2019-11-13 16:41:49,Mr Loke
"Update - Bad news - I'm going about my business saving my processed data - now i'm paranoid this isn't even saving properly. Need to investigate correct procedures for saving on google cloud, without destroying data",117671,2019-11-17 05:40:05,Mark Isaac
Update - This was the fault of the Jupyter set-up on google cloud. When I saved the predictions on coloabs - it worked. ,117671,2019-11-17 04:23:38,Mark Isaac
I also want to know,117937,2019-11-20 13:45:16,zidaoziyan
"I just used the public leakage data to fill the submission csv. 
but I think the final score do not contain the leakage data. I try to use [Building gene… ]GitHub data 2015 ] to train. but it's don't help me to approve. for now , just forget the public score. to do what you want way to approve the score.  I'm pure kernel just get 1.06 in Public . so keep going you can get a good score.",117937,2019-11-19 01:22:22,Zhang Yunfei
"I did - I used to add a ""month"" feature, but so far it just overfits like crazy.",117937,2019-11-18 21:50:01,Konrad Banachewicz
"If you have stable and reliable CV, your CV should improve along side your LB score. So finding reliable CV is your first priority before attempting feature engineering.",119190,2019-11-27 09:16:34,Tim Yee
"In general, there are two different approaches for feature selection: wrapper and filter methods. 
Roughly speaking, wrapper methods create a lot of models with different subsets of features, and then select the features which result in the best performing model. As an example, you can take a look at Recursive Feature Elimination (RFE) here: https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html 
On the other hand, filter methods use statistics to investigate the relationship between each predictor and target variable. For example, correlation statistical measures are widely used as a basis for filter feature selection methods.  ",119190,2019-11-27 07:03:14,Stanislav Lykov
"Another note: if you are using XGB models, they can leak memory pretty badly. I use the following wrapper to work around the issue:
class XGBWrapper(BaseEstimator, RegressorMixin):
    def __init__(self, **params):
        self.model = XGBRegressor(**params)

    def fit(self, X, y, **kwargs):
        fitted = clone(self.model).fit(X, y.values, **kwargs)
        self.feature_importances_ = fitted.feature_importances_
        dumped = pickle.dumps(fitted)
        del fitted
        gc.collect()
        self.fitted = pickle.loads(dumped)
        del dumped
        return self

    def predict(self, X):
        return self.fitted.predict(X)

    def get_params(self, deep=True):
        return self.model.get_params(deep)

    def set_params(self, **params):
        self.model.set_params(params)
",119728,2019-12-01 22:29:24,Robert Stockton
Maybe It's out of memory when running the script.,119728,2019-12-01 02:19:19,Dean
Yes it is but how to tune it further without dropping essential features? ,119728,2019-12-01 02:53:30,Gopi Yedla
"If you really want to minimize data, you can represent most of the weather measurements as 16-bit integers without loss of information by simply multiplying by 10 and then truncating. It'll gain you a few extra bytes.
If you don't want to use ""del"" to get rid of unneeded data as soon as you are finished with it, you can put everything inside of bite-sized functions. As soon as the function exits, the local variables will fall out of scope and the memory will be released.
You don't need to have all of the test data in memory at the same time. You can split the main test table into pieces and join each individual piece with the building and weather tables as it is needed. You also don't need to have your train and validation data in memory at the same time, though if you are doing full CV you'll have some nasty time/space trade-offs to consider.",119728,2019-12-01 03:26:10,Robert Stockton
Thank you Robert. Let me try this.,119728,2019-12-01 09:28:23,Gopi Yedla
"Actually to release all the memory you have also to put the function in a subprocess, after the sub process is killed all memory will be released. That's because thanks to memory fragmentation gc.collect won't release all memory.
https://stackoverflow.com/a/1316799
To make a subprocess just follow that example:
https://stackoverflow.com/a/2046630",119728,2019-12-01 21:01:43,Davide Stenner
"good job, I'm just confused why I call gc.collect, and nothing change. @stenford23 ",119728,2019-12-07 02:54:58,DiegoJohnson
"@diegojohnson 
Because python memory management works with threshold and generation to choose when to release some memory. 
So if your dataset has too many (""unused"") reference you can't free it's memory therefore deleting a variable and calling gc.collect won't do anything in that case.
You should check every reference of a dataset and work on it, but i think it's more simple to just make all the work on a separated subprocess and kill it after it finish (maybe saving the results on an hdf or pickle).",119728,2019-12-07 06:28:27,Davide Stenner
"thanks very much, it's useful.",119728,2019-12-07 06:31:58,DiegoJohnson
Can you tell me what new features you've constructed?,120869,2019-12-10 02:31:19,XiQiao
"CV 0.8425
LB 1.06, very similar to 1.05",120869,2019-12-09 13:27:17,bigbigbang
"https://weatherstreet.com/weatherquestions/What_is_dewpoint_temperature.htm

The dewpoint temperature is the temperature at which the air can no longer ""hold"" all of the water vapor which is mixed with it, and some of the water vapor must condense into liquid water. The dew point is always lower than (or equal to) the air temperature……
  Interesting facts:
  SOUPY AIR: When the dewpoint approaches 75 degrees F, most people can ""feel"" the thickness of the air as they breathe, since the water vapor content is so high (about 20 grams of water vapor per kilogram of dry air, or 2% of the air's mass).

hmm…",113737,2019-10-23 04:34:31,LeiZhou
"You may be interested in this wiki article 
Dew point",113737,2019-10-21 22:13:55,Jie Lu
precipdepth1_hr whats..? this,113737,2019-10-21 22:06:32,pranshu patel
This Comment was deleted.,113737,2019-10-21 22:05:01,No user
means it convert air temperature into Degree celsius,113737,2019-10-21 22:07:14,pranshu patel
"I would say try to mix temperature and dew_temprature, it should give you something…
Just my point of view.",113737,2019-10-22 20:13:45,mezoganet
"Try different approaches and see how it affects your scoring. E.g.:

Fill in the mean or median. 
Group by siteid or primaryuse and then calculate specific means/medians to fill in.
Calculate mean energy consumption of building with missing floor_count, then find buildings with similar energy consumptions taking into account other comparable features. 
",114082,2019-10-24 10:51:47,chmaxx
"sub.to_csv(FULL_PATH + 'subs/submission_0.1.gz', index = False, compression= 'gzip')",114121,2019-10-24 13:11:44,Oleg Knaub
Creating a gz-file worked! Thanks for your help.,114121,2019-10-25 08:15:02,Ellen de Ridder
"Yes, as Oleg said, writing zip or gz file is very easy with python + pandas, and I recommend that you use those formats, too. You just write .zip or .gz at the end of your file name in pandas DataFrame.to_csv() then the library automatically creates a compressed file. You can submit the compressed file as usual.
RAR or 7z file could be smaller than zip/gz, but I didn't see a dramatic difference when I applied to train.csv.
train.csv.zip  120M
train.rar        110M
train.7z          80M

I got the same error with rar file using Ubuntu, while 7z worked.
sudo apt install p7zip-full p7zip-rar
7z a submission.7z submission.csv

Summary: Use zip or gz for simplicity. You can use 7z if file size is crucial. I don't know how rar file should be created, maybe depends on the software for compression.",114121,2019-10-24 16:38:40,🐢 Jun Koda
"I would be happy to team up, but we have too many submissions combined. After seeing your 1.11 linear models, I got inspired and got myself a somewhat crazy 1.12 ""linear"" model. Need a little more work to get to your 1.11 though, but I think I could be close :/",116538,2019-11-10 00:18:02,Tim Yee
Amazing!You just use the same features in linear method as those in lgb model? or originally the same features but do some transform?,116538,2019-11-10 01:46:04,Dxx
"no transform, same variables",116538,2019-11-10 02:12:00,SteveKane
"Unfortunately you have too many submissions that we can't team up, but it would be great of course.",116538,2019-11-09 21:55:35,Dmitry Labazkin
"Oh, there is a rule about that?",116538,2019-11-09 22:12:13,SteveKane
"Yes, in Rules section: ""Team mergers are allowed and can be performed by the team leader. In order to merge, the combined team must have a total submission count less than or equal to the maximum allowed as of the merge date. The maximum allowed is the number of submissions per day multiplied by the number of days the competition has been running.""
So as I understand this competition was started 25 days ago, and for 2 people who want to team up have to submit no more than 50 submits now. So I think you can now team up only with kagglers with <= 1 submit. ",116538,2019-11-09 23:01:50,Dmitry Labazkin
Okay. I could stop making submissions today and maybe someone will team up later,116538,2019-11-09 23:03:38,SteveKane
"I have a lot to share, but not shure it would be of interest when I see your positon right now… 
Sorry, impressed an shy.",116538,2019-11-09 21:35:41,mezoganet
"Hi Mezoganet if you are looking for teams , we are open to join yours ",116538,2019-11-13 06:31:39,AdityaVikramSingh
"there are so many way to handle the missing values one good way is find one Attribute in data set with is linearly related to the Missing values Attribute fit a llinear reg to that and find the missing values …! :)
i used to do this :)",117618,2019-12-13 06:21:30,Deepu Deepu
I wanted to know if anybody has used some sophisticated methods for imputing missing values which may be interesting to try. Now I have tried interpolation and estimator based imputation. Thanks for your comment.,117618,2019-12-13 07:14:39,Ishaan Jain
How did you manage that? I tried using an imputer but it keeps giving me memory errors,117618,2019-12-12 22:11:07,Juan Vazquez
"For avoiding memory errors you can do the data pre-processing and feature engineering part before merging the train and weather files. This will help in low memory consumption. 
I simply dropped all the features with any NaN values for starter kernel and if you wanna try imputation then you can go with Interpolation as it has many methods which can be useful. 
For reference:
Link to my Starter code kernel : https://www.kaggle.com/ishaan45/starter-code
Link to interpolation kernel : https://www.kaggle.com/ishaan45/pandas-profiling-missing-value-imputation",117618,2019-12-13 03:47:36,Ishaan Jain
Fill with disclosed useless predictions will show you clearer improvement of the prediction of your model when you evaluate the model with still valid predictions by checking LB.,118343,2019-11-20 21:33:14,Hanjoon Choe
"Thanks! You're right using the ground truth labels will show us clearer improvement on the data that is not leaked… 
I'm still interested if we can use the data in a different way to improve our models on other sides :)",118343,2019-11-20 23:00:19,Timon Gurcke
I just started with this competition. I know there is a leak so it is affecting the leaderboard. So should I use leaked data or not into my modeling?,118343,2019-11-22 07:02:29,0DD1
you should try training without and with additional data and see what happens. you can also use external data for validation setup if you wish. You can check out my notebook for ideas. ,118343,2019-11-22 08:26:22,Tim Yee
"Exactly, I am also wondering how can we use the leaked data without overfitting the model? Suggestions are welcome.",118343,2019-11-21 05:29:38,arnab
If organizers had provided us with 15 sites - all of them having 2016 data and 5 of them have 2016 to 2018 data what would you do?,118343,2019-11-21 07:47:56,PC Jimmmy
That is pretty much exactly my question!,118343,2019-11-21 10:31:12,Timon Gurcke
"Lets assume it turns out we do have 5 sites of added data in the next weeks.  Lets also pretend that the 5 sites with all 3 years of data had been part of the initial data offered.
OK - using leaked data to simply fill in the submission for the appearance of a better score would NOT be something you would have done if the data had been offered right up front.  So those kernels that show you how to back fill your predictions with the scraped data are useless and would never have occurred if the added 5 had been offered up front.  Always try to remember that in the real world you don't get a leader board to see how your doing.
One of my major issues with the original data set was the need to predict two years given only a single year.  Having data for all three years for some sites lets me see a number of features in action.  Biggest one that comes to mind is holidays.  Assuming a USA site there are I think around a dozen legal holidays.  Some of them are major, like Christmas where almost everyone takes that day off, schools are closed, additional days off around the holiday, etc.  Some are minor and should only have a small blip on the power usage.  Even adding in days off around the major holidays the impact is probably hard to see.   Now we can look at those kind of features over the full three year span.
I started a feature set of football games at the USA sites - the population of many colleges doubles on home football games, homecoming weekend, etc.  But less than 6 home games for most teams - now I can look at 18 home games.  SO - features that were infrequent in the original data set now occur more often with greater potential that your model can see them.  It's looking like from discussion posts that we are also closing in on knowing almost all of the cities - so lots more potential for events as features.
Most of the models I have created have very nice matches (ok you could call them over-trained) to the 2016 data.  Some of the very best matches are the worst performers on leader board.  Cannot think of too many folks who would have looked at the 5 added sites with all three years and said - hmm - better not put this data right into my train data set for my model.   So pretty much my exact answer would be - put the added data into your training set - that's would you would have done if it was offered as part of the original   data package.  Now you do have some additional worries as you now have an unbalanced data set so you get the chance to learn how to handle that (rather than the current learning of how to google and scrape).
As almost a universal truth - adding more data does NOT lead to over-fitting - it's one of the cures for over fitting.",118343,2019-11-21 17:11:04,PC Jimmmy
"Dividing by 'site_id' may be intuitive, Vopani' kernel is a great job",118681,2019-11-23 14:33:38,ll
"I included this in my analysis in .Strategy evaluation: What Helps and How Much?. The net improvement if you fix timestamps is about 0.4%, which is worth a few places on the LB.",118731,2019-11-26 00:45:41,Robert Stockton
People are going to copy paste leaks instead of reading them from csv file,118913,2019-11-25 13:34:31,Gunes Evitan
Do you have any suggestions to save this competition? @gunesevitan ,118913,2019-11-27 02:55:15,Shahules786
"I already posted my suggestion here
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/118375",118913,2019-11-27 05:11:32,Gunes Evitan
"I think that it is not the solution, because you have the access to test public/private data and can use it (not directly) while training the model even in kernels-only competition (if you assume that any site with leakage will remain in final test dataset).",118913,2019-11-25 12:29:41,Dmitry Labazkin
Do you mean hard coding of this dataset?,118913,2019-11-25 12:55:25,Kostiantyn Isaienkov
This Comment was deleted.,118913,2019-11-25 13:08:31,No user
"I mean any tuning that is based on leakage info, including indirect usage.",118913,2019-11-25 13:38:07,Dmitry Labazkin
"I think it might be able to fix the issue to a high degree. If kaggle can somehow block web scraping libraries in their notebooks, most people would compete fairly. Kaggle can then reset the leaderboard, and then only people with enough time to copy each value of the test set to the kernel will have a good score, which I am sure almost no one has.",118913,2019-11-27 16:36:41,Ayush Goel
My memory is wrong. ;) i saw some says that  he uses  leak data mixing with training data in discussion rather than public kernel.,119020,2019-11-26 08:15:22,jacobfrey
"I think your reply maybe right .My predict data has many same number among different building. Maybe adding leak data makes model overfitted. And i think whether adding leak data make  the distributions of training data far different from test data.However i also find in public kernel, someone using leak data  has a good result.And i don't know how they do that.",119020,2019-11-26 07:55:53,jacobfrey
"Yes that could make your train set leaked. ;) I did not quiet see that kernel, is that kernel predicting itself with leaked data of that building?",119020,2019-11-26 07:58:00,Hanjoon Choe
"Although I do not know a lot , this is one possibility. Meter readings of the same building with different years are tend to be high correlation, but the other buildings might tend to get poor correlation with that building. If leaked data is overly fitted to the building itself. The model could lose generalization of the overall trends. I have some doubts augmenting leaked data is helpful to predict the others also. Meter readings of each building are having quiet different behavior? Then adding meter readings of another year is helpful?",119020,2019-11-26 07:34:38,Hanjoon Choe
"What I thought about the leaked data at the first time. It could be used for validation set set not train set. This might be highly useful , we could find some nice spot penetrating all meter reading trends in years only if it could be found. I do not use it though anyways.",119020,2019-11-26 07:50:03,Hanjoon Choe
"just lightgbm, Five features has missing values, but just 17000. ",119020,2019-11-26 07:19:16,jacobfrey
"try to clean data (maybe you have other error in the pipeline).
I used leak data and i observed better model and better cv estimation.",119020,2019-11-26 07:28:14,Davide Stenner
"Which model are you using?
Is the leak data added correctly during the pipeline or they have missing values on some features?",119020,2019-11-26 06:52:14,Davide Stenner
the LB evaluation is still on and the number of teams is not yet finalized. The final team count will be different than what you see now (or what you have seen till yesterday). Let's wait for the LB to be finalized. As per the organizers it should take 24-48 business hours (which means quite a long wait considering the weekend and upcoming Christmas holidays). ,122423,2019-12-20 03:59:42,sandy1112
Thank you very much,122423,2019-12-20 04:50:36,sekfook97
"Im not even on the leaderboard anymore. My submissions shows loading sigh….not sure what that means. I did all my stuff on Kaggles kernel, it shows all my progression….",122423,2019-12-20 03:46:58,Ben Fan
"did you try expanding the entire Public LB? Most of the teams are still there. Your last held public LB is still showing, so I believe you are still in the competition. Submissions loading sign is for most of the teams as of now. let's wait for a few more days for the final LB to show up",122423,2019-12-20 12:51:57,sandy1112
Measurement is still on going！,122423,2019-12-20 03:42:43,Jie Wu
Finally i improved my workflow with great results. Since version 4 the kernel can train on the whole data and predict all test data in acceptable time. ,113481,2019-10-20 17:09:52,Rico Hoffmann
Use a virtual machine with high ram and work on a IDE instead of kaggle kernel.,113481,2019-10-19 23:09:58,Cem
Many thanks for your response.,113481,2019-10-20 17:07:47,Rico Hoffmann
"My model already predicts values less than zero.  I am trying to break it of that bad habit.
Unless your correcting for it, your model probably doing the same right now.",113959,2019-10-23 16:18:58,PC Jimmmy
"You added weather conditions to your model, so the model already has this information and if there is some interaction between failure (target = 0) and whether, the model will recognise that and predict it in future. I don't think that it makes sense to predict target = 0 or not",113959,2019-10-23 12:06:51,Oleg Knaub
This Comment was deleted.,113959,2019-10-23 16:16:46,No user
You can use it to introduce some weather indexes. I tried to use it.,114989,2019-10-30 16:01:33,Hanjoon Choe
Did it improve results ? How did it come out in feature importance ?,114989,2019-10-30 16:10:35,Khairul Islam
"I hope, but I did not make it into submission seriously. There are some difficulties to submit my works (computing power etc…)  :(",114989,2019-10-30 16:22:27,Hanjoon Choe
Same exp here: lagged weather features actually results in an overfit #WTH,117517,2019-11-16 10:04:28,Konrad Banachewicz
Have you tried seeing the correlation between Target and your lagged variables? It's a very basic thing that you can do and understand how your lagged feautures are working. ,117517,2019-11-16 06:51:31,AdityaVikramSingh
"Don't use LSTM and any time series model.
You have to forecast over 2 years with 1 years of data… quite difficult.
Use tabular model : lgbm, xgboost, nn ,…",120474,2019-12-06 11:33:16,Davide Stenner
"Maybe you can use some ideas from this competition for feature engineering, although the competition context is a bit different, the features (those that explain some statistics such as min, max, median, difference, etc.) could be useful!",121129,2019-12-12 21:45:26,Novin Shahroudi
Thank you so much! ,121129,2019-12-16 08:23:49,Aron
https://datascience.stackexchange.com/questions/44087/how-can-i-check-if-a-bigger-training-data-set-would-improve-my-accuracy-of-my-sc,121858,2019-12-16 11:53:13,Larry Schuster
See test.csv for that mapping.,112860,2019-10-15 22:12:19,Sohier Dane
On test we have 2 years and on train 1 year so… should we do a year prediction? total final prediction? that point its not clear,112860,2019-10-15 22:21:48,EnricRovira
Ok i saw row_id  description,112860,2019-10-15 22:58:34,EnricRovira
"I'd suggest you start by checking out the Notebook section, and upvoting notebooks you find helpful. There are already a few notebooks that do a good job explaining what you might be looking for.",112966,2019-10-16 13:50:18,Rob Mulla
Discussion post from the sponsor -1 indicates trace amount of rain,113031,2019-10-18 04:49:05,PC Jimmmy
"Hi, try with this:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/113103#latest-657313 ",113031,2019-10-28 13:22:14,Jose Muñoz
"Done, thanks!",113034,2019-10-16 17:21:22,Sohier Dane
"Thanks for the quick reflection. 

And I really like the concept of this competition.
Thank you for hosting this contest.",113034,2019-10-16 17:23:22,Subin An
"Ohhh i diden't make the idea clear, some series (when i talk about series is a buildingid +  meter). Each buildingid in combination with a meter is a series.
Some series dont have data for some dates. 
",113108,2019-10-17 13:32:41,ragnar
"In the picture above, in y-axis you have each buildingid and in the x-axis you have the days of the February. You can see that top builidingids dont have observations for days 11-29.",113108,2019-10-17 13:36:36,ragnar
This is for the electricity meter. ,113108,2019-10-17 13:37:28,ragnar
"Nope, no missing date values.",113108,2019-10-17 08:53:36,Daniel
"year_built column missing values: 12127645/20216100
floor_count column missing values: 16709167/20216100
air_temperature column missing values: 96658/20216100
cloud_coverage column missing values: 8825365/20216100
dew_temperature column missing values: 100140/20216100
precip_depth_1_hr column missing values: 3749023/20216100
sea_level_pressure column missing values: 1231669/20216100
wind_direction column missing values: 1449048/20216100
wind_speed column missing values: 143676/20216100


year_built column missing values: 24598080/41697600
floor_count column missing values: 34444320/41697600
air_temperature column missing values: 221901/41697600
cloud_coverage column missing values: 19542180/41697600
dew_temperature column missing values: 260799/41697600
precip_depth_1_hr column missing values: 7801563/41697600
sea_level_pressure column missing values: 2516826/41697600
wind_direction column missing values: 2978663/41697600
wind_speed column missing values: 302089/41697600

These are the missing values in features for me. You probably did something wrong while merging data sets.",113108,2019-10-17 08:42:10,Gunes Evitan
"
square_feet - Gross floor area of the building

From the Data tab",113126,2019-10-17 10:04:10,Gunes Evitan
"it seems highly correlated to the floor count, so it's probably the total area…",113126,2019-10-17 10:54:11,Henrique Mendonça
Likely the ground floor!,113126,2019-10-17 09:35:08,Mohammed Tayor
Not correct. @gunesevitan is correct.,113126,2019-11-01 22:09:35,Bill Holst
"from knockknock import email_sender

@email_sender(recipient_email=""your_email@address.com"", sender_email=""grandma's_email@gmail.com"")
def train_your_nicest_model(your_nicest_parameters):
    import time
    time.sleep(10000)
    return {'loss': 0.9} # Optional return value

Nice way to get emails from your grandma! ;)",113148,2019-10-17 11:12:48,Henrique Mendonça
"If you were very clever you could use the weather data to find the city - of course, you need all the weather data.  While lots of it online these days it's seems hard to find a single download that does it all.  A decade ago you could download the entire USA dataset as couple of huge files.  Not sure you can do that anymore.",113307,2019-10-18 22:58:20,PC Jimmmy
You can find out the when & if daylight savings changes occur. That can might give you time zones. ,113307,2019-10-18 23:51:53,Ptolemy
So you think in the train data set on change date there will be a hour missing for all buildings for a site (city) when we jump ahead ?  Don't think you can spot the fall back change - you agree?,113307,2019-10-19 07:05:22,PC Jimmmy
@claytonmiller or @sohier Could we have the  general geographical regions for the buildings if not the states?,113307,2019-10-18 18:45:05,Ptolemy
"@ptolemy , 
It is good idea, 
but I don't think we have enough data to extract location of buildings.",113307,2019-10-18 14:31:14,Kostiantyn Isaienkov
"Test is double as its a full two years - train is one year with lots of missing dates.  Think row id in test just to help keep stuff straight for submission purposes but most kernels I see drop pretty quick.
The buildings can have up to 4 different types of meters - so a row will contain a meter reading and tell you the meter type.  
Time stamp should be actual day by 1 hour increments.  Again train is missing some, but test should have all 24 hours for each day for each building, etc.  
Hope this helps.",113391,2019-10-19 07:02:47,PC Jimmmy
Try to generate multiple outputs for different configuration of your model and then you can use GMEAN of low correlated values.,114019,2019-10-24 08:31:22,Shashi Prakash Tripathi
Not sure what method you plan on using but catboost and lightgbm both run well under the 9 hour kaggle limit - catboost without burning GPU hours also good.,114083,2019-10-25 03:45:58,PC Jimmmy
If you need to install dfply into kernel make sure you have internet turned on.,114134,2019-10-25 03:42:46,PC Jimmmy
"I haven't tried it yet, but filling the weather features can be done with a rolling window. I'll try to fill them with 2/3/4 hour window means and update my post here tomorrow.",114220,2019-10-25 19:51:50,Gunes Evitan
"great idea, thanks for sharing",114220,2019-10-25 21:55:07,Rashid Haffadi
"Do you mean missing features or missing target (or both?) When I model with gradient boosting, I typically just encode null feature values as something extreme like -99999 and let the model figure out for itself how to handle nullness - this usually seems to work as well as or better than manual imputation. 
For a non tree-based model (e.g. neural net), you'd probably want to impute very carefully, since the model is probably very scale sensitive. Using column means/medians is an ok starting point, but using other feature information to increase precision (e.g. fill with mean of category groups) is often better in my experience. If you think the feature is critical to the model, it may even be worth it to take the time to build a dedicated model to predict that feature and then use this model for imputation.",114220,2019-10-25 03:29:35,Joe Eddy
"I agree with you.
I'm talking about features that have a few missing values like air_temperature
im using random forest for benchmarking and feature selection with feature importance and partial dependance. ",114220,2019-10-25 19:01:52,Rashid Haffadi
"for df in [df_train, df_test]:

    # timestamp and timedelta (hours)
    df['timestamp'] = pd.to_datetime(df['timestamp'], infer_datetime_format=True)
    df['timedelta'] = ((pd.to_timedelta(df['timestamp'] - df_train['timestamp'].min()).dt.total_seconds().astype('int64')) / 3600)

    # Categorical date and  time features
    df['HourOfDay'] = df['timestamp'].dt.hour.values.astype(np.uint8)
    df['DayOfWeek'] = df['timestamp'].dt.dayofweek.values.astype(np.uint8)
    df['DayOfMonth'] = df['timestamp'].dt.day.values.astype(np.uint8)  
    df['DayOfYear'] = df['timestamp'].dt.dayofyear.values.astype(np.uint16)
    df['WeekOfYear'] = df['timestamp'].dt.weekofyear.values.astype(np.uint8)
    df['MonthOfYear'] = df['timestamp'].dt.month.values.astype(np.uint8)

    # Continuous date and time features
    df['Hour'] = df['timedelta'].astype(np.uint16)
    df['Day'] = (df['Hour'] / 24).astype(np.uint16)
    df['Week'] = (df['Day'] / 7).astype(np.uint8)

This takes 1 minute for me to do it on Kaggle kernels.",114422,2019-10-26 06:11:21,Gunes Evitan
"That looks more optimized, but I do not see you using sin/cos for cyclic variables. How you will say to the algorithm that 23h00 and 00h00 are close ",114422,2019-10-26 06:46:01,Mohamed Djemai
I don't think it is cumulative. You are training the model for meter reading on an hourly basis.,114590,2019-10-28 00:19:15,Mr Loke
It's not cumulative.,114590,2019-10-28 00:17:58,Juanma Hernández
"2016 = Training set, 2017/2018 = Test set. They make that clear from the start, and please see the existing threads where this was already discussed. (Beware of the obvious dangers with only having one year as training set, such as entraining to specific public holiday dates (for the year 2016) and date-and-site-specific weather values).",116725,2019-11-11 05:06:20,Stephen McInerney
"Hi !
I think that this Notebook may help you:
https://www.kaggle.com/frednavruzov/aligning-temperature-timestamp",117490,2019-11-15 22:15:45,Pandas
,113162,2019-10-17 14:41:07,Manraj Singh
Interesting. Thanks!,113162,2019-10-17 14:44:03,Abhishek Girish Patil
"@themonologue Where does it say that NaN value of floor count implies 'ground floor'? I am not able to figure it out. If I see the definition of floor_count:

floor_count - Number of floors of the building

Does not it mean that a building with just ground floor (single floored building) should have a count of 1? Am I missing something?",113162,2019-10-21 06:24:14,arnab
weather test provides the weather data for the test period - so you know the weather for the full three years,113814,2019-10-23 05:41:17,PC Jimmmy
For different timestamps and site_id you have the corresponding meteorological data. ,113814,2019-10-22 18:29:31,Manraj Singh
Schools often don't have full student loads during summer.,113910,2019-10-23 05:38:07,PC Jimmmy
"That's some kind of bias, what if they only own summer schools?",113910,2019-10-23 18:10:52,Ivan de los Santos
"Some buildings have primary use as ""Education"". Do you expect weekends would be different for them?",113910,2019-10-23 03:31:03,SteveKane
Nice catch 🤜 ,113910,2019-10-23 18:11:57,Ivan de los Santos
No issue here.,117353,2019-11-14 23:43:03,Mr Loke
"Use reductmemoryreduction functions (search on kaggle, it's a function which reduce the memory consumption by downcasting the type of variables) on building dataset, weather and train before join.",117355,2019-11-14 21:46:02,Davide Stenner
You may check the feather format to load the data as well - solves a lot of memory issues.  ,117355,2019-11-15 08:52:03,Ioannis M
Support/Contact,117434,2019-11-15 22:00:49,PC Jimmmy
"Hi @marquis08, thanks for reporting. This should now be resolved 👍 ",117434,2019-11-18 20:26:09,Ian Catindig
@ian thanks. solved !,117434,2019-11-20 10:06:10,YGSEO
I don't think you need to worry about people merging ground truth labels onto test prediction submission. What you should worry about is people finding a way to make the leaked test labels useful for lowering private LB by training/validating using all available data. This part is extremely difficult and tricky. Look at people's score without leak here.,118348,2019-11-20 23:13:39,Tim Yee
Yes! what you said is correct! I think it might be better if we use leaked test data and train data as a new train data to train model. ,118348,2019-11-21 00:55:14,Yixinchen
"LB will be corrected after eliminating dummy predictions, but I do not think it will be much different from current position unless someone has a similar true LB score with Top scorers and do not replace dummies with leaked data.",118348,2019-11-21 10:00:34,Hanjoon Choe
"I wonder how they can eliminate dummy predictions…
Maybe the real prediction will have some little difference from true value so the perfectly same data will be recognized as dummy data?",118348,2019-11-21 13:15:48,Yixinchen
Official says they will exclude predictions publicly available. I do not know how they know if these are though.,118348,2019-11-21 20:10:19,Hanjoon Choe
"There are a lot of 0.98 today，I think it's because of @yamsam 's kernel ，I haven't added site 4, 15 to my submission yet",119049,2019-11-26 11:31:00,firstbloodY
Sorry for that. I want to try to keep this competition to be fair for everyone. Or It probably break the LB more🤕 ,119049,2019-11-26 12:20:55,Isamu Yamashita
It doesn't matter. Your idea of Leak Validation is great. It allows me to focus on improving my model and get more accurate feedback,119049,2019-11-26 13:37:44,firstbloodY
You can add too. :D,119049,2019-11-26 15:02:38,Hanjoon Choe
Site 4 is UC Berkeley and it is GMT-8 but your dictionary mentions it as 7.,119758,2019-12-02 05:39:50,Sefik
"If you work in a kernel, and do not use this function, then most likely your kernel will fall from a lack of resources.
Some features take up more memory space than they should, and since there is too much data, this is critical",113199,2019-10-17 23:45:09,Yaroslav Isaienkov
thnx a lot . ,113199,2019-10-18 03:23:06,Anurag
We should always use reduce memory when tackling large amount of data in kaggle or others?,113199,2019-10-18 05:56:47,Anurag
I think yes. Bringing the data to the desired (smallest) type saves our resources and the running time of the algorithm,113199,2019-10-18 19:59:37,Yaroslav Isaienkov
If by 'normalized' you mean the meter_reading needs to be 'converted'  than that's correct.  If you did your work like most with log of the meter readings you need to use the exp to get back to standard units for the submission.,114474,2019-10-27 01:24:16,PC Jimmmy
"No, your submission's meter reading shoudn't be normalized. You can predict as normilized value and then transform back with np.expm1 for example.",114474,2019-10-26 16:52:37,Yaroslav Isaienkov
"is your kernel stuck in queue?
If yes , shut it down and run your kernel in edit mode , one cell at a time . and try to see whether it's actually getting stuck, most of the times , merge on huge dataframes or StratifiedKfold get's stuck on this . If you using some boosting method then reduce the parameters like - numboostedrounds, leaves etc",115419,2019-11-02 20:35:45,AdityaVikramSingh
reduce estimators and increase your learning rate. ,115419,2019-11-02 17:40:46,Manraj Singh
"If you are using LightGBM, there is a problem with current numpy version. You can see my post at Speed up LightGBM.",115419,2019-11-03 04:14:55,Juanma Hernández
"https://www.kaggle.com/c/ashrae-energy-prediction/discussion/114046#latest-657659
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/115547#latest-665116",118313,2019-11-20 22:21:29,Larry Schuster
"According to another post, there seem to have negative meter_readings inside of the predictions. This systematic negative predictions need to be converted to 0 or something in my opinion. ;-)",118313,2019-11-20 21:45:50,Hanjoon Choe
true : you can't calculate the metric with negative values but I am guessing Kaggle engine is converting them automatically to 0 if you submit any negative values.,118313,2019-11-20 23:07:46,eagle4
"I think when meter reading is less than or equal to -1 since this makes log1p into DNE(approaching to negative infinity as a value goes to -1) . Am i right or miss something? :)
I do not know how to manage to interpret when submission has some negative values when actual meter reading is positive, but one post show us that there are impressive negative readings inside of the target. In this case, metric could be calculated since both numerator and denominator will be negative so that it will be positive only if the prediction predicts the readings in negative way(synchronization).
I just came across one more Idea for author. have you tried to change meter reading into log1p form and then convert it with exmp1 after? This would be better…",118313,2019-11-21 00:20:22,Hanjoon Choe
"@hanjoonchoe  I think you are referring to this discussion:
Negative readings in the test set? ",118313,2019-11-21 05:24:31,arnab
Yes :),118313,2019-11-21 09:33:09,Hanjoon Choe
"Unfortunately, it is impossible according to Game Theory.",118444,2019-11-21 13:44:58,Ahmet Erdem
Interesting! @divrikwicky Do you mind elaborating?,118444,2019-11-21 16:44:39,Rishabh Agrahari
"Going strike may be mutually best option but let's look at individually:
If I go strike: The rest goes to strike and Kaggle learns a lesson. Or the rest doesn't go strike and I get ranked the last.
If I don't go strike: The rest goes to strike and I get the first position. Or the rest doesn't go strike and I get ranked 50th.",118444,2019-11-21 17:10:50,Ahmet Erdem
"Strike is passive action, someone can screw up whole things by finding all data if possible. ^^",118444,2019-11-21 19:30:44,Hanjoon Choe
or we can work together and share the leaks,118444,2019-11-22 13:37:17,Gunes Evitan
"Sorry, but even if I'm french, I don't plan to go on strike, because:

I enjoy spending time on Kaggle
I enjoy the gamification introduced by rankings
I love Data Science
I love Science, problems, and problems solving
I love the thematic diversity of problems offered by Kaggle's plateform
I love the idea of pertaining to a growing community of smart people
I want Kaggle to be around there for many more years
etc…
",118444,2019-11-21 20:00:06,FabienDaniel
"In the words of Ali G ""is you for real""… ",118444,2019-11-21 15:01:55,Konrad Banachewicz
a strike ? are you French ?,118444,2019-11-21 16:26:50,eagle4
"What if my employer of almost 50 years had given me an unfair problem to solve.  What does my boss learn if I go on strike?   Probably learns he can do without me.
What would my military years have looked like if I told my company commander that I was not going to do what he asked because he did not tell me all the right information - in the USA my military years would have been spent at Fort Leavenworth - - the site of the military prison.
I happen to think that the Kaggle staff are pretty smart group of folks - like any smart organization they will learn from this competition.   In fact, over my 50 years of working I learned the most from my greatest failures.  I did not need my crew to go on strike to learn and grow from things that did not go as planned.",118444,2019-11-21 17:27:20,PC Jimmmy
"You have posted very good points. Timestamp includes date, so month can be extracted from it.",112911,2019-10-16 07:15:26,Prashant Chaudhari
"Another thing that can affect energy consumption is the building geometry. All else being equal a simple rectangular ""box"" will have better retention than one that's spread out with various wings. I guess that the total area (e.g. in square feet) of exterior walls would be one way to characterize this.",112911,2019-10-17 00:28:47,Byron Dom
"IMHO we should start with considering it as a Regression Problem for the following reason:
The data doesn't only contain multiple time series features related to weather, but it consists of building meta data as well which is static in nature (don't vary with time).
I will say this is a regression problem with time as a feature. The problem is we can't use timestamps as it is. We have to break time stamp into day, month, hour, minute etc. Please look into the winning solutions of ""Power Laws: Forecasting Energy Consumption - By DrivenData"" mentioned here. That will give you further idea.",112969,2019-10-16 15:16:14,arnab
"@arnabbiswas1 thanks for the link, i had went through that in your previous thread and tried the details, and its quite inspiring, although the top solutions had more than 2-3 models, which was quite unusual for me as a beginner, could you shed some light in that? and its advantages",112969,2019-10-16 15:25:09,Rajat Ranjan
"Different models could help you predict on different data. 
Suppose model 1 is able to predict a+b = c
and model 2 is able to predict a + f = g
but vice versa isn't, so if you use both it'll be always helpful.",112969,2019-10-16 15:29:20,Manraj Singh
"Welcome to Kaggle. When you are not sure which algorithm you should try first, go for LightGBM or XGBoost. :-)",112969,2019-10-16 15:39:59,arnab
"Hello Rajat, as we are considering a target that ""moves"" considering a time variable (timestamp), the Time Series behaves better than Regression, but nothing prevents us from testing all approaches because it will always depend on the problem. business to be solved, as well as the data that is available for modeling.",112969,2019-10-16 15:09:45,José Bruno
"yes, the field is vast and we should try different approaches as well, will keep it in mind",112969,2019-10-16 15:25:46,Rajat Ranjan
"In other words, this competition is too realistic!",114893,2019-10-30 09:03:59,Richard Allen
"Unless you're pulling the data from 7 legacy systems owned by different organizations who don't want anyone else using the data, then, no, it's not ""realistic"". 😂 ",114893,2019-10-31 14:51:07,inversion
"I am sorry I can't catch you…
Does Kaggle is a simple ""sending files"" and try to ""pull it all over"" ?
I just think that we are not there to clean up data. Just my opinion not a judgement, again and again, Mr Inversion.
I remember last Sberbank and Mercedes competitions…",114893,2019-10-31 19:48:57,mezoganet
"If Kaggle provides absolutely clean datasets, Kagglers will never get a job.",114893,2019-10-30 05:02:17,Vopani
Absolutely right!,114893,2019-10-31 05:20:05,KevinHuCMU
"Did you ever get a Job with Kaggle ? Phew ! (Homer Simpson saying accordind to WikiPedia)
I agree that beeing among the best Kaggler can help, but that's not enough… I'm afraid. At least in France.
Do not want to critisize anything here, NEVER.",114893,2019-10-31 19:58:29,mezoganet
"Temperatures look like they are all in C.
There is indeed an issue with timestamps not being aligned between weather data and meter data. I hope it will get corrected. Mistakes happen.
I haven't looked at the pressure data, is there something wrong with it?
I don't think you need to know the location of the buildings to proceed, but people who like to investigate data will surely find some things out. Happens with all competitions. Diving deep into the data is part of the data analyst job.
It's a pretty hard problem. There's 1400+ buildings which all have their own specificities. There's a lot of data cleaning to do. But I wouldn't call it ""unfair"". We're all in the same boat.",114893,2019-10-29 23:12:52,S D
"""Temperatures look like they are all in C.""
Not at all … check !
*""timestamps"" *are all fake !
*""pressure data, tempair, dewtemp"" *does'nt mean anything… except if you analise them one by one … Dewww
""There's a lot of data cleaning to do"" and we are not Scotland Yard neither NY PD… 
Just Kagglers not Clusot Inspectors… VaBe ognitanto !",114893,2019-10-29 23:23:32,mezoganet
"Looks like celsius to me
",114893,2019-10-29 23:42:13,S D
"Are you really sure ?
I did the same …  hem…
I am not there for the money, but the fun to compete.",114893,2019-10-29 23:47:32,mezoganet
"You can see also tiny histograms of the data without even downloading it, on the data page for the competition. Looks about the same as what I posted above for temperature.
Sea level pressure looks like it's centered on 1000ish; has to be in mbar? Again that doesn't look wrong to me.",114893,2019-10-30 00:09:09,S D
"Sure, it seems loyal and clean ….
But anything about hours(timestamp) and the rest ? 
Well it's Saturday Night, I've beena DJ long time ago…. Come on Guys, let's dance an foot your stump ! Yaaahhh
https://www.youtube.com/watch?v=00ySZOPPDD4",114893,2019-10-30 00:27:15,mezoganet
Isn't that the fun part?,114893,2019-10-30 06:25:06,Gunes Evitan
"I don't think so, with all my respect…  Ask  Fred about my feelings.",114893,2019-10-31 19:51:16,mezoganet
"For sure real world data sets are real crap - this data set not quite as clean as past Kaggle, but it seems to me that the hard part of this competition is making the data make sense.  
Never got the chance while I was working to use ML tools - pretty much had pencil and paper eventually up to JMP version 14.
For those tools over almost 50 years, by the time I got a decent data set I almost did not need the statistics - because I spent 200 hours cleaning up the data for every 1 hour of JMP analysis time and came to a model in my head by the time I was done cleaning.
Vopani has it almost right IMO - if Kaggle provides absolutely clean data sets, Kagglers will get a job but lose it when the boss figures out your probably better at washing dishes than you are at cleaning data.
Cleaning is the real skill set to learn - the ML tools will almost let a monkey run the model within a few years, while the abundance of data will make the cleaning job much harder.",114893,2019-10-30 06:56:13,PC Jimmmy
"import sklearn
fit 
predict",114893,2019-10-31 04:23:04,spongebob
"What do you mean ?
Is SciKitLearn a new recipe from Mc Donalds ?
I don't know (SciKitLearn neither Keras, neither LGB, neither XGB… kinda newbie I guess)
Are you kidding ? ",114893,2019-10-31 19:42:32,mezoganet
"Thanks for the downvotes…
You were not there somtimes ago, I may guess.
I just pretend we do not have to guess but JUST predict. It will remain my judgement.
We're not there to guess, jut proove
Okay, I stop there.
PS : I am not the last soldier on this classification, and I will fight till the end. Just a little bit upset even if nobody share my feeling/opinion.",114893,2019-10-30 00:10:17,mezoganet
"Did you really delete the old post about 'Not fair at all' cause of downvotes and created a new one, copy-pasting the same stuff??? Man thats vain. ",114893,2019-11-05 12:37:19,Georgios Sarantitis
"Not at all ! I did not delete anything…
Still the same post since 7 days ago.",114893,2019-11-05 16:01:15,mezoganet
Then how come I cant find my previoius comment on fairness….Stranger Things…,114893,2019-11-06 07:07:24,Georgios Sarantitis
"Gentle Kaggle guys, t would be fair to stop downvoting : are we back to Dark Ages ? 
Wille you ligth a pyre or will you burn me to death ?
Am I a Keggle heretic ? 
RELAX please ! Not the 3rd world war… just a comment !
Like Anglo-Saxons use to say : if you don't like it don't reply… nothing positive to say, don,t say nothing.
(-)11 downvotes, never seen that before.
I was just saying ""This competition is not fair at all…"" and blah blah blah .
Will I have to do my ""way of the cross"" and receive ""whiplash"" till the Golgotha mountain ? 
Think about it :  11 DownVotes ! Heww…
https://www.youtube.com/watch?v=Mw0vGezzHqo
I would bet on 20 downvotes by tomorrow ;-)",114893,2019-11-04 19:50:03,mezoganet
"Why are you bumping this thread every day?  There was 11 downvotes 4 days ago, there's still 11 downvotes today; why not just let it go?",114893,2019-11-04 19:54:23,S D
"@sdoria 
Because I don’t accept and never will.
We’re all here to compete and sometimes express our frustration not against someone but just about our incapacity to find our way and shout at our misunderstanding, but never attack anybody.
I come from Brittany, France, a Celtic guy and We allways needed to find the answers, and never accepted insults.
And 11 downvotes is kind of insult, when I express what a lot of all of us said in another way…
I would say it’s just some stress from all of us, but not acceptable.
Just read again what I wrote, no insulting people, no rude sentences, nothing evil. So why all those negative votes ?
Let it go ? When I understand why…
Thank you for your comment anyway ! Muchas Gracias ! Grazzie tante volte ! Trugarez !",114893,2019-11-04 22:56:42,mezoganet
"Oh ! Like a hurricane here 💪 
Hey guys was there something insulting from my side ? 
o tempora o mores !
Wonderful Life – The Burgess Shale and the Nature of History, by Steven Jay Gould and try to understand that we are all descendants from Pikaia with no stree, no aggressivity, just succeed to survive.
I don't wanna fight, I just wanna get a good score in theis competition …
I know : ""only the fittest of the fittest will survive"", Darwin… But I down want to kill just get a score and no downvotes by 100's
A good reading againt aggressivity : ""https://en.wikipedia.org/wiki/WonderfulLife(book)""
Jay Gould is one of my master, among some others… But Steven Jay Gould may be the paleontologist one must read, no kidding. 
After tha, you can keep your gun at his place, not in your hand. 
9 downvotes ? Are you really so upset guys ???
This is what I listen to when I am upset guys : 
https://www.youtube.com/watch?v=kOFu6b3w6c0",114893,2019-11-02 20:52:06,mezoganet
"Please ! stop downvoting … 
It was just some kind of ""exhauted"" guy who wrote… really tired guy and a little bit upset. No aggressivity !
Thanks a lot for your undestanding all of you, thanks again.
https://www.youtube.com/watch?v=F1B55wEB030
Bob Marley will remain some kind of Hero for the future…",114893,2019-11-02 19:43:46,mezoganet
Thanks a lot for no more downvotes… It would be fair.,114893,2019-10-31 20:02:24,mezoganet
"Did you undesrtand ?
""Thanks a lot for no more downvotes… It would be fair.""",114893,2019-11-02 20:57:36,mezoganet
"Thanks a lot for the eleven (at least) downvoters.
Just a a feeling that we will not find a clew among all this messy data.
Like every body here I just struggle to get a score.
Did not want to offense any body and all  those downvotes are kinda misunderstanding , that's what I feel, and it aches.
I beg your pardon, if it can solve something.
Never being injurious here, just kind of upset…
Kaggle is allways a  kind of challenge and sometimes one can loose its one patience.
Am I the only one ?
I beg your pardon… Sorry, so sorry for that .",114893,2019-10-31 19:37:44,mezoganet
Why not try to be creative with your modeling? I don't know any of these answers either and I have done ZERO investigation into these data peculiarities. Are you assuming you need to figure these out to win? ,114893,2019-10-29 23:24:05,SteveKane
"Wellcome on board Sir…
Kaggle is meant to be a Machine Learning site, not a cleaning garbage Data or Did I miss something ? 
I am really tired to find out where Data is, real Data and not ""guess where Data is hidden ?""…
Heewwwwhhh ! Tired, really tired.
Where have all the good times gone ?
https://www.google.fr/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&cad=rja&uact=8&ved=2ahUKEwiA-rmA08LlAhXRy4UKHbUMB8EQyCkwAHoECAkQBQ&url=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DEqrwsGrfv7M&usg=AOvVaw3inSBG2_mhh-QVlODDxLSg",114893,2019-10-29 23:36:07,mezoganet
"Love the Kinks! New tune for me. Thanks for that. Where have all the good times gone? Well, you just passed some on to me. That ought to count for something, I surely appreciate it. Take care mate!",114893,2019-10-30 00:03:43,Martin Elfstadius
"allways wellcome when you talk about Kinks !
But this maybe one one of the best :
https://www.youtube.com/watch?v=VBjbxJ1ZjHY",114893,2019-10-30 00:16:55,mezoganet
"I allways take care…
To young to die but too old for Rock and Roll…
That's why I am still alive even though … even though … I just can't stand it sometimes.
Justfor you : 
Ladies and gentlemen : THE KINKS !
https://www.youtube.com/watch?v=h2svmUcsKeg",114893,2019-10-30 00:33:42,mezoganet
"Another excellent song! Thanks a lot!
Rock’n’Roll never outgrows you 👍🏽
https://youtu.be/1_xwnb3cymc
Nite! :)",114893,2019-10-30 01:23:49,Martin Elfstadius
"Never happen before and now again. If it would be so easy why someone will do a competition about it? Also you should try Santander competitions as an example of ""fair"" competitions (irony)",114893,2019-10-30 07:23:08,WispZero
,114893,2019-10-29 23:08:44,mezoganet
Similar discussion here,118353,2019-11-21 00:41:31,Chris Deotte
Actually it is the only feature works for me in aggregations,119265,2019-11-27 19:14:27,Gunes Evitan
Aggregations on that feature havent worked for me. Always end up at the bottom end in feature importance. Can you shed some light how it worked for you?,119265,2019-11-28 08:25:22,Jonathan Mallia
"you can get a gold medal without using leak because the results will not use leaked data points. you're public LB could be low, but private high",119292,2019-11-28 00:18:19,SteveKane
Your 1.04 submission without leaks？Amazing.,119292,2019-11-28 06:04:53,firstbloodY
Hey @firstbloody I'm pretty sure it has leaks in it :-(,119292,2019-12-01 08:00:46,XLCBobby
"No, even if your model predicts NaN, you should introduce a numeric, e.g. 0, otherwise the submission system will give you an error. If your model needs weather data that is unavailable try using a flag for Nan (e.g. -999999), or a mean/ median value or interpolate the missing values from the ones that are available. ",119670,2019-11-30 21:27:36,Panos
Thank you both!,119670,2019-12-01 10:03:01,Nathan Zhai
Interpolation isn't too hard. Check out this kernel. It gives a practical implementation of interpolation but also shows that (surprisingly) it only helps by about half a percent.,119670,2019-12-01 03:34:26,Robert Stockton
"In R, use left_join instead of merge and do it by combining variables, e.g. by=c(""site_id"",""timestamp"").",119788,2019-12-01 18:33:52,Panos
,119788,2019-12-01 17:11:28,rackson3861
"In python reduce memory of every data frame before merging, so reduce: building,weather and train/test.
In R data.table is quiet strong, so you can merge without any issues.",119788,2019-12-01 16:57:29,Davide Stenner
Can you help me in one thing. Do i need to track the training set for each building separately and there meters also?,119788,2019-12-02 02:58:19,rackson3861
"@rackson3861
It depends from the model you are using. In some kernel as divide and conquer it has been created one model for each meter (or site_id).
In my experience one single model for all data fine tuned with right features is better because lgbm are quite strong.
But if you're running every time out of memory and you don't want to optimize to much your code you can analyse every different meter or siteid separately and merge final results, this will surely help to reduce memory issues.",119788,2019-12-02 05:59:21,Davide Stenner
"In R ? Take a look at  https://cran.r-project.org/web/packages/data.table/vignettes/datatable-intro.html
I also spend my day (and yesterday) figure out how to make my kernel not crashing during normalization of data. (Spoiler alert : gc() after every steps, but one was difficult to spot).",119788,2019-12-01 16:14:11,Etienne R
"I agree with @pcjimmmy , it depends. If it oscillates between 0 and some small value, it is probably correct. But, if it looks like this, clearly something is wrong. It doesn't look natural.
",119865,2019-12-04 05:42:38,Gunes Evitan
"I think the answer is ""it depends"".
Seems like some 0.0 are legitimate - for example there are a number of swimming pools that pretty much show only summer energy.  Some 0.0 there look to be the real deal.
Others seem to be very clearly failed sensor readings.  The site_0 early readings are for sure not real - that site was not built in 2016.
When you consider that the meters are spread across 1400+ buildings and 16 sites - than there's going to be a nice mix of real and not real.  This is a super dirty data set - getting it clean would seem to be a high percentage of the problem.",119865,2019-12-04 01:39:49,PC Jimmmy
You can find a similar question here.,119865,2019-12-03 15:26:54,Mohammed Tayor
"I think you are passing timestamp or objects into lgbm, try checking dtypes.",120875,2019-12-10 01:12:15,Mr Loke
"thanks Man, it worked. 👍 ",120875,2019-12-10 05:43:11,ds.my.way
@jielu0728 are you chinese?,121211,2019-12-12 09:23:52,Kai Shu
@jielu0728 I invited you to my team,121211,2019-12-12 06:15:28,Kai Shu
I sent you the merge request,121211,2019-12-12 09:14:03,Jie Lu
I have xgboost 1.06 without leak!,121211,2019-12-12 06:13:35,Kai Shu
"Do you want to team up with me? I have LightGBM trained on leak data with 1.06 score. Another LightGBM model trained on leak data with 1.07 score. Merge of both those two models with leak data, give me a score of 0.96 on LB. I am probably very very close to 0.95",121211,2019-12-12 12:40:28,Kamal Chhirang
I am interested.,121211,2019-12-11 23:04:41,Cakey
"Hello Kai I'm interested in teaming with you, if you don't mind me having already 90+ submissions.
I have several 1.06 single models without leak",121211,2019-12-11 21:14:22,Jie Lu
I'd like to team up with you guys too! ,121211,2019-12-12 00:59:12,Fernando Wittmann
"Not sure if it is possible to send PM from Kaggle. Anyone interested to team up with me, my email is fernando.wittmann@gmail.com ",121211,2019-12-12 01:39:31,Fernando Wittmann
"I wonder if you are interested in my team, our no leak model also is 1.06,maybe we can try get a better res",121211,2019-12-12 02:34:00,kaihenguestc
This Comment was deleted.,121211,2019-12-12 00:25:57,No user
They could be looking for huge boosts between submissions for leak detection.,122670,2019-12-22 04:32:05,Gunes Evitan
"Kaggle admins have confirmed the LB is not reliable and has a few issues while they are rescoring and fixing the 'snags': https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122554
Would recommend you to wait until its done. Most likely it will be back to normal.
I think all submissions are rescored for consistency and post-competition analysis and submissions.",122670,2019-12-22 02:20:35,Vopani
It means you need to wait and agonize until the private LB is displayed.,122881,2019-12-23 16:04:24,CPMP
And you are more than welcomed to downvote to claim your illiteracy.,122974,2019-12-27 16:08:53,TongLi
"I didn't expect that Kaggle attracts so many illiterates, at least 13 found in the post.
Please learn how to read first.",122974,2019-12-27 15:50:50,TongLi
Why the downvotes?,122974,2019-12-24 01:36:16,Aleck Wu
They probably thought that I was making an announcement 😅 ,122974,2019-12-24 03:09:30,TongLi
"Please post here in the formal thread:
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/112843#latest-660383
Also, adding few words about yourself (generally) increases the probability to find the right team mate. 
Enjoy!",114763,2019-10-29 04:50:57,arnab
"In order to save memory and avoid processing entire test set in one go, prediction dataframe is built in batches of 50k rows.",114897,2019-10-29 23:29:48,Konrad Banachewicz
"The buildings are totally same.
Only difference b/w train and test data is the time-span.
So we don't have another data file for building metadata.",114914,2019-10-30 05:19:48,JumpeiT
Thanks a lot,114914,2019-10-30 05:42:55,Prasidha 
"I am assuming the missing values in floor denotes that it's single floor building ,
Coming to your question some alogrithms:

you can try kNN for determining the number of a floor using features like , year of build , primary use , average meter reading , square foor , average number of meter types installed  etc .
In my opinion the squarefeet is a good indicator of number of floors 👍 
",115290,2019-11-03 05:43:04,AdityaVikramSingh
"No need for a separate submission! The csv file already contains the records that are included in the private dataset, we just don't know which ones they are. You'll get to see your private LB score right after the competition is finalized.",121104,2019-12-11 09:59:25,Panos
"You need to commit your kernel, then click on the ""My submission"" button, then select your kernel name by selecting ""Use for Final Score"".",121104,2019-12-11 05:12:57,Mr Loke
You can use as many kernels as you want in your pipeline.,121507,2019-12-14 12:16:53,Gunes Evitan
When this happened to me I had somehow messed up the row_id's.,122070,2019-12-17 17:05:56,PC Jimmmy
"Yes, you are right, that was my problem. Thank you!",122070,2019-12-17 19:05:01,one1111
"https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122296
It may take a little longer",122740,2019-12-22 20:11:14,ysuzuki
"It says We expect to finalize the leaderboard approximately 24-48 business hours after the competition has closed, per our usual timeline.",122740,2019-12-22 21:00:35,Manoj
"Comments are a little helpful
https://www.kaggle.com/c/ashrae-energy-prediction/discussion/122562",122740,2019-12-22 21:08:42,ysuzuki
"Building_id1099 was not scored in PrivateBoard
It was a meaningless question",123297,2019-12-28 03:35:09,pea98258
"You should watch out for some buildings like 403, 604 as they have only several hundreds of examples in the training set.
I agree with Oleg. In this case you should try to make your model learn from other similar buildings to predict for example building id 403 and 604.",115174,2019-10-31 21:40:48,Jie Lu
Do you have the best results with deep learning MLP?,115174,2019-10-31 23:48:07,HenryHub
For each meter is better than for each building.at least for me,115174,2019-10-31 20:52:44,Oleg Knaub
"There is no right or wrong answer, you just have to explore :-). meter_reading is indeed your dependent variable, but as far as how you get to it…",115174,2019-10-31 20:23:51,authman
Thanks. Is it legitimate per kaggle standards to break down all the data into individual CSVs and join them together again?,115174,2019-10-31 20:36:06,HenryHub
"There are no Kaggle standards. That stated, I wouldn't recommend doing this. Even if you're hurting for RAM, it'll still be more straightforward to stream the loading of your data into chunks in my opinion.",115174,2019-11-01 02:48:44,authman
"Hey @authman, one quick question. What's the problem of breaking down into CSV's?",115174,2019-11-01 17:33:56,Manraj Singh
"I think you mean floor_count and year_built. There is no missingsquare_feet data. If you use tree based algos, you can probably try assigning them -1 and treat them as categorical. There's also model based imputation, mode, or some other constant.",115303,2019-11-01 19:06:19,Tim Yee
"The whole idea of not showing additional decimal places is to prevent users from probing the public LB. Which is a good thing to avoid solutions based on LB abuse. The leaderboard position still shows your true position using additional decimals. So, you can know a 'broad' estimate of the additional digit looking at the ranking of your submissions and the relative ranking of all competitors with the same score up to 2nd decimal.
My advice: Try to make improvements of > 0.01 :-)
This has been asked and discussed multiple times before, so it's unlikely to be changed.",115348,2019-11-02 07:00:17,Vopani
"Well, 2 sub/day and 2 digit LB score, all means the message ""Establish concrete validation early with limited subs and focus to try to improve your CV"" … That makes sense. ",115348,2019-11-02 07:25:31,resistance0108
I agree to you point. Though you can roughly estimate your score has it increased or decreased!,115348,2019-11-02 16:42:03,Manraj Singh
One way of not getting thousands of notification is to not post the same topic in every 7 forums in a row. That would help,115566,2019-11-03 19:33:21,Luca Basanisi
Or you can unfollow the posts in 'Options' -> 'Unfollow'.,115566,2019-11-04 06:21:25,Juanma Hernández
I think you can adjust your notification from the notification tab. My Profile --> Three Dots (…) -> Notifications. ,115566,2019-11-04 04:05:41,arnab
"Simple and useful solution ! https://www.kaggle.com/c/ashrae-energy-prediction/discussion/123288
Kernel by @wittmannf : https://www.kaggle.com/wittmannf/0-939-lb-public-blend-leak-valid-ridgecv/",123164,2019-12-26 16:08:26,CaesarLupum
"You could look at https://www.kaggle.com/isaienkov/keras-nn-with-embeddings-for-cat-features-1-15 to start with. 
To improve it you need to do some data cleanup.",123164,2019-12-25 21:15:48,Helgi
"You have to predict 2 years with 1 years of data. So it's a bit different to do it with LSTM for example. I would suggest lightgbm or xgboost for this competition.
This kernel should help you if you are looking only for the implementation and not in the performance of the model.",123164,2019-12-25 10:58:56,Oussa
